{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transplant.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMiZokwLwRhQq8zMHuaRpKY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunxueliang96/WF-FrameWork/blob/master/transplant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3WCsIaUTuNV",
        "colab_type": "code",
        "outputId": "e1092327-100e-40e7-e9ed-106eaf05f317",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "#Mount Google Drive as folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF9tyC-TTxmk",
        "colab_type": "code",
        "outputId": "8379bb07-f6ba-4572-8a7b-8de1e7e799a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/drive/'My Drive'/datasets/no_paded/open_world/walkiebatch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/datasets/no_paded/open_world/walkiebatch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thC1b8glT6p9",
        "colab_type": "code",
        "outputId": "0526defc-2d5c-46c3-b425-a13974d7d60b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LOCALLY_CHANGE_historys_DNN_NB_TIMES.pkl\n",
            "LOCALLY_CHANGE_historys_DNN_PERCENT_CHANGED.pkl\n",
            "NOISE_ADDED_historys_DNN_NB_TIMES.pkl\n",
            "NOISE_ADDED_historys_DNN_PERCENT_NOISE.pkl\n",
            "X_walkiebatch.pkl\n",
            "y_walkiebatch.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOpTb4DkT9db",
        "colab_type": "code",
        "outputId": "64034d77-4572-4c6a-fd87-8b39b2175246",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "print('loading data...')\n",
        "with open('X_walkiebatch.pkl','rb') as handle:\n",
        "  X = np.array(pickle.load(handle))\n",
        "with open('y_walkiebatch.pkl','rb') as handle:\n",
        "  y = np.array(pickle.load(handle))\n",
        "print('the shape of X',X.shape)\n",
        "print('the shape of y',y.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading data...\n",
            "the shape of X (22159,)\n",
            "the shape of y (22159,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnVKARJIuhwD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBm0gBfZZ_UG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math \n",
        "import random\n",
        "def creat_data(X,y,PERCENT_TRANSPLANT,NB_TIMES):        #Magnificate dataset by adding noising randomly\n",
        "    X = list(X)\n",
        "    y = list(y)\n",
        "    PERCENT_TRANSPLANT = PERCENT_TRANSPLANT                 #PERCENT of Noise at each time\n",
        "    NB_TIMES = NB_TIMES                            #TIMES of Magnification\n",
        "    X_sum = X.copy()\n",
        "    y_sum = y.copy()\n",
        "    print(\"{} samples has beed add to X_train, {}% of noise for each sample\".format(math.ceil(len(X)*NB_TIMES),PERCENT_NOISE))\n",
        "    for i in range(math.ceil(len(X)*NB_TIMES)):\n",
        "        while(True):                               # target pos p (sensitive website), pos q (non-sensitive website)\n",
        "            p = random.choice(range(len(X)))\n",
        "            q = random.choice(range(len(X)))\n",
        "            if(y[p]!=-1 & y[q]==-1):               # open_world\n",
        "           # if(y[p]!=-1):                         # close_world\n",
        "                break\n",
        "            else:\n",
        "                #print('-1 detected')\n",
        "                pass\n",
        "        \n",
        "        X_new = X[p].copy()\n",
        "        target = X[q].copy()\n",
        "        y_new = y[p]\n",
        "\n",
        "        length_X = math.ceil(PERCENT_TRANSPLANT*len(X_new)/100)\n",
        "        length_target =  math.ceil(PERCENT_TRANSPLANT*len(target)/100)\n",
        "        start_X = random.choice(range(len(X_new)-length_X))\n",
        "        start_target = random.choice(range(len(X_new)-length_target))\n",
        "        temp = target[start_target:start_target+length_target]\n",
        "        X_new[start_X:start_X+length_X] = temp\n",
        "     \n",
        "        X_sum.append(X_new)\n",
        "        y_sum.append(y_new)\n",
        "    return np.array(X_sum),np.array(y_sum)                        #return two array of X, y \n",
        "\n",
        "    '''\n",
        "def locally_change(X,y,PERCENT_CHANGE,METHOD):                               \n",
        "    X = list(X)\n",
        "    y = list(y)\n",
        "    PERCENT_NOISE = PERCENT_NOISE                  #PERCENT of Changing\n",
        "    NB_TIMES = METHOD                              #Method of Changing\n",
        "    X_sum = X.copy()\n",
        "    y_sum = y.copy()\n",
        "\n",
        "    for i in range(math.ceil(len(X)*NB_TIMES)):\n",
        "        p = random.choice(range(len(X)))\n",
        "        #print('###########################################',p)\n",
        "        X_new = X[p].copy()\n",
        "        y_new = y[p]\n",
        "        for n in range(math.ceil(NB_TIMES*len(X_new)/100)):#dealing X_new\n",
        "            noise = random.choice([1,-1])       #noise could only be 1 or -1\n",
        "            pos = random.choice(range(len(X_new)))  #position belong to (0,len(sequence))\n",
        "            X_new.insert(pos,noise)\n",
        "            #print('insert {} at position {} in sequence {} now len of sequence is {}'.format(noise,pos,p,len(X_new))) #for test\n",
        "        X_sum.append(X_new)\n",
        "        y_sum.append(y_new)\n",
        "    return np.array(X_sum),np.array(y_sum)                        #return two array of X, y         \n",
        "    #return x,y\n",
        "def locally_delete():                                \n",
        "    pass\n",
        "def rnn_padding():                                  #a seq to seq model\n",
        "    pass\n",
        "def bayes_padding():\n",
        "    pass\n",
        "def mult_scale_windowing():\n",
        "    pass\n",
        "def GAN_padding():\n",
        "    pass\n",
        "def data_mixing():\n",
        "    pass\n",
        "    '''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZpMmHj3Uefl",
        "colab_type": "code",
        "outputId": "dfdc7075-471f-481d-cced-9a37159386dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import sequence\n",
        "from collections import Counter\n",
        "#CLASSES_KNN = 101########################################################################################################################################################################################################\n",
        "#CLASSES_WAKIE = 100########################################################################################################################################################################################################\n",
        "MAXLEN_KNN = 2000\n",
        "MAXLEN_WAKIE =5000\n",
        "\n",
        "maxlen = 5000\n",
        "NB_CLASSES = 0\n",
        "\n",
        "def choose():\n",
        "    global maxlen,NB_CLASSES\n",
        "    print('if padding, the max of length of seq is {}'.format(maxlen))\n",
        "    NB_CLASSES = len(Counter(y).keys())\n",
        "    print('number of classes is {}'.format(NB_CLASSES))\n",
        "\n",
        "choose()################################################################################################################################################################################################################\n",
        "\n",
        "#print('spliting data...')\n",
        "#X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
        "#X_train,y_train = creat_data(X_train,y_train,PERCENT_NOISE=0.5,NB_TIMES=1)\n",
        "\n",
        "#print(len(X_train), 'train sequences')\n",
        "#print(len(y_train), 'test sequences')\n",
        "print('Average sequence length: {}'.format(np.mean(list(map(len, X)), dtype=int)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "if padding, the max of length of seq is 5000\n",
            "number of classes is 109\n",
            "Average sequence length: 4683\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_gm7tqd4yEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#FCN\n",
        "from keras import Sequential\n",
        "from keras.layers.core import Activation, Flatten, Dense, Dropout\n",
        "from keras.optimizers import Adamax\n",
        "\n",
        "def DNN(input_shape, classes):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_shape=(input_shape,)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(256))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(512))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(1024))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(512))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(classes))\n",
        "    model.add(Activation('softmax'))\n",
        "    return model\n",
        "def run_DNN(X_train,y_train,X_test,y_test):\n",
        "    print('Pad sequences to ',maxlen)\n",
        "    x_train = X_train[:]\n",
        "    x_test = X_test[:]\n",
        "    x_train = sequence.pad_sequences(x_train, maxlen=maxlen,padding='post',truncating='post')\n",
        "    x_test = sequence.pad_sequences(x_test, maxlen=maxlen,padding='post',truncating='post')\n",
        "    y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
        "    y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
        "    print('the shape of x_train',x_train.shape)\n",
        "    print('the shape of y_train',y_train.shape)\n",
        "    print('the shape of x_test',x_test.shape)\n",
        "    print('the shape of y_test',y_test.shape)\n",
        "    model_DNN = DNN(maxlen,NB_CLASSES)\n",
        "    #model_DNN.summary()\n",
        "    model_DNN.compile(loss='categorical_crossentropy',optimizer=OPTIMIZER,metrics=['accuracy', precision, recall, fmeasure])\n",
        "    history = model_DNN.fit(x_train,y_train,batch_size=BATCH_SIZE,epochs=NB_EPOCH,validation_data=(x_test,y_test),verbose=0)\n",
        "    score = model_DNN.evaluate(x_test,y_test,verbose=VERBOSE)\n",
        "    #print(history.history)\n",
        "    #print(score)\n",
        "    return(history.history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9hQ5LZK6sFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CNN\n",
        "from keras import Input,Model,Sequential\n",
        "from keras.layers import Embedding,GlobalAveragePooling1D,Dense,Dropout\n",
        "from keras.layers import Conv1D, MaxPooling1D, BatchNormalization\n",
        "from keras.layers.core import Activation, Flatten, Dense, Dropout\n",
        "from keras.layers.advanced_activations import ELU\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras.optimizers import Adamax\n",
        "def CNN(input_shape, classes):\n",
        "    model = Sequential()\n",
        "    #Block1\n",
        "    filter_num = ['None',32,64,128,256]\n",
        "    kernel_size = ['None',8,8,8,8]\n",
        "    conv_stride_size = ['None',1,1,1,1]\n",
        "    pool_stride_size = ['None',4,4,4,4]\n",
        "    pool_size = ['None',8,8,8,8]\n",
        "\n",
        "    model.add(Conv1D(filters=filter_num[1], kernel_size=kernel_size[1], input_shape=(input_shape,1),\n",
        "                      strides=conv_stride_size[1], padding='same',\n",
        "                      name='block1_conv1'))\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "    model.add(ELU(alpha=1.0, name='block1_adv_act1'))\n",
        "    model.add(Conv1D(filters=filter_num[1], kernel_size=kernel_size[1],\n",
        "                      strides=conv_stride_size[1], padding='same',\n",
        "                      name='block1_conv2'))\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "    model.add(ELU(alpha=1.0, name='block1_adv_act2'))\n",
        "    model.add(MaxPooling1D(pool_size=pool_size[1], strides=pool_stride_size[1],\n",
        "                            padding='same', name='block1_pool'))\n",
        "    model.add(Dropout(0.1, name='block1_dropout'))\n",
        "\n",
        "    model.add(Conv1D(filters=filter_num[2], kernel_size=kernel_size[2],\n",
        "                      strides=conv_stride_size[2], padding='same',\n",
        "                      name='block2_conv1'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu', name='block2_act1'))\n",
        "\n",
        "    model.add(Conv1D(filters=filter_num[2], kernel_size=kernel_size[2],\n",
        "                      strides=conv_stride_size[2], padding='same',\n",
        "                      name='block2_conv2'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu', name='block2_act2'))\n",
        "    model.add(MaxPooling1D(pool_size=pool_size[2], strides=pool_stride_size[3],\n",
        "                            padding='same', name='block2_pool'))\n",
        "    model.add(Dropout(0.1, name='block2_dropout'))\n",
        "\n",
        "    model.add(Conv1D(filters=filter_num[3], kernel_size=kernel_size[3],\n",
        "                      strides=conv_stride_size[3], padding='same',\n",
        "                      name='block3_conv1'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu', name='block3_act1'))\n",
        "    model.add(Conv1D(filters=filter_num[3], kernel_size=kernel_size[3],\n",
        "                      strides=conv_stride_size[3], padding='same',\n",
        "                      name='block3_conv2'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu', name='block3_act2'))\n",
        "    model.add(MaxPooling1D(pool_size=pool_size[3], strides=pool_stride_size[3],\n",
        "                            padding='same', name='block3_pool'))\n",
        "    model.add(Dropout(0.1, name='block3_dropout'))\n",
        "\n",
        "    model.add(Conv1D(filters=filter_num[4], kernel_size=kernel_size[4],\n",
        "                      strides=conv_stride_size[4], padding='same',\n",
        "                      name='block4_conv1'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu', name='block4_act1'))\n",
        "    model.add(Conv1D(filters=filter_num[4], kernel_size=kernel_size[4],\n",
        "                      strides=conv_stride_size[4], padding='same',\n",
        "                      name='block4_conv2'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu', name='block4_act2'))\n",
        "    model.add(MaxPooling1D(pool_size=pool_size[4], strides=pool_stride_size[4],\n",
        "                            padding='same', name='block4_pool'))\n",
        "    model.add(Dropout(0.1, name='block4_dropout'))\n",
        "\n",
        "    model.add(Flatten(name='flatten'))\n",
        "    model.add(Dense(512, kernel_initializer=glorot_uniform(seed=0), name='fc1'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu', name='fc1_act'))\n",
        "\n",
        "    model.add(Dropout(0.7, name='fc1_dropout'))\n",
        "\n",
        "    model.add(Dense(512, kernel_initializer=glorot_uniform(seed=0), name='fc2'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu', name='fc2_act'))\n",
        "\n",
        "    model.add(Dropout(0.5, name='fc2_dropout'))\n",
        "\n",
        "    model.add(Dense(classes, kernel_initializer=glorot_uniform(seed=0), name='fc3'))\n",
        "    model.add(Activation('softmax', name=\"softmax\"))\n",
        "    return model\n",
        "def run_CNN(X_train,y_train,X_test,y_test):\n",
        "    print('Pad sequences to ',maxlen)\n",
        "    x_train = X_train[:]\n",
        "    x_test = X_test[:]\n",
        "    x_train = sequence.pad_sequences(x_train, maxlen=maxlen,padding='post',truncating='post')\n",
        "    x_test = sequence.pad_sequences(x_test, maxlen=maxlen,padding='post',truncating='post')\n",
        "    x_train = x_train[:,:,np.newaxis]\n",
        "    x_test = x_test[:,:,np.newaxis]\n",
        "    y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
        "    y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
        "    print('the shape of x_train',x_train.shape)\n",
        "    print('the shape of y_train',y_train.shape)\n",
        "    print('the shape of x_test',x_test.shape)\n",
        "    print('the shape of y_test',y_test.shape)\n",
        "    model_CNN = CNN(maxlen,NB_CLASSES)\n",
        "    #model_CNN.summary()\n",
        "    model_CNN.compile(loss='categorical_crossentropy',optimizer=OPTIMIZER,metrics=['accuracy', precision, recall, fmeasure])\n",
        "    history = model_CNN.fit(x_train,y_train,batch_size=BATCH_SIZE,epochs=NB_EPOCH,validation_data=(x_test,y_test),verbose=0)\n",
        "    score = model_CNN.evaluate(x_test,y_test,verbose=VERBOSE)\n",
        "    #print(score)\n",
        "    return(history.history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNxUC1-miWSA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#LSTM\n",
        "from keras import Input,Model,Sequential\n",
        "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, TimeDistributed\n",
        "from keras.layers.convolutional import Conv1D,MaxPooling1D\n",
        "from keras.layers.core import Activation, Flatten, Dense\n",
        "from keras.layers.advanced_activations import ELU\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras.optimizers import Adamax\n",
        "def my_LSTM(input_shape, classes):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(256,activation='relu',return_sequences=True,input_shape=(input_shape,1)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(LSTM(256,return_sequences=True))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(LSTM(512,return_sequences=False))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dense(classes, activation='softmax'))\n",
        "    return model\n",
        "\n",
        "def run_LSTM(X_train,y_train,X_test,y_test):\n",
        "    print('Pad sequences to ',maxlen)\n",
        "    x_train = X_train[:]\n",
        "    x_test = X_test[:]\n",
        "    x_train = sequence.pad_sequences(x_train, maxlen=maxlen,padding='post',truncating='post')\n",
        "    x_test = sequence.pad_sequences(x_test, maxlen=maxlen,padding='post',truncating='post')\n",
        "    x_train = x_train[:,:,np.newaxis]\n",
        "    x_test = x_test[:,:,np.newaxis]\n",
        "    y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
        "    y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
        "    print('the shape of x_train',x_train.shape)\n",
        "    print('the shape of y_train',y_train.shape)\n",
        "    print('the shape of x_test',x_test.shape)\n",
        "    print('the shape of y_test',y_test.shape)\n",
        "    model_LSTM = my_LSTM(maxlen,NB_CLASSES)\n",
        "    #model_LSTM.summary()\n",
        "    model_LSTM.compile(loss='categorical_crossentropy',optimizer=OPTIMIZER,metrics=['accuracy', precision, recall, fmeasure])\n",
        "    history = model_LSTM.fit(x_train,y_train,batch_size=BATCH_SIZE,epochs=NB_EPOCH,validation_data=(x_test,y_test),verbose=0)\n",
        "    score = model_LSTM.evaluate(x_test,y_test,verbose=VERBOSE)\n",
        "    return(history.history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-J-rv6Ri0r2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CNN+LSTM\n",
        "from keras import Input,Model,Sequential\n",
        "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, TimeDistributed,BatchNormalization\n",
        "from keras.layers.convolutional import Conv1D,MaxPooling1D\n",
        "from keras.layers.core import Activation, Flatten, Dense\n",
        "from keras.layers.advanced_activations import ELU\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras.optimizers import Adamax\n",
        "\n",
        "def CNN_LSTM(n_features,NB_SPLIT,classes):\n",
        "    model = Sequential()\n",
        "    '''\n",
        "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'),input_shape=(None,n_features//NB_SPLIT,1)))\n",
        "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))\n",
        "    model.add(TimeDistributed(Dropout(0.5)))\n",
        "    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
        "    model.add(TimeDistributed(Flatten()))\n",
        "    model.add(LSTM(256))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(classes, activation='softmax'))\n",
        "    '''\n",
        "    #Block1\n",
        "    filter_num = ['None',32,64,128,256]\n",
        "    kernel_size = ['None',8,8,8,8]\n",
        "    conv_stride_size = ['None',1,1,1,1]\n",
        "    pool_stride_size = ['None',4,4,4,4]\n",
        "    pool_size = ['None',8,8,8,8]\n",
        "    model.add(TimeDistributed(Conv1D(filters=filter_num[1], kernel_size=kernel_size[1],\n",
        "                      strides=conv_stride_size[1], padding='same',\n",
        "                      name='block1_conv1'),input_shape=(None,n_features//NB_SPLIT,1)))\n",
        "    model.add(TimeDistributed(BatchNormalization(axis=-1)))\n",
        "    model.add(TimeDistributed(ELU(alpha=1.0, name='block1_adv_act1')))\n",
        "    model.add(TimeDistributed(Conv1D(filters=filter_num[1], kernel_size=kernel_size[1],\n",
        "                      strides=conv_stride_size[1], padding='same',\n",
        "                      name='block1_conv2')))\n",
        "    model.add(TimeDistributed(BatchNormalization(axis=-1)))\n",
        "    model.add(TimeDistributed(ELU(alpha=1.0, name='block1_adv_act2')))\n",
        "    model.add(TimeDistributed(MaxPooling1D(pool_size=pool_size[1], strides=pool_stride_size[1],\n",
        "                            padding='same', name='block1_pool')))\n",
        "    model.add(TimeDistributed(Dropout(0.1, name='block1_dropout')))\n",
        "\n",
        "    model.add(TimeDistributed(Conv1D(filters=filter_num[2], kernel_size=kernel_size[2],\n",
        "                      strides=conv_stride_size[2], padding='same',\n",
        "                      name='block2_conv1')))\n",
        "    model.add(TimeDistributed(BatchNormalization()))\n",
        "    model.add(TimeDistributed(Activation('relu', name='block2_act1')))\n",
        "\n",
        "    model.add(TimeDistributed(Conv1D(filters=filter_num[2], kernel_size=kernel_size[2],\n",
        "                      strides=conv_stride_size[2], padding='same',\n",
        "                      name='block2_conv2')))\n",
        "    model.add(TimeDistributed(BatchNormalization()))\n",
        "    model.add(TimeDistributed(Activation('relu', name='block2_act2')))\n",
        "    model.add(TimeDistributed(MaxPooling1D(pool_size=pool_size[2], strides=pool_stride_size[3],\n",
        "                            padding='same', name='block2_pool')))\n",
        "    model.add(TimeDistributed(Dropout(0.1, name='block2_dropout')))\n",
        "\n",
        "    model.add(TimeDistributed(Conv1D(filters=filter_num[3], kernel_size=kernel_size[3],\n",
        "                      strides=conv_stride_size[3], padding='same',\n",
        "                      name='block3_conv1')))\n",
        "    model.add(TimeDistributed(BatchNormalization()))\n",
        "    model.add(TimeDistributed(Activation('relu', name='block3_act1')))\n",
        "    model.add(TimeDistributed(Conv1D(filters=filter_num[3], kernel_size=kernel_size[3],\n",
        "                      strides=conv_stride_size[3], padding='same',\n",
        "                      name='block3_conv2')))\n",
        "    model.add(TimeDistributed(BatchNormalization()))\n",
        "    model.add(TimeDistributed(Activation('relu', name='block3_act2')))\n",
        "    model.add(TimeDistributed(MaxPooling1D(pool_size=pool_size[3], strides=pool_stride_size[3],\n",
        "                            padding='same', name='block3_pool')))\n",
        "    model.add(TimeDistributed(Dropout(0.1, name='block3_dropout')))\n",
        "\n",
        "    model.add(TimeDistributed(Conv1D(filters=filter_num[4], kernel_size=kernel_size[4],\n",
        "                      strides=conv_stride_size[4], padding='same',\n",
        "                      name='block4_conv1')))\n",
        "    model.add(TimeDistributed(BatchNormalization()))\n",
        "    model.add(TimeDistributed(Activation('relu', name='block4_act1')))\n",
        "    model.add(TimeDistributed(Conv1D(filters=filter_num[4], kernel_size=kernel_size[4],\n",
        "                      strides=conv_stride_size[4], padding='same',\n",
        "                      name='block4_conv2')))\n",
        "    model.add(TimeDistributed(BatchNormalization()))\n",
        "    model.add(TimeDistributed(Activation('relu', name='block4_act2')))\n",
        "    model.add(TimeDistributed(MaxPooling1D(pool_size=pool_size[4], strides=pool_stride_size[4],\n",
        "                            padding='same', name='block4_pool')))\n",
        "    model.add(TimeDistributed(Dropout(0.1, name='block4_dropout')))\n",
        "    model.add(TimeDistributed(Flatten(name='flatten')))\n",
        "    model.add(LSTM(256,return_sequences=True))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(LSTM(512))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(classes, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "def run_CNN_LSTM(X_train,y_train,X_test,y_test):\n",
        "    print('Pad sequences to ',maxlen)\n",
        "    NB_SPLIT = 10\n",
        "    x_train = X_train[:]\n",
        "    x_test = X_test[:]\n",
        "    x_train = sequence.pad_sequences(x_train, maxlen=maxlen,padding='post',truncating='post')\n",
        "    x_test = sequence.pad_sequences(x_test, maxlen=maxlen,padding='post',truncating='post')\n",
        "\n",
        "    x_train = x_train.reshape(x_train.shape[0],NB_SPLIT,maxlen//NB_SPLIT,1)# 把他拆分成NB_SPLIT个子序列，每个子序列分别交给CNN去处理，\n",
        "    x_test = x_test.reshape(x_test.shape[0],NB_SPLIT,maxlen//NB_SPLIT,1)\n",
        "    y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
        "    y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
        "    print('the shape of x_train',x_train.shape)\n",
        "    print('the shape of y_train',y_train.shape)\n",
        "    print('the shape of x_test',x_test.shape)\n",
        "    print('the shape of y_test',y_test.shape)\n",
        "\n",
        "    model_CNN_LSTM = CNN_LSTM(maxlen,NB_SPLIT,NB_CLASSES)\n",
        "    #model_CNN_LSTM.build()\n",
        "    #model_CNN_LSTM.summary()\n",
        "    model_CNN_LSTM.compile(loss='categorical_crossentropy',optimizer=OPTIMIZER,metrics=['accuracy', precision, recall, fmeasure])\n",
        "    history = model_CNN_LSTM.fit(x_train,y_train,batch_size=BATCH_SIZE,epochs=NB_EPOCH,validation_data=(x_test,y_test),verbose=0)\n",
        "    score = model_CNN_LSTM.evaluate(x_test,y_test,verbose=VERBOSE)\n",
        "    return(history.history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuVICfeHwymL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras import backend as K\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    # Calculates the precision\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    # Calculates the recall\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def fbeta_score(y_true, y_pred, beta=1):\n",
        "    # Calculates the F score, the weighted harmonic mean of precision and recall.\n",
        "    if beta < 0:\n",
        "        raise ValueError('The lowest choosable beta is zero (only precision).')\n",
        " \n",
        "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
        "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
        "        return 0\n",
        "\n",
        "    p = precision(y_true, y_pred)\n",
        "    r = recall(y_true, y_pred)\n",
        "    bb = beta ** 2\n",
        "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
        "    return fbeta_score\n",
        "\n",
        "def fmeasure(y_true, y_pred):\n",
        "    # Calculates the f-measure, the harmonic mean of precision and recall.\n",
        "    return fbeta_score(y_true, y_pred, beta=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEMNvc6PntbC",
        "colab_type": "code",
        "outputId": "0635f3d2-c482-4676-c87c-5fc79e028a22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#DNN               ############# test for noise adding  , function :: creat_data############\n",
        "BATCH_SIZE = 64\n",
        "NB_EPOCH = 50\n",
        "VALIDATION_SPLIT = 0.3\n",
        "VERBOSE = 1\n",
        "OPTIMIZER = Adamax()\n",
        "historys_DNN_PERCENT_NOISE = []\n",
        "historys_DNN_NB_TIMES = []\n",
        "PERCENT_NOISE = [0.1,0.5,1,1.5,2,2.5,3,3.5,4,4.5,5,5.5,6,10,20] #hundred percent  %\n",
        "NB_TIMES = [0.1,0.5,1,1.5,2,3,4,5]      #new part of sample\n",
        "print('base_line for PERCENT_NOISE#######################################################################################################################################')\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
        "historys_DNN_PERCENT_NOISE.append(run_DNN(X_train,y_train,X_test,y_test))\n",
        "for i in PERCENT_NOISE:\n",
        "    print('spliting data...')\n",
        "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
        "    X_train,y_train = creat_data(X_train,y_train,PERCENT_NOISE=i,NB_TIMES=0.2) \n",
        "    historys_DNN_PERCENT_NOISE.append(run_DNN(X_train,y_train,X_test,y_test))\n",
        "print('base_line for NB_CLASSES###########################################################################################################################################')\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
        "historys_DNN_NB_TIMES.append(run_DNN(X_train,y_train,X_test,y_test))\n",
        "for i in NB_TIMES:\n",
        "    print('spliting data...')\n",
        "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
        "    X_train,y_train = creat_data(X_train,y_train,PERCENT_NOISE=0.2,NB_TIMES=i) \n",
        "    historys_DNN_NB_TIMES.append(run_DNN(X_train,y_train,X_test,y_test))\n",
        "with open('NOISE_ADDED_historys_DNN_PERCENT_NOISE.pkl','wb') as handle:\n",
        "  pickle.dump(historys_DNN_PERCENT_NOISE,handle)\n",
        "with open('NOISE_ADDED_historys_DNN_NB_TIMES.pkl','wb') as handle:\n",
        "  pickle.dump(historys_DNN_NB_TIMES,handle) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "base_line for PERCENT_NOISE#######################################################################################################################################\n",
            "Pad sequences to  5000\n",
            "the shape of x_train (15511, 5000)\n",
            "the shape of y_train (15511, 109)\n",
            "the shape of x_test (6648, 5000)\n",
            "the shape of y_test (6648, 109)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-9fc97495dc81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'base_line for PERCENT_NOISE#######################################################################################################################################'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mhistorys_DNN_PERCENT_NOISE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_DNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mPERCENT_NOISE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spliting data...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-553a97c2cb70>\u001b[0m in \u001b[0;36mrun_DNN\u001b[0;34m(X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m#model_DNN.summary()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mmodel_DNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOPTIMIZER\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmeasure\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_DNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNB_EPOCH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_DNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVERBOSE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m#print(history.history)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    216\u001b[0m                                          \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                                          \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m                                          verbose=0)\n\u001b[0m\u001b[1;32m    219\u001b[0m                     \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                     \u001b[0;31m# Same labels assumed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mbatch_hook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mbatch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6Py6svpp1YU",
        "colab_type": "code",
        "outputId": "5e044d4f-8014-49bc-8757-d179e0150211",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "source": [
        "#CNN           ############# test for noise adding  , function :: creat_data############\n",
        "BATCH_SIZE = 64\n",
        "NB_EPOCH = 50\n",
        "VALIDATION_SPLIT = 0.3\n",
        "VERBOSE = 1\n",
        "OPTIMIZER = Adamax()\n",
        "historys_CNN_PERCENT_NOISE = []\n",
        "historys_CNN_NB_TIMES = []\n",
        "PERCENT_NOISE = [0.1,0.5,1,1.5,2,2.5,3,3.5,4,4.5,5,5.5,6,10,20] #hundred percent  %\n",
        "NB_TIMES = [0.1,0.5,1,1.5,2,3,4,5]      #new part of sample\n",
        "print('base_line for PERCENT_NOISE#######################################################################################################################################')\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
        "historys_CNN_PERCENT_NOISE.append(run_CNN(X_train,y_train,X_test,y_test))\n",
        "for i in PERCENT_NOISE:\n",
        "    print('spliting data...')\n",
        "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
        "    X_train,y_train = creat_data(X_train,y_train,PERCENT_NOISE=i,NB_TIMES=0.2) \n",
        "    historys_CNN_PERCENT_NOISE.append(run_CNN(X_train,y_train,X_test,y_test))\n",
        "print('base_line for NB_CLASSES###########################################################################################################################################')\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
        "historys_CNN_NB_TIMES.append(run_CNN(X_train,y_train,X_test,y_test))\n",
        "for i in NB_TIMES:\n",
        "    print('spliting data...')\n",
        "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
        "    X_train,y_train = creat_data(X_train,y_train,PERCENT_NOISE=0.2,NB_TIMES=i) \n",
        "    historys_CNN_NB_TIMES.append(run_CNN(X_train,y_train,X_test,y_test))\n",
        "with open('NOISE_ADDED_historys_CNN_PERCENT_NOISE.pkl','wb') as handle:\n",
        "  pickle.dump(historys_CNN_PERCENT_NOISE,handle)\n",
        "with open('NOISE_ADDED_historys_CNN_NB_TIMES.pkl','wb') as handle:\n",
        "  pickle.dump(historys_CNN_NB_TIMES,handle) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "base_line for PERCENT_NOISE#######################################################################################################################################\n",
            "Pad sequences to  5000\n",
            "the shape of x_train (15511, 5000, 1)\n",
            "the shape of y_train (15511, 109)\n",
            "the shape of x_test (6648, 5000, 1)\n",
            "the shape of y_test (6648, 109)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "6648/6648 [==============================] - 2s 330us/step\n",
            "spliting data...\n",
            "3103 samples has beed add to X_train, 0.1% of noise for each sample\n",
            "Pad sequences to  5000\n",
            "the shape of x_train (18614, 5000, 1)\n",
            "the shape of y_train (18614, 109)\n",
            "the shape of x_test (6648, 5000, 1)\n",
            "the shape of y_test (6648, 109)\n",
            "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "6648/6648 [==============================] - 2s 330us/step\n",
            "spliting data...\n",
            "3103 samples has beed add to X_train, 0.5% of noise for each sample\n",
            "Pad sequences to  5000\n",
            "the shape of x_train (18614, 5000, 1)\n",
            "the shape of y_train (18614, 109)\n",
            "the shape of x_test (6648, 5000, 1)\n",
            "the shape of y_test (6648, 109)\n",
            "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ah_JCMAUj_Xh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#LSTM           ############# test for noise adding  , function :: creat_data############\n",
        "BATCH_SIZE = 64\n",
        "NB_EPOCH = 50\n",
        "VALIDATION_SPLIT = 0.3\n",
        "VERBOSE = 1\n",
        "OPTIMIZER = Adamax()\n",
        "historys_LSTM_PERCENT_NOISE = []\n",
        "historys_LSTM_NB_TIMES = []\n",
        "PERCENT_NOISE = [0.1,0.5,1,1.5,2,2.5,3,3.5,4,4.5,5,5.5,6,10,20] #hundred percent  %\n",
        "NB_TIMES = [0.1,0.5,1,1.5,2,3,4,5]      #new part of sample\n",
        "print('base_line for PERCENT_NOISE#######################################################################################################################################')\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
        "historys_LSTM_PERCENT_NOISE.append(run_LSTM(X_train,y_train,X_test,y_test))\n",
        "for i in PERCENT_NOISE:\n",
        "    print('spliting data...')\n",
        "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
        "    X_train,y_train = creat_data(X_train,y_train,PERCENT_NOISE=i,NB_TIMES=0.2) \n",
        "    historys_LSTM_PERCENT_NOISE.append(run_LSTM(X_train,y_train,X_test,y_test))\n",
        "print('base_line for NB_CLASSES###########################################################################################################################################')\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
        "historys_LSTM_NB_TIMES.append(run_LSTM(X_train,y_train,X_test,y_test))\n",
        "for i in NB_TIMES:\n",
        "    print('spliting data...')\n",
        "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
        "    X_train,y_train = creat_data(X_train,y_train,PERCENT_NOISE=0.2,NB_TIMES=i) \n",
        "    historys_LSTM_NB_TIMES.append(run_LSTM(X_train,y_train,X_test,y_test))\n",
        "with open('NOISE_ADDED_historys_LSTM_PERCENT_NOISE.pkl','wb') as handle:\n",
        "  pickle.dump(historys_LSTM_PERCENT_NOISE,handle)\n",
        "with open('NOISE_ADDED_historys_LSTM_NB_TIMES.pkl','wb') as handle:\n",
        "  pickle.dump(historys_LSTM_NB_TIMES,handle) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFReqQ7kkdcK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CNN_LSTM           ############# test for noise adding  , function :: creat_data############\n",
        "BATCH_SIZE = 64\n",
        "NB_EPOCH = 50\n",
        "VALIDATION_SPLIT = 0.3\n",
        "VERBOSE = 1\n",
        "OPTIMIZER = Adamax()\n",
        "historys_CNN_LSTM_PERCENT_NOISE = []\n",
        "historys_CNN_LSTM_NB_TIMES = []\n",
        "PERCENT_NOISE = [0.1,0.5,1,1.5,2,2.5,3,3.5,4,4.5,5,5.5,6,10,20] #hundred percent  %\n",
        "NB_TIMES = [0.1,0.5,1,1.5,2,3,4,5]      #new part of sample\n",
        "print('base_line for PERCENT_NOISE#######################################################################################################################################')\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
        "historys_CNN_LSTM_PERCENT_NOISE.append(run_CNN_LSTM(X_train,y_train,X_test,y_test))\n",
        "for i in PERCENT_NOISE:\n",
        "    print('spliting data...')\n",
        "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
        "    X_train,y_train = creat_data(X_train,y_train,PERCENT_NOISE=i,NB_TIMES=0.2) \n",
        "    historys_CNN_LSTM_PERCENT_NOISE.append(run_CNN_LSTM(X_train,y_train,X_test,y_test))\n",
        "print('base_line for NB_CLASSES###########################################################################################################################################')\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
        "historys_CNN_LSTM_NB_TIMES.append(run_CNN_LSTM(X_train,y_train,X_test,y_test))\n",
        "for i in NB_TIMES:\n",
        "    print('spliting data...')\n",
        "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
        "    X_train,y_train = creat_data(X_train,y_train,PERCENT_NOISE=0.2,NB_TIMES=i) \n",
        "    historys_CNN_LSTM_NB_TIMES.append(run_CNN_LSTM(X_train,y_train,X_test,y_test))\n",
        "with open('NOISE_ADDED_historys_CNN_LSTM_PERCENT_NOISE.pkl','wb') as handle:\n",
        "  pickle.dump(historys_CNN_LSTM_PERCENT_NOISE,handle)\n",
        "with open('NOISE_ADDED_historys_CNN_LSTM_NB_TIMES.pkl','wb') as handle:\n",
        "  pickle.dump(historys_CNN_LSTM_NB_TIMES,handle) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmJjFI9gkmPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aklsdjfldsalkfjasdlkf\n",
        "故意报错停止"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZkPsw1S2SZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(30,10))\n",
        "labels_NB_TIMES = NB_TIMES.copy()\n",
        "labels_NB_TIMES.insert(0,0)\n",
        "for history,label in zip(historys_DNN_NB_TIMES,labels_NB_TIMES):\n",
        "    print(np.mean(history[-6:-1]),label)\n",
        "    plt.title('Result Analysis')\n",
        "    plt.plot(range(len(history)),history,label='P'+str(label))\n",
        "    [plt.annotate(label,(i,history[i])) for i in range(len(history))]\n",
        "    plt.legend() # 显示图例\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('acc')\n",
        "plt.show()\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X09FNfzf-K0k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "plt.figure(figsize=(30,10))\n",
        "labels_PERCENT_NOISE = PERCENT_NOISE.copy()\n",
        "labels_PERCENT_NOISE.insert(0,0)\n",
        "for history,label in zip(historys_DNN_PERCENT_NOISE,labels_PERCENT_NOISE):\n",
        "    print(np.mean(history[-6:-1]),label)\n",
        "    plt.title('Result Analysis')\n",
        "    plt.plot(range(len(history)),history,label='P'+str(label))\n",
        "    [plt.annotate(label,(i,history[i])) for i in range(len(history))]\n",
        "    plt.legend() # 显示图例\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('acc')\n",
        "plt.show()\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhoWRsiS95Fh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(30,10))\n",
        "labels_NB_TIMES = NB_TIMES.copy()\n",
        "labels_NB_TIMES.insert(0,0)\n",
        "for history,label in zip(historys_CNN_NB_TIMES,labels_NB_TIMES):\n",
        "    print(np.mean(history[-6:-1]),label)\n",
        "    plt.title('Result Analysis')\n",
        "    plt.plot(range(len(history)),history,label='P'+str(label))\n",
        "    [plt.annotate(label,(i,history[i])) for i in range(len(history))]\n",
        "    plt.legend() # 显示图例\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('acc')\n",
        "plt.show()\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ct7ddkVv2fsO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "plt.figure(figsize=(30,10))\n",
        "labels_PERCENT_NOISE = PERCENT_NOISE.copy()\n",
        "labels_PERCENT_NOISE.insert(0,0)\n",
        "for history,label in zip(historys_CNN_PERCENT_NOISE,labels_PERCENT_NOISE):\n",
        "    print(np.mean(history[-6:-1]),label)\n",
        "    plt.title('Result Analysis')\n",
        "    plt.plot(range(len(history)),history,label='P'+str(label))\n",
        "    [plt.annotate(label,(i,history[i])) for i in range(len(history))]\n",
        "    plt.legend() # 显示图例\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('acc')\n",
        "plt.show()\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASOofuK6U_Ps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5fGfE7yYekt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Rv1aH55UQFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbDvHvuPSLTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE9KpCI5Jdhl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY9Pt18bJtjD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}