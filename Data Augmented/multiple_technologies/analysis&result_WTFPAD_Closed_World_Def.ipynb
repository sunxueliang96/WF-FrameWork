{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multiple_technologies.ipynb（副本）",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNTxo6vWJLH0nvEoUeOe9Z2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunxueliang96/WF-FrameWork/blob/master/Data%20Augmented/multiple_technologies/analysis%26result_WTFPAD_Closed_World_Def.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3WCsIaUTuNV",
        "colab_type": "code",
        "outputId": "34882090-a762-4438-d76a-adcced325471",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#Mount Google Drive as folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF9tyC-TTxmk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cd /content/drive/'My Drive'/deep_firger/dataset/OpenWorld/WalkieTalkie"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmrLZ3D0oBGJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#datatype_Close = False   #Flase is Open_World, True is Closed World."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPlx2lQou8oO",
        "colab_type": "code",
        "outputId": "0f469763-c6bb-485d-96c6-6d850fcb840a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/drive/'My Drive'/deep_firger/dataset/ClosedWorld/WTFPAD"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/deep_firger/dataset/ClosedWorld/WTFPAD\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PwIP_Q9n_W1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datatype_Close = True   #Flase is Open_World, True is Closed World."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thC1b8glT6p9",
        "colab_type": "code",
        "outputId": "425e47bd-d8a0-45c7-f28c-e103d3e9810e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_test_WTFPAD.pkl   X_valid_WTFPAD.pkl  y_train_WTFPAD.pkl\n",
            "X_train_WTFPAD.pkl  y_test_WTFPAD.pkl   y_valid_WTFPAD.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhN18zUEaeas",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOpTb4DkT9db",
        "colab_type": "code",
        "outputId": "f84703d2-2261-43d2-cb56-10094b3f5861",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "print('loading data...')\n",
        "with open('X_train_WTFPAD.pkl','rb') as handle:\n",
        "    X_1 = np.array(pickle.load(handle,encoding='iso-8859-1'))\n",
        "with open('X_valid_WTFPAD.pkl','rb') as handle:\n",
        "    X_2 = np.array(pickle.load(handle,encoding='iso-8859-1'))\n",
        "with open('y_train_WTFPAD.pkl','rb') as handle:\n",
        "    y_1 = np.array(pickle.load(handle,encoding='iso-8859-1'))\n",
        "with open('y_valid_WTFPAD.pkl','rb') as handle:\n",
        "    y_2 = np.array(pickle.load(handle,encoding='iso-8859-1'))\n",
        "\n",
        "print('the shape of X_1',X_1.shape)\n",
        "print('the shape of X_2',X_2.shape)\n",
        "print('the shape of y_1',y_1.shape)\n",
        "print('the shape of y_2',y_2.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading data...\n",
            "the shape of X_1 (76000, 5000)\n",
            "the shape of X_2 (9500, 5000)\n",
            "the shape of y_1 (76000,)\n",
            "the shape of y_2 (9500,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu7zy9dEbJMj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = []\n",
        "y = []\n",
        "X.extend(X_1)\n",
        "X.extend(X_2)\n",
        "y.extend(y_1)\n",
        "y.extend(y_2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbCJWUGzbRwr",
        "colab_type": "code",
        "outputId": "4d951c18-4ba2-4d26-f13c-56962e18841e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "k = 0 \n",
        "for seq,i in zip(X,range(len(X))):\n",
        "    X[i] = list(filter(lambda x:x!=0, seq))\n",
        "    if(i in list(range(0,len(X),len(X)//10))):\n",
        "        print(\"{}0% data has been dealed\".format(k),flush = True)\n",
        "        k +=1\n",
        "y = [-1 if x==100 else x for x in y]\n",
        "print ('shape of X',np.shape(X))\n",
        "print ('shape of y',np.shape(y))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "00% data has been dealed\n",
            "10% data has been dealed\n",
            "20% data has been dealed\n",
            "30% data has been dealed\n",
            "40% data has been dealed\n",
            "50% data has been dealed\n",
            "60% data has been dealed\n",
            "70% data has been dealed\n",
            "80% data has been dealed\n",
            "90% data has been dealed\n",
            "shape of X (85500,)\n",
            "shape of y (85500,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBm0gBfZZ_UG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math \n",
        "import random\n",
        "def fun_noise(X,y,PERCENT_NOISE,TIMES_NOISE): \n",
        "    X = list(X)\n",
        "    y = list(y)\n",
        "    X_sum = []\n",
        "    y_sum = []\n",
        "    #X_sum = X.copy()\n",
        "    #y_sum = y.copy()\n",
        "    #print(\"{} samples has beed add to X_train, {}% of noise for each sample\".format(math.ceil(len(X)*TIMES_NOISE),PERCENT_NOISE))\n",
        "    k=0\n",
        "    for i in range(math.ceil(len(X)*TIMES_NOISE)): #ADDING NOISE from here \n",
        "        if(i in list(range(0,math.ceil(len(X)*TIMES_NOISE),math.ceil(len(X)*TIMES_NOISE)//10))):\n",
        "            print(\"{}0% data has been dealed\".format(k),flush = True)\n",
        "            k +=1\n",
        "        while(True):\n",
        "            p = random.choice(range(len(X)))\n",
        "            if(y[p]!=-1):\n",
        "                break\n",
        "            else:\n",
        "                pass\n",
        "        X_new = X[p].copy()\n",
        "        y_new = y[p]\n",
        "        for n in range(math.ceil(PERCENT_NOISE*len(X_new)/100)):#dealing X_new\n",
        "            noise = random.choice([1,-1])       #noise could only be 1 or -1\n",
        "            pos = random.choice(range(len(X_new)))  #position belong to (0,len(sequence))\n",
        "            X_new.insert(pos,noise)\n",
        "            #print('insert {} at position {} in sequence {} now len of sequence is {}, the tag of sequence is {}'.format(noise,pos,p,len(X_new),y_new)) #for test\n",
        "        X_sum.append(X_new)\n",
        "        y_sum.append(y_new)\n",
        "    return X_sum,y_sum\n",
        "def fun_delete(X,y,PERCENT_DELETE,TIMES_DELETE): \n",
        "    X = list(X)\n",
        "    y = list(y)\n",
        "    X_sum = []\n",
        "    y_sum = []\n",
        "    #X_sum = X.copy()\n",
        "    #y_sum = y.copy()\n",
        "    k = 0\n",
        "    for i in range(math.ceil(len(X)*TIMES_DELETE)): #RANDOM DELETE from here\n",
        "        if(i in list(range(0,math.ceil(len(X)*TIMES_DELETE),math.ceil(len(X)*TIMES_DELETE)//10))):\n",
        "            print(\"{}0% data has been dealed\".format(k),flush = True)\n",
        "            k +=1\n",
        "        while(True):\n",
        "            p = random.choice(range(len(X)))\n",
        "            if(y[p]!=-1):\n",
        "                break\n",
        "            else:\n",
        "                #print('-1 detected')\n",
        "                pass\n",
        "        X_new = X[p].copy()\n",
        "        y_new = y[p]\n",
        "        for n in range(math.ceil(PERCENT_DELETE*len(X_new)/100)):#dealing X_new\n",
        "            pos = random.choice(range(len(X_new)))  #position belong to (0,len(sequence))\n",
        "            del X_new[pos]\n",
        "            #print('insert {} at position {} in sequence {} now len of sequence is {}'.format(noise,pos,p,len(X_new))) #for test\n",
        "        X_sum.append(X_new)\n",
        "        y_sum.append(y_new)\n",
        "    return X_sum,y_sum\n",
        "def fun_shuffle(X,y,PERCENT_CHANGED,TIMES_SHUFFLE): \n",
        "    X = list(X)\n",
        "    y = list(y)\n",
        "    #X_sum = X.copy()\n",
        "    #y_sum = y.copy()\n",
        "    X_sum = []\n",
        "    y_sum = []\n",
        "    k=0\n",
        "    for i in range(math.ceil(len(X)*TIMES_SHUFFLE)):             #LOCALLY CHANGE from here\n",
        "        if(i in list(range(0,math.ceil(len(X)*TIMES_SHUFFLE),math.ceil(len(X)*TIMES_SHUFFLE)//10))):\n",
        "            print(\"{}0% data has been dealed\".format(k),flush = True)\n",
        "            k +=1\n",
        "        while(True):\n",
        "            p = random.choice(range(len(X)))\n",
        "            if(y[p]!=-1):\n",
        "                break\n",
        "            else:\n",
        "                #print('-1 detected')\n",
        "                pass\n",
        "        X_new = X[p].copy()\n",
        "        y_new = y[p]\n",
        "        length = math.ceil(PERCENT_CHANGED*len(X_new)/100)\n",
        "        start = random.choice(range(len(X_new)-length))\n",
        "        temp = X_new[start:start+length]\n",
        "        random.shuffle(temp)\n",
        "        X_new[start:start+length] = temp\n",
        "\n",
        "        X_sum.append(X_new)\n",
        "        y_sum.append(y_new)\n",
        "    return X_sum,y_sum\n",
        "def fun_transplant(X,y,PERCENT_TRANSPLANT,NB_TIMES):        #Magnificate dataset by adding noising randomly\n",
        "    global datatype_Close\n",
        "    X = list(X)\n",
        "    y = list(y)\n",
        "    X_sum = []\n",
        "    y_sum = []\n",
        "    #X_sum = X.copy()\n",
        "    #y_sum = y.copy()\n",
        "    #print(\"{} samples has beed add to X_train, {}% of noise for each sample\".format(math.ceil(len(X)*NB_TIMES),PERCENT_TRANSPLANT))\n",
        "    k=0\n",
        "    print('When the process freezing, please check that datatype_Close has been set correctly')\n",
        "    for i in range(math.ceil(len(X)*NB_TIMES)):\n",
        "        if(i in list(range(0,math.ceil(len(X)*NB_TIMES),math.ceil(len(X)*NB_TIMES)//10))):\n",
        "            print(\"{}0% data has been dealed\".format(k),flush = True)\n",
        "            k +=1\n",
        "        #print(datatype_Close)    \n",
        "        while(datatype_Close):                               # target pos p (sensitive website), pos q (non-sensitive website)\n",
        "            p = random.choice(range(len(X)))\n",
        "            q = random.choice(range(len(X)))\n",
        "            if(p != q):                           # Closed_World\n",
        "                break\n",
        "            else:\n",
        "                #print('-1 detected')\n",
        "                pass\n",
        "        while(not datatype_Close):                               # target pos p (sensitive website), pos q (non-sensitive website)\n",
        "            p = random.choice(range(len(X)))\n",
        "            q = random.choice(range(len(X)))\n",
        "            if(y[p]!=-1 and y[q]==-1):               # open_world\n",
        "                break\n",
        "            else:\n",
        "                #print('-1 detected')\n",
        "                pass\n",
        "        X_new = X[p].copy()\n",
        "        target = X[q].copy()\n",
        "        y_new = y[p]\n",
        "\n",
        "        length_X = math.ceil(PERCENT_TRANSPLANT*len(X_new)/100)\n",
        "        length_target =  math.ceil(PERCENT_TRANSPLANT*len(target)/100)\n",
        "        start_X = random.choice(range(len(X_new)-length_X))\n",
        "        start_target = random.choice(range(len(target)-length_target))\n",
        "        temp = target[start_target:start_target+length_target]\n",
        "        X_new[start_X:start_X+length_X] = temp\n",
        "     \n",
        "        X_sum.append(X_new)\n",
        "        y_sum.append(y_new)\n",
        "    return X_sum,y_sum\n",
        "\n",
        "\n",
        "def fun_mixup(X,y,TIMES_mixup):\n",
        "    global datatype_Close\n",
        "    X = list(X)\n",
        "    y = list(y)\n",
        "    X_sum = []\n",
        "    y_sum = []\n",
        "    #X_sum = X.copy()\n",
        "    #y_sum = y.copy()\n",
        "    #print(\"{} samples has beed add to X_train, {}% of noise for each sample\".format(math.ceil(len(X)*NB_TIMES),PERCENT_TRANSPLANT))\n",
        "    for i in range(math.ceil(len(X)*NB_TIMES)):  \n",
        "        while(datatype_Close):                               # target pos p (sensitive website), pos q (non-sensitive website)\n",
        "            p = random.choice(range(len(X)))\n",
        "            q = random.choice(range(len(X)))\n",
        "            if(p != q):                           # Closed_World\n",
        "                break\n",
        "            else:\n",
        "                #print('-1 detected')\n",
        "                pass\n",
        "        while(not datatype_Close):                               # target pos p (sensitive website), pos q (non-sensitive website)\n",
        "            p = random.choice(range(len(X)))\n",
        "            q = random.choice(range(len(X)))\n",
        "            if(y[p]!=-1 and y[q]==-1):               # open_world\n",
        "                break\n",
        "            else:\n",
        "                #print('-1 detected')\n",
        "                pass\n",
        "        X_new = X[p].copy()\n",
        "        target = X[q].copy()\n",
        "        y_new = y[p]\n",
        "        X_temp = []\n",
        "        for n,t in zip(X_new,target):\n",
        "            temp = n*t\n",
        "            X_temp.append(temp)\n",
        "        X_sum.append(X_temp)\n",
        "        y_sum.append(y_new)\n",
        "    return X_sum,y_sum\n",
        "\n",
        "def random_launch(X,y,NB_tech):\n",
        "    X = list(X)\n",
        "    y = list(y)\n",
        "    #X_sum = X.copy()\n",
        "    #y_sum = y.copy()\n",
        "    X_sum = []\n",
        "    y_sum = []\n",
        "    if (NB_tech==0):\n",
        "        print('adding noise......')\n",
        "        X_new,y_new = fun_noise(X,y,PERCENT_NOISE=40,TIMES_NOISE=3)\n",
        "        X_sum.extend(X_new)\n",
        "        y_sum.extend(y_new)\n",
        "    elif (NB_tech==1):\n",
        "        print('deleting......')\n",
        "        X_new,y_new = fun_delete(X,y,PERCENT_DELETE=40,TIMES_DELETE=3)\n",
        "        X_sum.extend(X_new)\n",
        "        y_sum.extend(y_new)\n",
        "    elif (NB_tech==2):\n",
        "        print('shuffling......')\n",
        "        X_new,y_new = fun_shuffle(X,y,PERCENT_CHANGED=60,TIMES_SHUFFLE=3)\n",
        "        X_sum.extend(X_new)\n",
        "        y_sum.extend(y_new)\n",
        "    elif (NB_tech==3):\n",
        "        print('transplanting......')\n",
        "        X_new,y_new = fun_transplant(X,y,PERCENT_TRANSPLANT=60,NB_TIMES=3)\n",
        "        X_sum.extend(X_new)\n",
        "        y_sum.extend(y_new)\n",
        "    elif (NB_tech==4):\n",
        "        print('mixup......')\n",
        "        X_new,y_new = fun_transplant(X,y,PERCENT_TRANSPLANT=40,NB_TIMES=3)\n",
        "        X_sum.extend(X_new)\n",
        "        y_sum.extend(y_new)\n",
        "    return np.array(X_sum),np.array(y_sum)                        #return two array of X, y \n",
        "\n",
        "NB_TECHS = 5 # set Number of techs here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZpMmHj3Uefl",
        "colab_type": "code",
        "outputId": "43332165-bd95-43e9-c218-bf57e67a1ca0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import sequence\n",
        "from collections import Counter\n",
        "#CLASSES_KNN = 101########################################################################################################################################################################################################\n",
        "#CLASSES_WAKIE = 100########################################################################################################################################################################################################\n",
        "MAXLEN_KNN = 2000\n",
        "MAXLEN_WAKIE =5000\n",
        "\n",
        "maxlen = 5000\n",
        "NB_CLASSES = 0\n",
        "\n",
        "def choose():\n",
        "    global maxlen,NB_CLASSES\n",
        "    print('if padding, the max of length of seq is {}'.format(maxlen))\n",
        "    NB_CLASSES = len(Counter(y).keys())\n",
        "    print('number of classes is {}'.format(NB_CLASSES))\n",
        "\n",
        "choose()################################################################################################################################################################################################################\n",
        "\n",
        "print('Average sequence length: {}'.format(np.mean(list(map(len, X)), dtype=int)))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "if padding, the max of length of seq is 5000\n",
            "number of classes is 95\n",
            "Average sequence length: 2705\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-wfFtrwvnAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#MLP\n",
        "from keras import Sequential\n",
        "from keras.layers.core import Activation, Flatten, Dense, Dropout\n",
        "from keras.optimizers import Adamax\n",
        "def MLP(input_shape,classes):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_shape=(input_shape,)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(256))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(512))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(1024))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(512))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(classes))\n",
        "    model.add(Activation('softmax'))\n",
        "    return model\n",
        "def run_MLP(X_train,y_train,X_test,y_test,X_temp,y_temp,BATCH_SIZE,history):\n",
        "    for i in range(NB_EPOCHS):\n",
        "        if (i<=NB_EPOCHS/5):\n",
        "            log = model_MLP.fit(X_train,y_train,batch_size=BATCH_SIZE,epochs=1,validation_data=(X_test,y_test),verbose=1)\n",
        "            history.recorder(log)\n",
        "        elif (NB_EPOCHS/5<i and i%2==1 and i<(NB_EPOCHS*4/5)):\n",
        "            log = model_MLP.fit(X_temp,y_temp,batch_size=BATCH_SIZE,epochs=1,validation_data=(X_test,y_test),verbose=1)\n",
        "            history.recorder(log)\n",
        "        elif (NB_EPOCHS/5<i and i%2==0 and i<(NB_EPOCHS*4/5)):\n",
        "            log = model_MLP.fit(X_train,y_train,batch_size=BATCH_SIZE,epochs=1,validation_data=(X_test,y_test),verbose=1)\n",
        "            history.recorder(log)\n",
        "        else:\n",
        "            log = model_MLP.fit(X_train,y_train,batch_size=BATCH_SIZE,epochs=1,validation_data=(X_test,y_test),verbose=1)\n",
        "            history.recorder(log)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_gm7tqd4yEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#FCN\n",
        "from keras import Sequential\n",
        "from keras.layers.core import Activation, Flatten, Dense, Dropout\n",
        "from keras.layers import GlobalAveragePooling1D\n",
        "from keras.optimizers import Adamax\n",
        "def DNN(input_shape,classes):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_shape=(input_shape,)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(256))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(512))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(1024))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(512))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "   # model.add(GlobalAveragePooling1D()) #parimatier is not defined\n",
        "    model.add(Dense(classes))\n",
        "    model.add(Activation('softmax'))\n",
        "    return model\n",
        "def run_DNN(X_train,y_train,X_test,y_test,X_temp,y_temp,BATCH_SIZE,history):\n",
        "    for i in range(NB_EPOCHS):\n",
        "        if (i<=NB_EPOCHS/5):\n",
        "            log = model_DNN.fit(X_train,y_train,batch_size=BATCH_SIZE,epochs=1,validation_data=(X_test,y_test),verbose=1)\n",
        "            history.recorder(log)\n",
        "        elif (NB_EPOCHS/5<i and i%2==1 and i<(NB_EPOCHS*4/5)):\n",
        "            log = model_DNN.fit(X_temp,y_temp,batch_size=BATCH_SIZE,epochs=1,validation_data=(X_test,y_test),verbose=1)\n",
        "            history.recorder(log)\n",
        "        elif (NB_EPOCHS/5<i and i%2==0 and i<(NB_EPOCHS*4/5)):\n",
        "            log = model_DNN.fit(X_train,y_train,batch_size=BATCH_SIZE,epochs=1,validation_data=(X_test,y_test),verbose=1)\n",
        "            history.recorder(log)\n",
        "        else:\n",
        "            log = model_DNN.fit(X_train,y_train,batch_size=BATCH_SIZE,epochs=1,validation_data=(X_test,y_test),verbose=1)\n",
        "            history.recorder(log)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9hQ5LZK6sFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CNN\n",
        "from keras import Input,Model,Sequential\n",
        "from keras.layers import Embedding,GlobalAveragePooling1D,Dense,Dropout\n",
        "from keras.layers import Conv1D, MaxPooling1D, BatchNormalization\n",
        "from keras.layers.core import Activation, Flatten, Dense, Dropout\n",
        "from keras.layers.advanced_activations import ELU\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras.optimizers import Adamax\n",
        "def CNN(input_shape, classes):\n",
        "    model = Sequential()\n",
        "    #Block1\n",
        "    filter_num = ['None',32,64,128,256]\n",
        "    kernel_size = ['None',8,8,8,8]\n",
        "    conv_stride_size = ['None',1,1,1,1]\n",
        "    pool_stride_size = ['None',4,4,4,4]\n",
        "    pool_size = ['None',8,8,8,8]\n",
        "\n",
        "    model.add(Conv1D(filters=filter_num[1], kernel_size=kernel_size[1], input_shape=(input_shape,1),\n",
        "                      strides=conv_stride_size[1], padding='same',\n",
        "                      name='block1_conv1'))\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "    model.add(ELU(alpha=1.0, name='block1_adv_act1'))\n",
        "    model.add(Conv1D(filters=filter_num[1], kernel_size=kernel_size[1],\n",
        "                      strides=conv_stride_size[1], padding='same',\n",
        "                      name='block1_conv2'))\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "    model.add(ELU(alpha=1.0, name='block1_adv_act2'))\n",
        "    model.add(MaxPooling1D(pool_size=pool_size[1], strides=pool_stride_size[1],\n",
        "                            padding='same', name='block1_pool'))\n",
        "    model.add(Dropout(0.1, name='block1_dropout'))\n",
        "\n",
        "    model.add(Conv1D(filters=filter_num[2], kernel_size=kernel_size[2],\n",
        "                      strides=conv_stride_size[2], padding='same',\n",
        "                      name='block2_conv1'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu', name='block2_act1'))\n",
        "\n",
        "    model.add(Conv1D(filters=filter_num[2], kernel_size=kernel_size[2],\n",
        "                      strides=conv_stride_size[2], padding='same',\n",
        "                      name='block2_conv2'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu', name='block2_act2'))\n",
        "    model.add(MaxPooling1D(pool_size=pool_size[2], strides=pool_stride_size[3],\n",
        "                            padding='same', name='block2_pool'))\n",
        "    model.add(Dropout(0.1, name='block2_dropout'))\n",
        "\n",
        "    model.add(Conv1D(filters=filter_num[3], kernel_size=kernel_size[3],\n",
        "                      strides=conv_stride_size[3], padding='same',\n",
        "                      name='block3_conv1'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu', name='block3_act1'))\n",
        "    model.add(Conv1D(filters=filter_num[3], kernel_size=kernel_size[3],\n",
        "                      strides=conv_stride_size[3], padding='same',\n",
        "                      name='block3_conv2'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu', name='block3_act2'))\n",
        "    model.add(MaxPooling1D(pool_size=pool_size[3], strides=pool_stride_size[3],\n",
        "                            padding='same', name='block3_pool'))\n",
        "    model.add(Dropout(0.1, name='block3_dropout'))\n",
        "\n",
        "    model.add(Conv1D(filters=filter_num[4], kernel_size=kernel_size[4],\n",
        "                      strides=conv_stride_size[4], padding='same',\n",
        "                      name='block4_conv1'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu', name='block4_act1'))\n",
        "    model.add(Conv1D(filters=filter_num[4], kernel_size=kernel_size[4],\n",
        "                      strides=conv_stride_size[4], padding='same',\n",
        "                      name='block4_conv2'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu', name='block4_act2'))\n",
        "    model.add(MaxPooling1D(pool_size=pool_size[4], strides=pool_stride_size[4],\n",
        "                            padding='same', name='block4_pool'))\n",
        "    model.add(Dropout(0.1, name='block4_dropout'))\n",
        "\n",
        "    model.add(Flatten(name='flatten'))\n",
        "    model.add(Dense(512, kernel_initializer=glorot_uniform(seed=0), name='fc1'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu', name='fc1_act'))\n",
        "\n",
        "    model.add(Dropout(0.7, name='fc1_dropout'))\n",
        "\n",
        "    model.add(Dense(512, kernel_initializer=glorot_uniform(seed=0), name='fc2'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu', name='fc2_act'))\n",
        "\n",
        "    model.add(Dropout(0.5, name='fc2_dropout'))\n",
        "\n",
        "    model.add(Dense(classes, kernel_initializer=glorot_uniform(seed=0), name='fc3'))\n",
        "    model.add(Activation('softmax', name=\"softmax\"))\n",
        "    return model\n",
        "def run_CNN(X_train,y_train,X_test,y_test,X_temp,y_temp,BATCH_SIZE,history):\n",
        "    for i in range(NB_EPOCHS):\n",
        "        if (i<=NB_EPOCHS/5):\n",
        "            log = model_CNN.fit(X_train,y_train,batch_size=BATCH_SIZE,epochs=1,validation_data=(X_test,y_test),verbose=1)\n",
        "            history.recorder(log)\n",
        "        elif (NB_EPOCHS/5<i and i%2==1 and i<(NB_EPOCHS*4/5)):\n",
        "            log = model_CNN.fit(X_temp,y_temp,batch_size=BATCH_SIZE,epochs=1,validation_data=(X_test,y_test),verbose=1)\n",
        "            history.recorder(log)\n",
        "        elif (NB_EPOCHS/5<i and i%2==0 and i<(NB_EPOCHS*4/5)):\n",
        "            log = model_CNN.fit(X_train,y_train,batch_size=BATCH_SIZE,epochs=1,validation_data=(X_test,y_test),verbose=1)\n",
        "            history.recorder(log)\n",
        "        else:\n",
        "            log = model_CNN.fit(X_train,y_train,batch_size=BATCH_SIZE,epochs=1,validation_data=(X_test,y_test),verbose=1)\n",
        "            history.recorder(log)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuVICfeHwymL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras import backend as K\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    # Calculates the precision\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    # Calculates the recall\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def fbeta_score(y_true, y_pred, beta=1):\n",
        "    # Calculates the F score, the weighted harmonic mean of precision and recall.\n",
        "    if beta < 0:\n",
        "        raise ValueError('The lowest choosable beta is zero (only precision).')\n",
        " \n",
        "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
        "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
        "        return 0\n",
        "\n",
        "    p = precision(y_true, y_pred)\n",
        "    r = recall(y_true, y_pred)\n",
        "    bb = beta ** 2\n",
        "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
        "    return fbeta_score\n",
        "\n",
        "def fmeasure(y_true, y_pred):\n",
        "    # Calculates the f-measure, the harmonic mean of precision and recall.\n",
        "    return fbeta_score(y_true, y_pred, beta=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UiYFTuaiZ_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gc\n",
        "del X_1\n",
        "del X_2\n",
        "del y_1\n",
        "del y_2\n",
        "gc.collect()\n",
        "\n",
        "class my_history():\n",
        "    def __init__(self):\n",
        "        self.history = {'loss':[],'acc':[],'top_k_categorical_accuracy':[],'precision':[],'recall':[],'fmeasure':[],'val_loss':[],'val_acc':[],'val_top_k_categorical_accuracy':[],'val_precision':[],'val_recall':[],'val_fmeasure':[]}\n",
        "    def recorder(self,history):\n",
        "        self.history['loss'].append(history.history['loss'][0])\n",
        "        self.history['acc'].append(history.history['acc'][0])\n",
        "        self.history['top_k_categorical_accuracy'].append(history.history['top_k_categorical_accuracy'][0])\n",
        "        self.history['precision'].append(history.history['precision'][0])\n",
        "        self.history['recall'].append(history.history['recall'][0])\n",
        "        self.history['fmeasure'].append(history.history['fmeasure'][0])\n",
        "        self.history['val_loss'].append(history.history['val_loss'][0])\n",
        "        self.history['val_acc'].append(history.history['val_acc'][0])\n",
        "        self.history['val_top_k_categorical_accuracy'].append(history.history['val_top_k_categorical_accuracy'][0])\n",
        "        self.history['val_precision'].append(history.history['val_precision'][0])\n",
        "        self.history['val_recall'].append(history.history['val_recall'][0])\n",
        "        self.history['val_fmeasure'].append(history.history['val_fmeasure'][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8SJXUjNvuCR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "36f4884a-0650-49de-856c-8460c82d0275"
      },
      "source": [
        "import gc\n",
        "VALIDATION_SPLIT = 0.3\n",
        "\n",
        "X_train_saved,X_test_saved,y_train_saved,y_test_saved = train_test_split(X,y,test_size=VALIDATION_SPLIT)\n",
        "del X\n",
        "del y\n",
        "print('launching  garbage collect .....')\n",
        "gc.collect()\n",
        "print('garbage collected .....')\n",
        "\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "NB_EPOCHS = 100\n",
        "\n",
        "OPTIMIZER = Adamax()\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "launching  garbage collect .....\n",
            "garbage collected .....\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEMNvc6PntbC",
        "colab_type": "code",
        "outputId": "984a146a-cde0-4de0-dfef-9721d9b5e8ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#MLP               ############# test for noise adding  , function :: creat_data############\n",
        "\n",
        "history_MLP = []\n",
        "for i in range(NB_TECHS):\n",
        "    his = my_history()\n",
        "    X_temp,y_temp = random_launch(X_train_saved,y_train_saved,i)\n",
        "    X_temp = sequence.pad_sequences(X_temp, maxlen=maxlen,padding='post',truncating='post')\n",
        "    y_temp = np_utils.to_categorical(y_temp, NB_CLASSES)\n",
        "    X_train = sequence.pad_sequences(X_train_saved, maxlen=maxlen,padding='post',truncating='post')\n",
        "    y_train = np_utils.to_categorical(y_train_saved, NB_CLASSES)\n",
        "    X_test = sequence.pad_sequences(X_test_saved, maxlen=maxlen,padding='post',truncating='post')\n",
        "    y_test = np_utils.to_categorical(y_test_saved, NB_CLASSES)\n",
        "    model_MLP = MLP(maxlen,NB_CLASSES)\n",
        "    #model_MLP.summary()\n",
        "    model_MLP.compile(loss='categorical_crossentropy',optimizer=OPTIMIZER,metrics=['accuracy','top_k_categorical_accuracy', precision, recall, fmeasure])\n",
        "    run_MLP(X_train,y_train,X_test,y_test,X_temp,y_temp,BATCH_SIZE,history = his)\n",
        "    history_MLP.append(his)\n",
        "\n",
        "\n",
        "print('the shape of x_train',X_train.shape)\n",
        "print('the shape of y_train',y_train.shape)\n",
        "print('the shape of x_test',X_test.shape)\n",
        "print('the shape of y_test',y_test.shape)\n",
        "\n",
        "print('base_line_now#######################################################################################################################################################################################################')\n",
        "model_MLP_org = MLP(maxlen,NB_CLASSES)\n",
        "model_MLP_org.compile(loss='categorical_crossentropy',optimizer=OPTIMIZER,metrics=['accuracy','top_k_categorical_accuracy', precision, recall, fmeasure])\n",
        "history_MLP_org = model_MLP_org.fit(X_train,y_train,batch_size=BATCH_SIZE,epochs=NB_EPOCHS,validation_data=(X_test,y_test),verbose=1)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adding noise......\n",
            "00% data has been dealed\n",
            "10% data has been dealed\n",
            "20% data has been dealed\n",
            "30% data has been dealed\n",
            "40% data has been dealed\n",
            "50% data has been dealed\n",
            "60% data has been dealed\n",
            "70% data has been dealed\n",
            "80% data has been dealed\n",
            "90% data has been dealed\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "59850/59850 [==============================] - 17s 282us/step - loss: 3.1316 - acc: 0.1712 - top_k_categorical_accuracy: 0.4862 - precision: 0.3793 - recall: 0.0200 - fmeasure: 0.0373 - val_loss: 2.6928 - val_acc: 0.2554 - val_top_k_categorical_accuracy: 0.6359 - val_precision: 0.5375 - val_recall: 0.0448 - val_fmeasure: 0.0816\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 2.3021 - acc: 0.3447 - top_k_categorical_accuracy: 0.7276 - precision: 0.6556 - recall: 0.1175 - fmeasure: 0.1971 - val_loss: 2.4913 - val_acc: 0.3118 - val_top_k_categorical_accuracy: 0.6906 - val_precision: 0.5546 - val_recall: 0.1186 - val_fmeasure: 0.1937\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 1.8228 - acc: 0.4624 - top_k_categorical_accuracy: 0.8251 - precision: 0.7083 - recall: 0.2584 - fmeasure: 0.3766 - val_loss: 2.5014 - val_acc: 0.3345 - val_top_k_categorical_accuracy: 0.7054 - val_precision: 0.5206 - val_recall: 0.1770 - val_fmeasure: 0.2626\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 117us/step - loss: 1.4441 - acc: 0.5657 - top_k_categorical_accuracy: 0.8844 - precision: 0.7611 - recall: 0.4054 - fmeasure: 0.5272 - val_loss: 2.6406 - val_acc: 0.3337 - val_top_k_categorical_accuracy: 0.7005 - val_precision: 0.4887 - val_recall: 0.2057 - val_fmeasure: 0.2881\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 1.1497 - acc: 0.6474 - top_k_categorical_accuracy: 0.9230 - precision: 0.7992 - recall: 0.5234 - fmeasure: 0.6311 - val_loss: 3.0002 - val_acc: 0.3227 - val_top_k_categorical_accuracy: 0.6878 - val_precision: 0.4248 - val_recall: 0.2316 - val_fmeasure: 0.2989\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 117us/step - loss: 0.9164 - acc: 0.7156 - top_k_categorical_accuracy: 0.9480 - precision: 0.8291 - recall: 0.6228 - fmeasure: 0.7102 - val_loss: 3.3204 - val_acc: 0.3246 - val_top_k_categorical_accuracy: 0.6835 - val_precision: 0.4000 - val_recall: 0.2525 - val_fmeasure: 0.3089\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.7369 - acc: 0.7688 - top_k_categorical_accuracy: 0.9641 - precision: 0.8568 - recall: 0.6984 - fmeasure: 0.7688 - val_loss: 3.7052 - val_acc: 0.3207 - val_top_k_categorical_accuracy: 0.6749 - val_precision: 0.3795 - val_recall: 0.2659 - val_fmeasure: 0.3122\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 117us/step - loss: 0.6066 - acc: 0.8088 - top_k_categorical_accuracy: 0.9742 - precision: 0.8788 - recall: 0.7543 - fmeasure: 0.8113 - val_loss: 3.9896 - val_acc: 0.3116 - val_top_k_categorical_accuracy: 0.6600 - val_precision: 0.3652 - val_recall: 0.2667 - val_fmeasure: 0.3079\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 117us/step - loss: 0.4924 - acc: 0.8430 - top_k_categorical_accuracy: 0.9817 - precision: 0.8966 - recall: 0.8017 - fmeasure: 0.8461 - val_loss: 4.2163 - val_acc: 0.3144 - val_top_k_categorical_accuracy: 0.6609 - val_precision: 0.3612 - val_recall: 0.2737 - val_fmeasure: 0.3111\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 117us/step - loss: 0.3930 - acc: 0.8746 - top_k_categorical_accuracy: 0.9876 - precision: 0.9157 - recall: 0.8432 - fmeasure: 0.8777 - val_loss: 4.8130 - val_acc: 0.3060 - val_top_k_categorical_accuracy: 0.6557 - val_precision: 0.3395 - val_recall: 0.2778 - val_fmeasure: 0.3053\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 117us/step - loss: 0.3355 - acc: 0.8927 - top_k_categorical_accuracy: 0.9913 - precision: 0.9255 - recall: 0.8687 - fmeasure: 0.8960 - val_loss: 5.0464 - val_acc: 0.3057 - val_top_k_categorical_accuracy: 0.6514 - val_precision: 0.3366 - val_recall: 0.2798 - val_fmeasure: 0.3054\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 117us/step - loss: 0.2891 - acc: 0.9078 - top_k_categorical_accuracy: 0.9939 - precision: 0.9323 - recall: 0.8861 - fmeasure: 0.9084 - val_loss: 5.2013 - val_acc: 0.3100 - val_top_k_categorical_accuracy: 0.6522 - val_precision: 0.3365 - val_recall: 0.2873 - val_fmeasure: 0.3097\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.2532 - acc: 0.9198 - top_k_categorical_accuracy: 0.9956 - precision: 0.9400 - recall: 0.9028 - fmeasure: 0.9208 - val_loss: 5.5822 - val_acc: 0.3044 - val_top_k_categorical_accuracy: 0.6424 - val_precision: 0.3270 - val_recall: 0.2869 - val_fmeasure: 0.3054\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 117us/step - loss: 0.2188 - acc: 0.9302 - top_k_categorical_accuracy: 0.9969 - precision: 0.9461 - recall: 0.9164 - fmeasure: 0.9309 - val_loss: 5.6727 - val_acc: 0.3046 - val_top_k_categorical_accuracy: 0.6430 - val_precision: 0.3268 - val_recall: 0.2876 - val_fmeasure: 0.3057\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.1973 - acc: 0.9368 - top_k_categorical_accuracy: 0.9976 - precision: 0.9490 - recall: 0.9250 - fmeasure: 0.9368 - val_loss: 5.8033 - val_acc: 0.3084 - val_top_k_categorical_accuracy: 0.6460 - val_precision: 0.3273 - val_recall: 0.2924 - val_fmeasure: 0.3087\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 117us/step - loss: 0.1685 - acc: 0.9468 - top_k_categorical_accuracy: 0.9984 - precision: 0.9568 - recall: 0.9377 - fmeasure: 0.9471 - val_loss: 5.9953 - val_acc: 0.3025 - val_top_k_categorical_accuracy: 0.6400 - val_precision: 0.3211 - val_recall: 0.2881 - val_fmeasure: 0.3036\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 117us/step - loss: 0.1489 - acc: 0.9521 - top_k_categorical_accuracy: 0.9990 - precision: 0.9602 - recall: 0.9445 - fmeasure: 0.9522 - val_loss: 6.2777 - val_acc: 0.3053 - val_top_k_categorical_accuracy: 0.6474 - val_precision: 0.3198 - val_recall: 0.2930 - val_fmeasure: 0.3057\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 117us/step - loss: 0.1368 - acc: 0.9570 - top_k_categorical_accuracy: 0.9991 - precision: 0.9632 - recall: 0.9499 - fmeasure: 0.9565 - val_loss: 6.3378 - val_acc: 0.3050 - val_top_k_categorical_accuracy: 0.6375 - val_precision: 0.3200 - val_recall: 0.2932 - val_fmeasure: 0.3059\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 117us/step - loss: 0.1227 - acc: 0.9619 - top_k_categorical_accuracy: 0.9994 - precision: 0.9668 - recall: 0.9565 - fmeasure: 0.9616 - val_loss: 6.4401 - val_acc: 0.3049 - val_top_k_categorical_accuracy: 0.6449 - val_precision: 0.3190 - val_recall: 0.2945 - val_fmeasure: 0.3062\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.1194 - acc: 0.9629 - top_k_categorical_accuracy: 0.9993 - precision: 0.9673 - recall: 0.9583 - fmeasure: 0.9627 - val_loss: 6.4932 - val_acc: 0.3020 - val_top_k_categorical_accuracy: 0.6430 - val_precision: 0.3157 - val_recall: 0.2909 - val_fmeasure: 0.3027\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 117us/step - loss: 0.1106 - acc: 0.9654 - top_k_categorical_accuracy: 0.9997 - precision: 0.9696 - recall: 0.9612 - fmeasure: 0.9653 - val_loss: 6.6897 - val_acc: 0.3084 - val_top_k_categorical_accuracy: 0.6409 - val_precision: 0.3206 - val_recall: 0.2993 - val_fmeasure: 0.3095\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 105us/step - loss: 2.6939 - acc: 0.2780 - top_k_categorical_accuracy: 0.6403 - precision: 0.5566 - recall: 0.0777 - fmeasure: 0.1335 - val_loss: 3.8392 - val_acc: 0.1750 - val_top_k_categorical_accuracy: 0.4957 - val_precision: 0.2209 - val_recall: 0.0712 - val_fmeasure: 0.1070\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.8750 - acc: 0.7290 - top_k_categorical_accuracy: 0.9541 - precision: 0.8151 - recall: 0.6496 - fmeasure: 0.7161 - val_loss: 3.7255 - val_acc: 0.3504 - val_top_k_categorical_accuracy: 0.7037 - val_precision: 0.4007 - val_recall: 0.3150 - val_fmeasure: 0.3523\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 104us/step - loss: 2.1171 - acc: 0.3944 - top_k_categorical_accuracy: 0.7693 - precision: 0.6591 - recall: 0.1857 - fmeasure: 0.2871 - val_loss: 3.8129 - val_acc: 0.1913 - val_top_k_categorical_accuracy: 0.5158 - val_precision: 0.2527 - val_recall: 0.0958 - val_fmeasure: 0.1381\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 120us/step - loss: 0.9024 - acc: 0.7176 - top_k_categorical_accuracy: 0.9544 - precision: 0.8079 - recall: 0.6372 - fmeasure: 0.7077 - val_loss: 3.4720 - val_acc: 0.3686 - val_top_k_categorical_accuracy: 0.7246 - val_precision: 0.4215 - val_recall: 0.3260 - val_fmeasure: 0.3672\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 104us/step - loss: 1.7827 - acc: 0.4770 - top_k_categorical_accuracy: 0.8325 - precision: 0.7040 - recall: 0.2889 - fmeasure: 0.4073 - val_loss: 4.0130 - val_acc: 0.2021 - val_top_k_categorical_accuracy: 0.5269 - val_precision: 0.2553 - val_recall: 0.1170 - val_fmeasure: 0.1599\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 122us/step - loss: 0.8331 - acc: 0.7374 - top_k_categorical_accuracy: 0.9612 - precision: 0.8198 - recall: 0.6663 - fmeasure: 0.7317 - val_loss: 3.4442 - val_acc: 0.3792 - val_top_k_categorical_accuracy: 0.7356 - val_precision: 0.4307 - val_recall: 0.3387 - val_fmeasure: 0.3787\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 104us/step - loss: 1.5305 - acc: 0.5414 - top_k_categorical_accuracy: 0.8745 - precision: 0.7363 - recall: 0.3755 - fmeasure: 0.4952 - val_loss: 4.0899 - val_acc: 0.2006 - val_top_k_categorical_accuracy: 0.5380 - val_precision: 0.2512 - val_recall: 0.1281 - val_fmeasure: 0.1692\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 117us/step - loss: 0.7654 - acc: 0.7567 - top_k_categorical_accuracy: 0.9671 - precision: 0.8287 - recall: 0.6959 - fmeasure: 0.7541 - val_loss: 3.5995 - val_acc: 0.3765 - val_top_k_categorical_accuracy: 0.7304 - val_precision: 0.4213 - val_recall: 0.3400 - val_fmeasure: 0.3760\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 104us/step - loss: 1.3217 - acc: 0.5960 - top_k_categorical_accuracy: 0.9046 - precision: 0.7635 - recall: 0.4553 - fmeasure: 0.5685 - val_loss: 4.5949 - val_acc: 0.1953 - val_top_k_categorical_accuracy: 0.5173 - val_precision: 0.2401 - val_recall: 0.1388 - val_fmeasure: 0.1754\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 117us/step - loss: 0.6986 - acc: 0.7774 - top_k_categorical_accuracy: 0.9722 - precision: 0.8396 - recall: 0.7220 - fmeasure: 0.7743 - val_loss: 3.7092 - val_acc: 0.3857 - val_top_k_categorical_accuracy: 0.7371 - val_precision: 0.4265 - val_recall: 0.3514 - val_fmeasure: 0.3850\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 103us/step - loss: 1.1438 - acc: 0.6459 - top_k_categorical_accuracy: 0.9281 - precision: 0.7879 - recall: 0.5250 - fmeasure: 0.6286 - val_loss: 4.9157 - val_acc: 0.1966 - val_top_k_categorical_accuracy: 0.5182 - val_precision: 0.2329 - val_recall: 0.1453 - val_fmeasure: 0.1785\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 116us/step - loss: 0.6527 - acc: 0.7903 - top_k_categorical_accuracy: 0.9763 - precision: 0.8467 - recall: 0.7433 - fmeasure: 0.7900 - val_loss: 3.8671 - val_acc: 0.3793 - val_top_k_categorical_accuracy: 0.7357 - val_precision: 0.4213 - val_recall: 0.3530 - val_fmeasure: 0.3839\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 18s 103us/step - loss: 0.9923 - acc: 0.6887 - top_k_categorical_accuracy: 0.9451 - precision: 0.8101 - recall: 0.5863 - fmeasure: 0.6788 - val_loss: 5.2357 - val_acc: 0.1993 - val_top_k_categorical_accuracy: 0.5155 - val_precision: 0.2356 - val_recall: 0.1580 - val_fmeasure: 0.1888\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 117us/step - loss: 0.5983 - acc: 0.8079 - top_k_categorical_accuracy: 0.9800 - precision: 0.8566 - recall: 0.7670 - fmeasure: 0.8081 - val_loss: 4.0222 - val_acc: 0.3837 - val_top_k_categorical_accuracy: 0.7330 - val_precision: 0.4190 - val_recall: 0.3575 - val_fmeasure: 0.3855\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 104us/step - loss: 0.8589 - acc: 0.7281 - top_k_categorical_accuracy: 0.9591 - precision: 0.8276 - recall: 0.6416 - fmeasure: 0.7217 - val_loss: 5.5080 - val_acc: 0.2019 - val_top_k_categorical_accuracy: 0.5180 - val_precision: 0.2332 - val_recall: 0.1643 - val_fmeasure: 0.1925\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.5457 - acc: 0.8261 - top_k_categorical_accuracy: 0.9835 - precision: 0.8664 - recall: 0.7898 - fmeasure: 0.8253 - val_loss: 4.2027 - val_acc: 0.3848 - val_top_k_categorical_accuracy: 0.7324 - val_precision: 0.4160 - val_recall: 0.3609 - val_fmeasure: 0.3863\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 104us/step - loss: 0.7447 - acc: 0.7610 - top_k_categorical_accuracy: 0.9695 - precision: 0.8438 - recall: 0.6894 - fmeasure: 0.7579 - val_loss: 5.7056 - val_acc: 0.2092 - val_top_k_categorical_accuracy: 0.5322 - val_precision: 0.2388 - val_recall: 0.1785 - val_fmeasure: 0.2040\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.5018 - acc: 0.8405 - top_k_categorical_accuracy: 0.9857 - precision: 0.8754 - recall: 0.8111 - fmeasure: 0.8412 - val_loss: 4.4511 - val_acc: 0.3814 - val_top_k_categorical_accuracy: 0.7282 - val_precision: 0.4095 - val_recall: 0.3610 - val_fmeasure: 0.3835\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 104us/step - loss: 0.6513 - acc: 0.7889 - top_k_categorical_accuracy: 0.9770 - precision: 0.8574 - recall: 0.7293 - fmeasure: 0.7874 - val_loss: 6.4172 - val_acc: 0.1969 - val_top_k_categorical_accuracy: 0.5009 - val_precision: 0.2192 - val_recall: 0.1720 - val_fmeasure: 0.1925\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.4656 - acc: 0.8535 - top_k_categorical_accuracy: 0.9874 - precision: 0.8833 - recall: 0.8274 - fmeasure: 0.8537 - val_loss: 4.5927 - val_acc: 0.3842 - val_top_k_categorical_accuracy: 0.7296 - val_precision: 0.4105 - val_recall: 0.3673 - val_fmeasure: 0.3875\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 104us/step - loss: 0.5716 - acc: 0.8146 - top_k_categorical_accuracy: 0.9823 - precision: 0.8719 - recall: 0.7640 - fmeasure: 0.8137 - val_loss: 6.3288 - val_acc: 0.2086 - val_top_k_categorical_accuracy: 0.5301 - val_precision: 0.2300 - val_recall: 0.1850 - val_fmeasure: 0.2048\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.4364 - acc: 0.8651 - top_k_categorical_accuracy: 0.9887 - precision: 0.8906 - recall: 0.8430 - fmeasure: 0.8655 - val_loss: 4.8011 - val_acc: 0.3834 - val_top_k_categorical_accuracy: 0.7296 - val_precision: 0.4065 - val_recall: 0.3691 - val_fmeasure: 0.3867\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 103us/step - loss: 0.4984 - acc: 0.8383 - top_k_categorical_accuracy: 0.9867 - precision: 0.8846 - recall: 0.7971 - fmeasure: 0.8381 - val_loss: 6.5656 - val_acc: 0.2066 - val_top_k_categorical_accuracy: 0.5313 - val_precision: 0.2250 - val_recall: 0.1855 - val_fmeasure: 0.2032\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.4124 - acc: 0.8721 - top_k_categorical_accuracy: 0.9890 - precision: 0.8943 - recall: 0.8539 - fmeasure: 0.8732 - val_loss: 5.0248 - val_acc: 0.3779 - val_top_k_categorical_accuracy: 0.7255 - val_precision: 0.3993 - val_recall: 0.3644 - val_fmeasure: 0.3809\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 104us/step - loss: 0.4467 - acc: 0.8534 - top_k_categorical_accuracy: 0.9897 - precision: 0.8925 - recall: 0.8183 - fmeasure: 0.8533 - val_loss: 6.8750 - val_acc: 0.2123 - val_top_k_categorical_accuracy: 0.5426 - val_precision: 0.2289 - val_recall: 0.1949 - val_fmeasure: 0.2104\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 116us/step - loss: 0.3821 - acc: 0.8827 - top_k_categorical_accuracy: 0.9911 - precision: 0.9016 - recall: 0.8659 - fmeasure: 0.8831 - val_loss: 5.0515 - val_acc: 0.3783 - val_top_k_categorical_accuracy: 0.7227 - val_precision: 0.4000 - val_recall: 0.3649 - val_fmeasure: 0.3815\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 104us/step - loss: 0.4027 - acc: 0.8688 - top_k_categorical_accuracy: 0.9920 - precision: 0.9008 - recall: 0.8393 - fmeasure: 0.8685 - val_loss: 6.9985 - val_acc: 0.2097 - val_top_k_categorical_accuracy: 0.5445 - val_precision: 0.2247 - val_recall: 0.1931 - val_fmeasure: 0.2075\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.3558 - acc: 0.8915 - top_k_categorical_accuracy: 0.9917 - precision: 0.9094 - recall: 0.8767 - fmeasure: 0.8923 - val_loss: 5.2864 - val_acc: 0.3814 - val_top_k_categorical_accuracy: 0.7267 - val_precision: 0.4004 - val_recall: 0.3699 - val_fmeasure: 0.3844\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 104us/step - loss: 0.3574 - acc: 0.8828 - top_k_categorical_accuracy: 0.9941 - precision: 0.9096 - recall: 0.8587 - fmeasure: 0.8831 - val_loss: 7.6042 - val_acc: 0.2041 - val_top_k_categorical_accuracy: 0.5199 - val_precision: 0.2170 - val_recall: 0.1913 - val_fmeasure: 0.2032\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 123us/step - loss: 0.3565 - acc: 0.8928 - top_k_categorical_accuracy: 0.9914 - precision: 0.9099 - recall: 0.8789 - fmeasure: 0.8937 - val_loss: 5.3505 - val_acc: 0.3804 - val_top_k_categorical_accuracy: 0.7260 - val_precision: 0.3964 - val_recall: 0.3686 - val_fmeasure: 0.3819\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 105us/step - loss: 0.3331 - acc: 0.8913 - top_k_categorical_accuracy: 0.9949 - precision: 0.9139 - recall: 0.8705 - fmeasure: 0.8914 - val_loss: 7.6217 - val_acc: 0.2057 - val_top_k_categorical_accuracy: 0.5311 - val_precision: 0.2170 - val_recall: 0.1938 - val_fmeasure: 0.2047\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 117us/step - loss: 0.3343 - acc: 0.9021 - top_k_categorical_accuracy: 0.9930 - precision: 0.9154 - recall: 0.8907 - fmeasure: 0.9026 - val_loss: 5.4833 - val_acc: 0.3803 - val_top_k_categorical_accuracy: 0.7230 - val_precision: 0.3962 - val_recall: 0.3704 - val_fmeasure: 0.3828\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 105us/step - loss: 0.2998 - acc: 0.9021 - top_k_categorical_accuracy: 0.9958 - precision: 0.9216 - recall: 0.8849 - fmeasure: 0.9026 - val_loss: 8.0450 - val_acc: 0.1991 - val_top_k_categorical_accuracy: 0.5159 - val_precision: 0.2089 - val_recall: 0.1885 - val_fmeasure: 0.1981\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.3135 - acc: 0.9091 - top_k_categorical_accuracy: 0.9935 - precision: 0.9215 - recall: 0.8992 - fmeasure: 0.9100 - val_loss: 5.5745 - val_acc: 0.3781 - val_top_k_categorical_accuracy: 0.7216 - val_precision: 0.3943 - val_recall: 0.3687 - val_fmeasure: 0.3810\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 104us/step - loss: 0.2776 - acc: 0.9106 - top_k_categorical_accuracy: 0.9965 - precision: 0.9267 - recall: 0.8952 - fmeasure: 0.9105 - val_loss: 7.9471 - val_acc: 0.2094 - val_top_k_categorical_accuracy: 0.5332 - val_precision: 0.2201 - val_recall: 0.1998 - val_fmeasure: 0.2094\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.3018 - acc: 0.9120 - top_k_categorical_accuracy: 0.9939 - precision: 0.9234 - recall: 0.9027 - fmeasure: 0.9127 - val_loss: 5.7655 - val_acc: 0.3831 - val_top_k_categorical_accuracy: 0.7235 - val_precision: 0.3965 - val_recall: 0.3752 - val_fmeasure: 0.3855\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 105us/step - loss: 0.2646 - acc: 0.9154 - top_k_categorical_accuracy: 0.9969 - precision: 0.9299 - recall: 0.9019 - fmeasure: 0.9155 - val_loss: 7.9380 - val_acc: 0.2071 - val_top_k_categorical_accuracy: 0.5353 - val_precision: 0.2172 - val_recall: 0.1977 - val_fmeasure: 0.2070\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.2982 - acc: 0.9140 - top_k_categorical_accuracy: 0.9939 - precision: 0.9248 - recall: 0.9052 - fmeasure: 0.9147 - val_loss: 5.6968 - val_acc: 0.3851 - val_top_k_categorical_accuracy: 0.7209 - val_precision: 0.3997 - val_recall: 0.3765 - val_fmeasure: 0.3877\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 105us/step - loss: 0.2473 - acc: 0.9212 - top_k_categorical_accuracy: 0.9973 - precision: 0.9340 - recall: 0.9092 - fmeasure: 0.9213 - val_loss: 8.1858 - val_acc: 0.2090 - val_top_k_categorical_accuracy: 0.5416 - val_precision: 0.2176 - val_recall: 0.2008 - val_fmeasure: 0.2088\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.2837 - acc: 0.9196 - top_k_categorical_accuracy: 0.9944 - precision: 0.9290 - recall: 0.9121 - fmeasure: 0.9203 - val_loss: 5.8150 - val_acc: 0.3824 - val_top_k_categorical_accuracy: 0.7291 - val_precision: 0.3952 - val_recall: 0.3752 - val_fmeasure: 0.3849\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 105us/step - loss: 0.2369 - acc: 0.9250 - top_k_categorical_accuracy: 0.9974 - precision: 0.9363 - recall: 0.9149 - fmeasure: 0.9253 - val_loss: 8.3122 - val_acc: 0.2060 - val_top_k_categorical_accuracy: 0.5310 - val_precision: 0.2131 - val_recall: 0.1965 - val_fmeasure: 0.2044\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.2825 - acc: 0.9209 - top_k_categorical_accuracy: 0.9946 - precision: 0.9300 - recall: 0.9131 - fmeasure: 0.9213 - val_loss: 5.9840 - val_acc: 0.3801 - val_top_k_categorical_accuracy: 0.7221 - val_precision: 0.3919 - val_recall: 0.3736 - val_fmeasure: 0.3825\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 105us/step - loss: 0.2292 - acc: 0.9288 - top_k_categorical_accuracy: 0.9972 - precision: 0.9390 - recall: 0.9193 - fmeasure: 0.9289 - val_loss: 8.3584 - val_acc: 0.2110 - val_top_k_categorical_accuracy: 0.5474 - val_precision: 0.2183 - val_recall: 0.2028 - val_fmeasure: 0.2102\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.2658 - acc: 0.9255 - top_k_categorical_accuracy: 0.9952 - precision: 0.9334 - recall: 0.9186 - fmeasure: 0.9258 - val_loss: 5.9888 - val_acc: 0.3823 - val_top_k_categorical_accuracy: 0.7242 - val_precision: 0.3945 - val_recall: 0.3752 - val_fmeasure: 0.3845\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 105us/step - loss: 0.2135 - acc: 0.9329 - top_k_categorical_accuracy: 0.9977 - precision: 0.9421 - recall: 0.9249 - fmeasure: 0.9333 - val_loss: 8.7804 - val_acc: 0.2032 - val_top_k_categorical_accuracy: 0.5205 - val_precision: 0.2096 - val_recall: 0.1956 - val_fmeasure: 0.2023\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.2740 - acc: 0.9261 - top_k_categorical_accuracy: 0.9943 - precision: 0.9338 - recall: 0.9196 - fmeasure: 0.9265 - val_loss: 6.1074 - val_acc: 0.3831 - val_top_k_categorical_accuracy: 0.7248 - val_precision: 0.3941 - val_recall: 0.3770 - val_fmeasure: 0.3853\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 105us/step - loss: 0.2077 - acc: 0.9354 - top_k_categorical_accuracy: 0.9980 - precision: 0.9437 - recall: 0.9278 - fmeasure: 0.9356 - val_loss: 8.5149 - val_acc: 0.2096 - val_top_k_categorical_accuracy: 0.5410 - val_precision: 0.2154 - val_recall: 0.2014 - val_fmeasure: 0.2081\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.2585 - acc: 0.9293 - top_k_categorical_accuracy: 0.9950 - precision: 0.9357 - recall: 0.9234 - fmeasure: 0.9294 - val_loss: 6.1430 - val_acc: 0.3820 - val_top_k_categorical_accuracy: 0.7214 - val_precision: 0.3931 - val_recall: 0.3755 - val_fmeasure: 0.3840\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 105us/step - loss: 0.2023 - acc: 0.9369 - top_k_categorical_accuracy: 0.9979 - precision: 0.9447 - recall: 0.9300 - fmeasure: 0.9372 - val_loss: 8.7446 - val_acc: 0.2038 - val_top_k_categorical_accuracy: 0.5307 - val_precision: 0.2111 - val_recall: 0.1973 - val_fmeasure: 0.2039\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.2582 - acc: 0.9309 - top_k_categorical_accuracy: 0.9948 - precision: 0.9384 - recall: 0.9254 - fmeasure: 0.9317 - val_loss: 6.1760 - val_acc: 0.3826 - val_top_k_categorical_accuracy: 0.7257 - val_precision: 0.3933 - val_recall: 0.3773 - val_fmeasure: 0.3851\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 106us/step - loss: 0.1898 - acc: 0.9409 - top_k_categorical_accuracy: 0.9982 - precision: 0.9479 - recall: 0.9344 - fmeasure: 0.9410 - val_loss: 8.9940 - val_acc: 0.2085 - val_top_k_categorical_accuracy: 0.5299 - val_precision: 0.2142 - val_recall: 0.2026 - val_fmeasure: 0.2082\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.2458 - acc: 0.9345 - top_k_categorical_accuracy: 0.9956 - precision: 0.9407 - recall: 0.9297 - fmeasure: 0.9350 - val_loss: 6.1745 - val_acc: 0.3868 - val_top_k_categorical_accuracy: 0.7247 - val_precision: 0.3962 - val_recall: 0.3802 - val_fmeasure: 0.3880\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 104us/step - loss: 0.1915 - acc: 0.9414 - top_k_categorical_accuracy: 0.9983 - precision: 0.9481 - recall: 0.9356 - fmeasure: 0.9418 - val_loss: 8.8087 - val_acc: 0.2105 - val_top_k_categorical_accuracy: 0.5405 - val_precision: 0.2164 - val_recall: 0.2047 - val_fmeasure: 0.2103\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 122us/step - loss: 0.2468 - acc: 0.9350 - top_k_categorical_accuracy: 0.9949 - precision: 0.9411 - recall: 0.9297 - fmeasure: 0.9353 - val_loss: 6.2087 - val_acc: 0.3860 - val_top_k_categorical_accuracy: 0.7248 - val_precision: 0.3961 - val_recall: 0.3802 - val_fmeasure: 0.3879\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 104us/step - loss: 0.1838 - acc: 0.9442 - top_k_categorical_accuracy: 0.9982 - precision: 0.9505 - recall: 0.9386 - fmeasure: 0.9444 - val_loss: 8.6362 - val_acc: 0.2191 - val_top_k_categorical_accuracy: 0.5504 - val_precision: 0.2253 - val_recall: 0.2126 - val_fmeasure: 0.2187\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.2381 - acc: 0.9365 - top_k_categorical_accuracy: 0.9952 - precision: 0.9426 - recall: 0.9316 - fmeasure: 0.9370 - val_loss: 6.2522 - val_acc: 0.3837 - val_top_k_categorical_accuracy: 0.7274 - val_precision: 0.3933 - val_recall: 0.3774 - val_fmeasure: 0.3852\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 104us/step - loss: 0.1722 - acc: 0.9473 - top_k_categorical_accuracy: 0.9984 - precision: 0.9528 - recall: 0.9423 - fmeasure: 0.9474 - val_loss: 8.8582 - val_acc: 0.2170 - val_top_k_categorical_accuracy: 0.5407 - val_precision: 0.2225 - val_recall: 0.2107 - val_fmeasure: 0.2164\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 117us/step - loss: 0.2432 - acc: 0.9362 - top_k_categorical_accuracy: 0.9950 - precision: 0.9419 - recall: 0.9319 - fmeasure: 0.9368 - val_loss: 6.3601 - val_acc: 0.3778 - val_top_k_categorical_accuracy: 0.7255 - val_precision: 0.3868 - val_recall: 0.3726 - val_fmeasure: 0.3795\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 104us/step - loss: 0.1701 - acc: 0.9480 - top_k_categorical_accuracy: 0.9985 - precision: 0.9532 - recall: 0.9433 - fmeasure: 0.9481 - val_loss: 8.9197 - val_acc: 0.2146 - val_top_k_categorical_accuracy: 0.5501 - val_precision: 0.2201 - val_recall: 0.2089 - val_fmeasure: 0.2143\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 117us/step - loss: 0.2320 - acc: 0.9387 - top_k_categorical_accuracy: 0.9957 - precision: 0.9441 - recall: 0.9342 - fmeasure: 0.9390 - val_loss: 6.4149 - val_acc: 0.3856 - val_top_k_categorical_accuracy: 0.7249 - val_precision: 0.3942 - val_recall: 0.3801 - val_fmeasure: 0.3870\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.0642 - acc: 0.9800 - top_k_categorical_accuracy: 0.9998 - precision: 0.9812 - recall: 0.9791 - fmeasure: 0.9801 - val_loss: 6.5786 - val_acc: 0.3874 - val_top_k_categorical_accuracy: 0.7277 - val_precision: 0.3950 - val_recall: 0.3829 - val_fmeasure: 0.3888\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 117us/step - loss: 0.0496 - acc: 0.9843 - top_k_categorical_accuracy: 0.9999 - precision: 0.9852 - recall: 0.9837 - fmeasure: 0.9844 - val_loss: 6.6104 - val_acc: 0.3871 - val_top_k_categorical_accuracy: 0.7269 - val_precision: 0.3946 - val_recall: 0.3828 - val_fmeasure: 0.3886\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.0388 - acc: 0.9883 - top_k_categorical_accuracy: 0.9999 - precision: 0.9888 - recall: 0.9877 - fmeasure: 0.9882 - val_loss: 6.7278 - val_acc: 0.3850 - val_top_k_categorical_accuracy: 0.7251 - val_precision: 0.3933 - val_recall: 0.3818 - val_fmeasure: 0.3874\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 116us/step - loss: 0.0353 - acc: 0.9891 - top_k_categorical_accuracy: 1.0000 - precision: 0.9896 - recall: 0.9886 - fmeasure: 0.9891 - val_loss: 6.8639 - val_acc: 0.3835 - val_top_k_categorical_accuracy: 0.7278 - val_precision: 0.3901 - val_recall: 0.3795 - val_fmeasure: 0.3847\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 116us/step - loss: 0.0311 - acc: 0.9899 - top_k_categorical_accuracy: 0.9999 - precision: 0.9903 - recall: 0.9896 - fmeasure: 0.9900 - val_loss: 6.9644 - val_acc: 0.3874 - val_top_k_categorical_accuracy: 0.7242 - val_precision: 0.3935 - val_recall: 0.3833 - val_fmeasure: 0.3883\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 117us/step - loss: 0.0343 - acc: 0.9896 - top_k_categorical_accuracy: 1.0000 - precision: 0.9901 - recall: 0.9893 - fmeasure: 0.9897 - val_loss: 6.9027 - val_acc: 0.3870 - val_top_k_categorical_accuracy: 0.7271 - val_precision: 0.3925 - val_recall: 0.3832 - val_fmeasure: 0.3877\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 116us/step - loss: 0.0310 - acc: 0.9909 - top_k_categorical_accuracy: 0.9999 - precision: 0.9912 - recall: 0.9906 - fmeasure: 0.9909 - val_loss: 7.0336 - val_acc: 0.3843 - val_top_k_categorical_accuracy: 0.7240 - val_precision: 0.3899 - val_recall: 0.3814 - val_fmeasure: 0.3856\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 8s 136us/step - loss: 0.0311 - acc: 0.9908 - top_k_categorical_accuracy: 0.9999 - precision: 0.9911 - recall: 0.9904 - fmeasure: 0.9908 - val_loss: 7.0222 - val_acc: 0.3844 - val_top_k_categorical_accuracy: 0.7256 - val_precision: 0.3902 - val_recall: 0.3810 - val_fmeasure: 0.3855\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.0290 - acc: 0.9914 - top_k_categorical_accuracy: 1.0000 - precision: 0.9917 - recall: 0.9911 - fmeasure: 0.9914 - val_loss: 7.0761 - val_acc: 0.3858 - val_top_k_categorical_accuracy: 0.7248 - val_precision: 0.3912 - val_recall: 0.3825 - val_fmeasure: 0.3868\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.0249 - acc: 0.9922 - top_k_categorical_accuracy: 1.0000 - precision: 0.9926 - recall: 0.9921 - fmeasure: 0.9924 - val_loss: 7.0900 - val_acc: 0.3899 - val_top_k_categorical_accuracy: 0.7260 - val_precision: 0.3942 - val_recall: 0.3860 - val_fmeasure: 0.3900\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.0206 - acc: 0.9942 - top_k_categorical_accuracy: 1.0000 - precision: 0.9944 - recall: 0.9940 - fmeasure: 0.9942 - val_loss: 7.1796 - val_acc: 0.3869 - val_top_k_categorical_accuracy: 0.7247 - val_precision: 0.3923 - val_recall: 0.3845 - val_fmeasure: 0.3883\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.0253 - acc: 0.9922 - top_k_categorical_accuracy: 1.0000 - precision: 0.9925 - recall: 0.9919 - fmeasure: 0.9922 - val_loss: 7.1299 - val_acc: 0.3865 - val_top_k_categorical_accuracy: 0.7270 - val_precision: 0.3907 - val_recall: 0.3832 - val_fmeasure: 0.3869\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.0238 - acc: 0.9934 - top_k_categorical_accuracy: 0.9999 - precision: 0.9936 - recall: 0.9933 - fmeasure: 0.9934 - val_loss: 7.2282 - val_acc: 0.3876 - val_top_k_categorical_accuracy: 0.7282 - val_precision: 0.3922 - val_recall: 0.3850 - val_fmeasure: 0.3886\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.0252 - acc: 0.9928 - top_k_categorical_accuracy: 0.9999 - precision: 0.9930 - recall: 0.9926 - fmeasure: 0.9928 - val_loss: 7.2797 - val_acc: 0.3843 - val_top_k_categorical_accuracy: 0.7223 - val_precision: 0.3889 - val_recall: 0.3812 - val_fmeasure: 0.3850\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.0198 - acc: 0.9937 - top_k_categorical_accuracy: 0.9999 - precision: 0.9940 - recall: 0.9936 - fmeasure: 0.9938 - val_loss: 7.2922 - val_acc: 0.3850 - val_top_k_categorical_accuracy: 0.7273 - val_precision: 0.3889 - val_recall: 0.3819 - val_fmeasure: 0.3854\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.0227 - acc: 0.9937 - top_k_categorical_accuracy: 1.0000 - precision: 0.9939 - recall: 0.9936 - fmeasure: 0.9937 - val_loss: 7.3169 - val_acc: 0.3910 - val_top_k_categorical_accuracy: 0.7257 - val_precision: 0.3952 - val_recall: 0.3886 - val_fmeasure: 0.3918\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.0223 - acc: 0.9936 - top_k_categorical_accuracy: 1.0000 - precision: 0.9937 - recall: 0.9935 - fmeasure: 0.9936 - val_loss: 7.3202 - val_acc: 0.3877 - val_top_k_categorical_accuracy: 0.7237 - val_precision: 0.3925 - val_recall: 0.3849 - val_fmeasure: 0.3886\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.0205 - acc: 0.9941 - top_k_categorical_accuracy: 1.0000 - precision: 0.9943 - recall: 0.9940 - fmeasure: 0.9941 - val_loss: 7.4507 - val_acc: 0.3874 - val_top_k_categorical_accuracy: 0.7219 - val_precision: 0.3916 - val_recall: 0.3851 - val_fmeasure: 0.3883\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 117us/step - loss: 0.0210 - acc: 0.9942 - top_k_categorical_accuracy: 0.9999 - precision: 0.9944 - recall: 0.9940 - fmeasure: 0.9942 - val_loss: 7.3999 - val_acc: 0.3833 - val_top_k_categorical_accuracy: 0.7240 - val_precision: 0.3875 - val_recall: 0.3806 - val_fmeasure: 0.3840\n",
            "deleting......\n",
            "00% data has been dealed\n",
            "10% data has been dealed\n",
            "20% data has been dealed\n",
            "30% data has been dealed\n",
            "40% data has been dealed\n",
            "50% data has been dealed\n",
            "60% data has been dealed\n",
            "70% data has been dealed\n",
            "80% data has been dealed\n",
            "90% data has been dealed\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 8s 127us/step - loss: 3.1042 - acc: 0.1808 - top_k_categorical_accuracy: 0.4941 - precision: 0.4338 - recall: 0.0236 - fmeasure: 0.0439 - val_loss: 2.6639 - val_acc: 0.2712 - val_top_k_categorical_accuracy: 0.6395 - val_precision: 0.5809 - val_recall: 0.0616 - val_fmeasure: 0.1102\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 2.2448 - acc: 0.3621 - top_k_categorical_accuracy: 0.7411 - precision: 0.6629 - recall: 0.1368 - fmeasure: 0.2241 - val_loss: 2.4314 - val_acc: 0.3281 - val_top_k_categorical_accuracy: 0.7050 - val_precision: 0.5715 - val_recall: 0.1345 - val_fmeasure: 0.2160\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 1.7402 - acc: 0.4890 - top_k_categorical_accuracy: 0.8408 - precision: 0.7227 - recall: 0.2961 - fmeasure: 0.4179 - val_loss: 2.4971 - val_acc: 0.3420 - val_top_k_categorical_accuracy: 0.7109 - val_precision: 0.5229 - val_recall: 0.1822 - val_fmeasure: 0.2686\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 120us/step - loss: 1.3472 - acc: 0.5939 - top_k_categorical_accuracy: 0.8990 - precision: 0.7720 - recall: 0.4478 - fmeasure: 0.5651 - val_loss: 2.6747 - val_acc: 0.3418 - val_top_k_categorical_accuracy: 0.7120 - val_precision: 0.4735 - val_recall: 0.2221 - val_fmeasure: 0.3012\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 1.0426 - acc: 0.6821 - top_k_categorical_accuracy: 0.9338 - precision: 0.8200 - recall: 0.5720 - fmeasure: 0.6728 - val_loss: 3.0777 - val_acc: 0.3390 - val_top_k_categorical_accuracy: 0.6985 - val_precision: 0.4236 - val_recall: 0.2566 - val_fmeasure: 0.3188\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.8278 - acc: 0.7435 - top_k_categorical_accuracy: 0.9550 - precision: 0.8493 - recall: 0.6616 - fmeasure: 0.7429 - val_loss: 3.2877 - val_acc: 0.3341 - val_top_k_categorical_accuracy: 0.6919 - val_precision: 0.4100 - val_recall: 0.2659 - val_fmeasure: 0.3220\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.6647 - acc: 0.7918 - top_k_categorical_accuracy: 0.9693 - precision: 0.8721 - recall: 0.7300 - fmeasure: 0.7941 - val_loss: 3.6883 - val_acc: 0.3268 - val_top_k_categorical_accuracy: 0.6743 - val_precision: 0.3899 - val_recall: 0.2760 - val_fmeasure: 0.3227\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.5335 - acc: 0.8318 - top_k_categorical_accuracy: 0.9787 - precision: 0.8935 - recall: 0.7845 - fmeasure: 0.8350 - val_loss: 4.0869 - val_acc: 0.3257 - val_top_k_categorical_accuracy: 0.6768 - val_precision: 0.3720 - val_recall: 0.2842 - val_fmeasure: 0.3219\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.4394 - acc: 0.8600 - top_k_categorical_accuracy: 0.9845 - precision: 0.9079 - recall: 0.8244 - fmeasure: 0.8638 - val_loss: 4.4255 - val_acc: 0.3205 - val_top_k_categorical_accuracy: 0.6717 - val_precision: 0.3612 - val_recall: 0.2864 - val_fmeasure: 0.3192\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.3654 - acc: 0.8839 - top_k_categorical_accuracy: 0.9893 - precision: 0.9211 - recall: 0.8555 - fmeasure: 0.8868 - val_loss: 4.6843 - val_acc: 0.3151 - val_top_k_categorical_accuracy: 0.6607 - val_precision: 0.3483 - val_recall: 0.2857 - val_fmeasure: 0.3137\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.3043 - acc: 0.9036 - top_k_categorical_accuracy: 0.9926 - precision: 0.9321 - recall: 0.8811 - fmeasure: 0.9056 - val_loss: 4.9648 - val_acc: 0.3160 - val_top_k_categorical_accuracy: 0.6585 - val_precision: 0.3474 - val_recall: 0.2915 - val_fmeasure: 0.3167\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.2677 - acc: 0.9151 - top_k_categorical_accuracy: 0.9941 - precision: 0.9376 - recall: 0.8964 - fmeasure: 0.9163 - val_loss: 5.1897 - val_acc: 0.3180 - val_top_k_categorical_accuracy: 0.6585 - val_precision: 0.3442 - val_recall: 0.2961 - val_fmeasure: 0.3182\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.2195 - acc: 0.9300 - top_k_categorical_accuracy: 0.9960 - precision: 0.9472 - recall: 0.9157 - fmeasure: 0.9311 - val_loss: 5.4107 - val_acc: 0.3158 - val_top_k_categorical_accuracy: 0.6600 - val_precision: 0.3384 - val_recall: 0.2977 - val_fmeasure: 0.3166\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.1930 - acc: 0.9386 - top_k_categorical_accuracy: 0.9975 - precision: 0.9522 - recall: 0.9269 - fmeasure: 0.9393 - val_loss: 5.6636 - val_acc: 0.3133 - val_top_k_categorical_accuracy: 0.6560 - val_precision: 0.3333 - val_recall: 0.2962 - val_fmeasure: 0.3135\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 120us/step - loss: 0.1768 - acc: 0.9446 - top_k_categorical_accuracy: 0.9980 - precision: 0.9559 - recall: 0.9348 - fmeasure: 0.9451 - val_loss: 5.8581 - val_acc: 0.3111 - val_top_k_categorical_accuracy: 0.6488 - val_precision: 0.3317 - val_recall: 0.2978 - val_fmeasure: 0.3137\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 120us/step - loss: 0.1517 - acc: 0.9535 - top_k_categorical_accuracy: 0.9987 - precision: 0.9625 - recall: 0.9451 - fmeasure: 0.9536 - val_loss: 5.9076 - val_acc: 0.3135 - val_top_k_categorical_accuracy: 0.6541 - val_precision: 0.3320 - val_recall: 0.3008 - val_fmeasure: 0.3155\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.1415 - acc: 0.9563 - top_k_categorical_accuracy: 0.9989 - precision: 0.9634 - recall: 0.9495 - fmeasure: 0.9563 - val_loss: 6.0639 - val_acc: 0.3130 - val_top_k_categorical_accuracy: 0.6515 - val_precision: 0.3299 - val_recall: 0.3012 - val_fmeasure: 0.3148\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.1296 - acc: 0.9598 - top_k_categorical_accuracy: 0.9991 - precision: 0.9658 - recall: 0.9539 - fmeasure: 0.9598 - val_loss: 6.2696 - val_acc: 0.3129 - val_top_k_categorical_accuracy: 0.6542 - val_precision: 0.3279 - val_recall: 0.3017 - val_fmeasure: 0.3142\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.1269 - acc: 0.9601 - top_k_categorical_accuracy: 0.9993 - precision: 0.9656 - recall: 0.9549 - fmeasure: 0.9602 - val_loss: 6.3710 - val_acc: 0.3083 - val_top_k_categorical_accuracy: 0.6482 - val_precision: 0.3222 - val_recall: 0.2982 - val_fmeasure: 0.3096\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.1139 - acc: 0.9656 - top_k_categorical_accuracy: 0.9995 - precision: 0.9702 - recall: 0.9610 - fmeasure: 0.9655 - val_loss: 6.3011 - val_acc: 0.3143 - val_top_k_categorical_accuracy: 0.6523 - val_precision: 0.3301 - val_recall: 0.3041 - val_fmeasure: 0.3164\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.1022 - acc: 0.9683 - top_k_categorical_accuracy: 0.9997 - precision: 0.9723 - recall: 0.9646 - fmeasure: 0.9684 - val_loss: 6.4262 - val_acc: 0.3176 - val_top_k_categorical_accuracy: 0.6627 - val_precision: 0.3311 - val_recall: 0.3079 - val_fmeasure: 0.3190\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 106us/step - loss: 2.4978 - acc: 0.3151 - top_k_categorical_accuracy: 0.6891 - precision: 0.5856 - recall: 0.1056 - fmeasure: 0.1757 - val_loss: 5.2715 - val_acc: 0.0663 - val_top_k_categorical_accuracy: 0.2304 - val_precision: 0.1016 - val_recall: 0.0191 - val_fmeasure: 0.0319\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.7713 - acc: 0.7646 - top_k_categorical_accuracy: 0.9590 - precision: 0.8395 - recall: 0.7007 - fmeasure: 0.7590 - val_loss: 4.2124 - val_acc: 0.3441 - val_top_k_categorical_accuracy: 0.7037 - val_precision: 0.3787 - val_recall: 0.3146 - val_fmeasure: 0.3434\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 106us/step - loss: 2.0060 - acc: 0.4211 - top_k_categorical_accuracy: 0.7944 - precision: 0.6706 - recall: 0.2119 - fmeasure: 0.3192 - val_loss: 5.3692 - val_acc: 0.0786 - val_top_k_categorical_accuracy: 0.2624 - val_precision: 0.1184 - val_recall: 0.0306 - val_fmeasure: 0.0483\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.7605 - acc: 0.7652 - top_k_categorical_accuracy: 0.9620 - precision: 0.8373 - recall: 0.7038 - fmeasure: 0.7614 - val_loss: 3.9637 - val_acc: 0.3593 - val_top_k_categorical_accuracy: 0.7131 - val_precision: 0.4002 - val_recall: 0.3273 - val_fmeasure: 0.3598\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 105us/step - loss: 1.7460 - acc: 0.4844 - top_k_categorical_accuracy: 0.8424 - precision: 0.7073 - recall: 0.2960 - fmeasure: 0.4147 - val_loss: 5.5069 - val_acc: 0.0929 - val_top_k_categorical_accuracy: 0.2884 - val_precision: 0.1420 - val_recall: 0.0512 - val_fmeasure: 0.0749\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.6992 - acc: 0.7803 - top_k_categorical_accuracy: 0.9691 - precision: 0.8467 - recall: 0.7257 - fmeasure: 0.7793 - val_loss: 3.9018 - val_acc: 0.3706 - val_top_k_categorical_accuracy: 0.7194 - val_precision: 0.4141 - val_recall: 0.3370 - val_fmeasure: 0.3713\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 105us/step - loss: 1.5387 - acc: 0.5373 - top_k_categorical_accuracy: 0.8754 - precision: 0.7353 - recall: 0.3687 - fmeasure: 0.4887 - val_loss: 6.1190 - val_acc: 0.0851 - val_top_k_categorical_accuracy: 0.2698 - val_precision: 0.1221 - val_recall: 0.0497 - val_fmeasure: 0.0703\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 122us/step - loss: 0.6550 - acc: 0.7949 - top_k_categorical_accuracy: 0.9722 - precision: 0.8557 - recall: 0.7469 - fmeasure: 0.7957 - val_loss: 4.0872 - val_acc: 0.3736 - val_top_k_categorical_accuracy: 0.7204 - val_precision: 0.4088 - val_recall: 0.3430 - val_fmeasure: 0.3727\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 105us/step - loss: 1.3711 - acc: 0.5824 - top_k_categorical_accuracy: 0.8996 - precision: 0.7597 - recall: 0.4315 - fmeasure: 0.5481 - val_loss: 6.5925 - val_acc: 0.0917 - val_top_k_categorical_accuracy: 0.2824 - val_precision: 0.1186 - val_recall: 0.0599 - val_fmeasure: 0.0793\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.6064 - acc: 0.8119 - top_k_categorical_accuracy: 0.9754 - precision: 0.8643 - recall: 0.7672 - fmeasure: 0.8114 - val_loss: 4.1383 - val_acc: 0.3706 - val_top_k_categorical_accuracy: 0.7135 - val_precision: 0.4083 - val_recall: 0.3383 - val_fmeasure: 0.3698\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 106us/step - loss: 1.2223 - acc: 0.6226 - top_k_categorical_accuracy: 0.9197 - precision: 0.7805 - recall: 0.4900 - fmeasure: 0.6000 - val_loss: 6.9266 - val_acc: 0.0902 - val_top_k_categorical_accuracy: 0.2849 - val_precision: 0.1180 - val_recall: 0.0648 - val_fmeasure: 0.0833\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 122us/step - loss: 0.5606 - acc: 0.8249 - top_k_categorical_accuracy: 0.9797 - precision: 0.8708 - recall: 0.7869 - fmeasure: 0.8256 - val_loss: 4.5213 - val_acc: 0.3729 - val_top_k_categorical_accuracy: 0.7173 - val_precision: 0.4020 - val_recall: 0.3502 - val_fmeasure: 0.3741\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 106us/step - loss: 1.0978 - acc: 0.6564 - top_k_categorical_accuracy: 0.9351 - precision: 0.7956 - recall: 0.5403 - fmeasure: 0.6418 - val_loss: 7.4147 - val_acc: 0.0975 - val_top_k_categorical_accuracy: 0.2952 - val_precision: 0.1174 - val_recall: 0.0723 - val_fmeasure: 0.0893\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.5298 - acc: 0.8381 - top_k_categorical_accuracy: 0.9814 - precision: 0.8786 - recall: 0.8036 - fmeasure: 0.8386 - val_loss: 4.5147 - val_acc: 0.3752 - val_top_k_categorical_accuracy: 0.7179 - val_precision: 0.4064 - val_recall: 0.3520 - val_fmeasure: 0.3771\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 106us/step - loss: 0.9858 - acc: 0.6899 - top_k_categorical_accuracy: 0.9479 - precision: 0.8110 - recall: 0.5857 - fmeasure: 0.6785 - val_loss: 7.8814 - val_acc: 0.0896 - val_top_k_categorical_accuracy: 0.2872 - val_precision: 0.1085 - val_recall: 0.0710 - val_fmeasure: 0.0857\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.4968 - acc: 0.8500 - top_k_categorical_accuracy: 0.9831 - precision: 0.8856 - recall: 0.8200 - fmeasure: 0.8506 - val_loss: 4.6637 - val_acc: 0.3705 - val_top_k_categorical_accuracy: 0.7142 - val_precision: 0.3966 - val_recall: 0.3476 - val_fmeasure: 0.3702\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 106us/step - loss: 0.8909 - acc: 0.7185 - top_k_categorical_accuracy: 0.9569 - precision: 0.8252 - recall: 0.6271 - fmeasure: 0.7111 - val_loss: 8.0543 - val_acc: 0.0982 - val_top_k_categorical_accuracy: 0.3037 - val_precision: 0.1161 - val_recall: 0.0813 - val_fmeasure: 0.0955\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 120us/step - loss: 0.4685 - acc: 0.8585 - top_k_categorical_accuracy: 0.9848 - precision: 0.8892 - recall: 0.8334 - fmeasure: 0.8597 - val_loss: 4.8393 - val_acc: 0.3703 - val_top_k_categorical_accuracy: 0.7112 - val_precision: 0.3963 - val_recall: 0.3503 - val_fmeasure: 0.3717\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 106us/step - loss: 0.7930 - acc: 0.7458 - top_k_categorical_accuracy: 0.9654 - precision: 0.8388 - recall: 0.6670 - fmeasure: 0.7418 - val_loss: 8.5183 - val_acc: 0.0981 - val_top_k_categorical_accuracy: 0.2936 - val_precision: 0.1145 - val_recall: 0.0831 - val_fmeasure: 0.0962\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 120us/step - loss: 0.4382 - acc: 0.8690 - top_k_categorical_accuracy: 0.9864 - precision: 0.8976 - recall: 0.8475 - fmeasure: 0.8713 - val_loss: 5.1042 - val_acc: 0.3689 - val_top_k_categorical_accuracy: 0.7090 - val_precision: 0.3898 - val_recall: 0.3514 - val_fmeasure: 0.3695\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 106us/step - loss: 0.7210 - acc: 0.7696 - top_k_categorical_accuracy: 0.9712 - precision: 0.8500 - recall: 0.6993 - fmeasure: 0.7661 - val_loss: 8.8149 - val_acc: 0.0995 - val_top_k_categorical_accuracy: 0.3052 - val_precision: 0.1135 - val_recall: 0.0866 - val_fmeasure: 0.0981\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.4193 - acc: 0.8774 - top_k_categorical_accuracy: 0.9876 - precision: 0.9009 - recall: 0.8578 - fmeasure: 0.8784 - val_loss: 5.1479 - val_acc: 0.3723 - val_top_k_categorical_accuracy: 0.7124 - val_precision: 0.3940 - val_recall: 0.3556 - val_fmeasure: 0.3736\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 106us/step - loss: 0.6523 - acc: 0.7906 - top_k_categorical_accuracy: 0.9768 - precision: 0.8602 - recall: 0.7292 - fmeasure: 0.7881 - val_loss: 9.2368 - val_acc: 0.0987 - val_top_k_categorical_accuracy: 0.2931 - val_precision: 0.1083 - val_recall: 0.0848 - val_fmeasure: 0.0950\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 120us/step - loss: 0.3950 - acc: 0.8862 - top_k_categorical_accuracy: 0.9882 - precision: 0.9070 - recall: 0.8687 - fmeasure: 0.8871 - val_loss: 5.3842 - val_acc: 0.3742 - val_top_k_categorical_accuracy: 0.7097 - val_precision: 0.3945 - val_recall: 0.3608 - val_fmeasure: 0.3768\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 106us/step - loss: 0.5902 - acc: 0.8101 - top_k_categorical_accuracy: 0.9812 - precision: 0.8694 - recall: 0.7575 - fmeasure: 0.8087 - val_loss: 9.2900 - val_acc: 0.1049 - val_top_k_categorical_accuracy: 0.3123 - val_precision: 0.1161 - val_recall: 0.0939 - val_fmeasure: 0.1037\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 120us/step - loss: 0.3696 - acc: 0.8927 - top_k_categorical_accuracy: 0.9891 - precision: 0.9114 - recall: 0.8782 - fmeasure: 0.8942 - val_loss: 5.5052 - val_acc: 0.3720 - val_top_k_categorical_accuracy: 0.7060 - val_precision: 0.3892 - val_recall: 0.3586 - val_fmeasure: 0.3732\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 106us/step - loss: 0.5348 - acc: 0.8277 - top_k_categorical_accuracy: 0.9847 - precision: 0.8809 - recall: 0.7817 - fmeasure: 0.8275 - val_loss: 9.7952 - val_acc: 0.1035 - val_top_k_categorical_accuracy: 0.3019 - val_precision: 0.1129 - val_recall: 0.0936 - val_fmeasure: 0.1022\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 120us/step - loss: 0.3642 - acc: 0.8984 - top_k_categorical_accuracy: 0.9897 - precision: 0.9148 - recall: 0.8849 - fmeasure: 0.8993 - val_loss: 5.7331 - val_acc: 0.3694 - val_top_k_categorical_accuracy: 0.7058 - val_precision: 0.3844 - val_recall: 0.3578 - val_fmeasure: 0.3705\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 107us/step - loss: 0.4886 - acc: 0.8434 - top_k_categorical_accuracy: 0.9867 - precision: 0.8889 - recall: 0.8031 - fmeasure: 0.8430 - val_loss: 10.1303 - val_acc: 0.1010 - val_top_k_categorical_accuracy: 0.3002 - val_precision: 0.1106 - val_recall: 0.0936 - val_fmeasure: 0.1013\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 120us/step - loss: 0.3531 - acc: 0.9025 - top_k_categorical_accuracy: 0.9899 - precision: 0.9186 - recall: 0.8915 - fmeasure: 0.9045 - val_loss: 5.8598 - val_acc: 0.3722 - val_top_k_categorical_accuracy: 0.7040 - val_precision: 0.3856 - val_recall: 0.3622 - val_fmeasure: 0.3734\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 107us/step - loss: 0.4455 - acc: 0.8572 - top_k_categorical_accuracy: 0.9893 - precision: 0.8956 - recall: 0.8223 - fmeasure: 0.8567 - val_loss: 10.3539 - val_acc: 0.1058 - val_top_k_categorical_accuracy: 0.3064 - val_precision: 0.1138 - val_recall: 0.0979 - val_fmeasure: 0.1051\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 120us/step - loss: 0.3393 - acc: 0.9097 - top_k_categorical_accuracy: 0.9904 - precision: 0.9226 - recall: 0.8985 - fmeasure: 0.9101 - val_loss: 6.0039 - val_acc: 0.3678 - val_top_k_categorical_accuracy: 0.7007 - val_precision: 0.3813 - val_recall: 0.3577 - val_fmeasure: 0.3690\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 107us/step - loss: 0.4065 - acc: 0.8692 - top_k_categorical_accuracy: 0.9914 - precision: 0.9024 - recall: 0.8391 - fmeasure: 0.8691 - val_loss: 10.3614 - val_acc: 0.1032 - val_top_k_categorical_accuracy: 0.3072 - val_precision: 0.1105 - val_recall: 0.0956 - val_fmeasure: 0.1025\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 120us/step - loss: 0.3270 - acc: 0.9124 - top_k_categorical_accuracy: 0.9914 - precision: 0.9244 - recall: 0.9025 - fmeasure: 0.9131 - val_loss: 6.1272 - val_acc: 0.3692 - val_top_k_categorical_accuracy: 0.6998 - val_precision: 0.3807 - val_recall: 0.3596 - val_fmeasure: 0.3698\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 108us/step - loss: 0.3807 - acc: 0.8786 - top_k_categorical_accuracy: 0.9922 - precision: 0.9080 - recall: 0.8524 - fmeasure: 0.8788 - val_loss: 10.5451 - val_acc: 0.1037 - val_top_k_categorical_accuracy: 0.3120 - val_precision: 0.1119 - val_recall: 0.0979 - val_fmeasure: 0.1044\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 120us/step - loss: 0.3158 - acc: 0.9175 - top_k_categorical_accuracy: 0.9916 - precision: 0.9275 - recall: 0.9097 - fmeasure: 0.9183 - val_loss: 6.2521 - val_acc: 0.3691 - val_top_k_categorical_accuracy: 0.7029 - val_precision: 0.3818 - val_recall: 0.3619 - val_fmeasure: 0.3715\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 106us/step - loss: 0.3577 - acc: 0.8878 - top_k_categorical_accuracy: 0.9930 - precision: 0.9136 - recall: 0.8642 - fmeasure: 0.8877 - val_loss: 10.5358 - val_acc: 0.1048 - val_top_k_categorical_accuracy: 0.3168 - val_precision: 0.1119 - val_recall: 0.0987 - val_fmeasure: 0.1048\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.3028 - acc: 0.9215 - top_k_categorical_accuracy: 0.9919 - precision: 0.9313 - recall: 0.9136 - fmeasure: 0.9222 - val_loss: 6.2617 - val_acc: 0.3688 - val_top_k_categorical_accuracy: 0.7010 - val_precision: 0.3809 - val_recall: 0.3611 - val_fmeasure: 0.3706\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 106us/step - loss: 0.3345 - acc: 0.8952 - top_k_categorical_accuracy: 0.9940 - precision: 0.9180 - recall: 0.8747 - fmeasure: 0.8954 - val_loss: 10.8133 - val_acc: 0.1044 - val_top_k_categorical_accuracy: 0.3072 - val_precision: 0.1105 - val_recall: 0.0985 - val_fmeasure: 0.1041\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.3049 - acc: 0.9231 - top_k_categorical_accuracy: 0.9916 - precision: 0.9318 - recall: 0.9159 - fmeasure: 0.9236 - val_loss: 6.4051 - val_acc: 0.3653 - val_top_k_categorical_accuracy: 0.6966 - val_precision: 0.3753 - val_recall: 0.3574 - val_fmeasure: 0.3661\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 106us/step - loss: 0.3158 - acc: 0.9003 - top_k_categorical_accuracy: 0.9944 - precision: 0.9202 - recall: 0.8822 - fmeasure: 0.9004 - val_loss: 10.8643 - val_acc: 0.1088 - val_top_k_categorical_accuracy: 0.3178 - val_precision: 0.1140 - val_recall: 0.1023 - val_fmeasure: 0.1078\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.2966 - acc: 0.9254 - top_k_categorical_accuracy: 0.9924 - precision: 0.9334 - recall: 0.9190 - fmeasure: 0.9261 - val_loss: 6.4492 - val_acc: 0.3692 - val_top_k_categorical_accuracy: 0.7022 - val_precision: 0.3797 - val_recall: 0.3615 - val_fmeasure: 0.3703\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 105us/step - loss: 0.2945 - acc: 0.9090 - top_k_categorical_accuracy: 0.9947 - precision: 0.9264 - recall: 0.8926 - fmeasure: 0.9089 - val_loss: 10.9266 - val_acc: 0.1103 - val_top_k_categorical_accuracy: 0.3233 - val_precision: 0.1156 - val_recall: 0.1048 - val_fmeasure: 0.1099\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.2830 - acc: 0.9293 - top_k_categorical_accuracy: 0.9928 - precision: 0.9370 - recall: 0.9235 - fmeasure: 0.9301 - val_loss: 6.5445 - val_acc: 0.3687 - val_top_k_categorical_accuracy: 0.7002 - val_precision: 0.3783 - val_recall: 0.3620 - val_fmeasure: 0.3699\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 105us/step - loss: 0.2824 - acc: 0.9136 - top_k_categorical_accuracy: 0.9950 - precision: 0.9294 - recall: 0.8989 - fmeasure: 0.9136 - val_loss: 11.2025 - val_acc: 0.1072 - val_top_k_categorical_accuracy: 0.3120 - val_precision: 0.1126 - val_recall: 0.1029 - val_fmeasure: 0.1075\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.2865 - acc: 0.9309 - top_k_categorical_accuracy: 0.9927 - precision: 0.9373 - recall: 0.9256 - fmeasure: 0.9314 - val_loss: 6.7108 - val_acc: 0.3635 - val_top_k_categorical_accuracy: 0.6996 - val_precision: 0.3724 - val_recall: 0.3568 - val_fmeasure: 0.3644\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 105us/step - loss: 0.2647 - acc: 0.9197 - top_k_categorical_accuracy: 0.9956 - precision: 0.9340 - recall: 0.9070 - fmeasure: 0.9200 - val_loss: 11.1183 - val_acc: 0.1128 - val_top_k_categorical_accuracy: 0.3292 - val_precision: 0.1179 - val_recall: 0.1084 - val_fmeasure: 0.1129\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.2861 - acc: 0.9322 - top_k_categorical_accuracy: 0.9925 - precision: 0.9386 - recall: 0.9270 - fmeasure: 0.9327 - val_loss: 6.5833 - val_acc: 0.3656 - val_top_k_categorical_accuracy: 0.6948 - val_precision: 0.3758 - val_recall: 0.3579 - val_fmeasure: 0.3665\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 105us/step - loss: 0.2559 - acc: 0.9223 - top_k_categorical_accuracy: 0.9957 - precision: 0.9354 - recall: 0.9105 - fmeasure: 0.9225 - val_loss: 11.3609 - val_acc: 0.1089 - val_top_k_categorical_accuracy: 0.3154 - val_precision: 0.1130 - val_recall: 0.1042 - val_fmeasure: 0.1084\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.2920 - acc: 0.9327 - top_k_categorical_accuracy: 0.9922 - precision: 0.9395 - recall: 0.9276 - fmeasure: 0.9334 - val_loss: 6.5937 - val_acc: 0.3682 - val_top_k_categorical_accuracy: 0.6950 - val_precision: 0.3781 - val_recall: 0.3619 - val_fmeasure: 0.3698\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 105us/step - loss: 0.2407 - acc: 0.9269 - top_k_categorical_accuracy: 0.9963 - precision: 0.9381 - recall: 0.9157 - fmeasure: 0.9265 - val_loss: 11.2673 - val_acc: 0.1101 - val_top_k_categorical_accuracy: 0.3172 - val_precision: 0.1147 - val_recall: 0.1058 - val_fmeasure: 0.1101\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.2673 - acc: 0.9373 - top_k_categorical_accuracy: 0.9937 - precision: 0.9423 - recall: 0.9329 - fmeasure: 0.9375 - val_loss: 6.8060 - val_acc: 0.3664 - val_top_k_categorical_accuracy: 0.6902 - val_precision: 0.3757 - val_recall: 0.3613 - val_fmeasure: 0.3683\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 105us/step - loss: 0.2394 - acc: 0.9291 - top_k_categorical_accuracy: 0.9960 - precision: 0.9397 - recall: 0.9192 - fmeasure: 0.9291 - val_loss: 11.3927 - val_acc: 0.1119 - val_top_k_categorical_accuracy: 0.3166 - val_precision: 0.1158 - val_recall: 0.1069 - val_fmeasure: 0.1112\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.2703 - acc: 0.9388 - top_k_categorical_accuracy: 0.9929 - precision: 0.9441 - recall: 0.9347 - fmeasure: 0.9393 - val_loss: 6.8991 - val_acc: 0.3704 - val_top_k_categorical_accuracy: 0.7003 - val_precision: 0.3777 - val_recall: 0.3653 - val_fmeasure: 0.3713\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 106us/step - loss: 0.2254 - acc: 0.9329 - top_k_categorical_accuracy: 0.9963 - precision: 0.9422 - recall: 0.9242 - fmeasure: 0.9329 - val_loss: 11.2417 - val_acc: 0.1198 - val_top_k_categorical_accuracy: 0.3387 - val_precision: 0.1242 - val_recall: 0.1156 - val_fmeasure: 0.1197\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.2536 - acc: 0.9435 - top_k_categorical_accuracy: 0.9937 - precision: 0.9479 - recall: 0.9403 - fmeasure: 0.9441 - val_loss: 6.9099 - val_acc: 0.3655 - val_top_k_categorical_accuracy: 0.6946 - val_precision: 0.3743 - val_recall: 0.3608 - val_fmeasure: 0.3674\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 105us/step - loss: 0.2170 - acc: 0.9363 - top_k_categorical_accuracy: 0.9965 - precision: 0.9449 - recall: 0.9279 - fmeasure: 0.9361 - val_loss: 11.5326 - val_acc: 0.1143 - val_top_k_categorical_accuracy: 0.3218 - val_precision: 0.1180 - val_recall: 0.1101 - val_fmeasure: 0.1139\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.2693 - acc: 0.9401 - top_k_categorical_accuracy: 0.9930 - precision: 0.9450 - recall: 0.9363 - fmeasure: 0.9406 - val_loss: 6.9042 - val_acc: 0.3617 - val_top_k_categorical_accuracy: 0.6945 - val_precision: 0.3697 - val_recall: 0.3566 - val_fmeasure: 0.3630\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 19s 106us/step - loss: 0.2182 - acc: 0.9369 - top_k_categorical_accuracy: 0.9963 - precision: 0.9453 - recall: 0.9290 - fmeasure: 0.9369 - val_loss: 11.6342 - val_acc: 0.1127 - val_top_k_categorical_accuracy: 0.3177 - val_precision: 0.1160 - val_recall: 0.1086 - val_fmeasure: 0.1122\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.2736 - acc: 0.9423 - top_k_categorical_accuracy: 0.9927 - precision: 0.9461 - recall: 0.9385 - fmeasure: 0.9422 - val_loss: 6.8986 - val_acc: 0.3637 - val_top_k_categorical_accuracy: 0.6958 - val_precision: 0.3728 - val_recall: 0.3593 - val_fmeasure: 0.3658\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.0679 - acc: 0.9790 - top_k_categorical_accuracy: 0.9999 - precision: 0.9805 - recall: 0.9780 - fmeasure: 0.9793 - val_loss: 7.0847 - val_acc: 0.3676 - val_top_k_categorical_accuracy: 0.6987 - val_precision: 0.3755 - val_recall: 0.3633 - val_fmeasure: 0.3693\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.0524 - acc: 0.9844 - top_k_categorical_accuracy: 1.0000 - precision: 0.9851 - recall: 0.9838 - fmeasure: 0.9845 - val_loss: 7.2280 - val_acc: 0.3677 - val_top_k_categorical_accuracy: 0.6986 - val_precision: 0.3732 - val_recall: 0.3633 - val_fmeasure: 0.3681\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.0423 - acc: 0.9874 - top_k_categorical_accuracy: 0.9999 - precision: 0.9881 - recall: 0.9870 - fmeasure: 0.9875 - val_loss: 7.2736 - val_acc: 0.3731 - val_top_k_categorical_accuracy: 0.7023 - val_precision: 0.3789 - val_recall: 0.3691 - val_fmeasure: 0.3739\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.0395 - acc: 0.9885 - top_k_categorical_accuracy: 0.9999 - precision: 0.9890 - recall: 0.9881 - fmeasure: 0.9885 - val_loss: 7.3662 - val_acc: 0.3713 - val_top_k_categorical_accuracy: 0.6976 - val_precision: 0.3766 - val_recall: 0.3674 - val_fmeasure: 0.3719\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.0319 - acc: 0.9907 - top_k_categorical_accuracy: 1.0000 - precision: 0.9911 - recall: 0.9904 - fmeasure: 0.9908 - val_loss: 7.3824 - val_acc: 0.3717 - val_top_k_categorical_accuracy: 0.6976 - val_precision: 0.3768 - val_recall: 0.3680 - val_fmeasure: 0.3723\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.0371 - acc: 0.9898 - top_k_categorical_accuracy: 1.0000 - precision: 0.9901 - recall: 0.9895 - fmeasure: 0.9898 - val_loss: 7.4358 - val_acc: 0.3693 - val_top_k_categorical_accuracy: 0.7009 - val_precision: 0.3749 - val_recall: 0.3662 - val_fmeasure: 0.3705\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.0348 - acc: 0.9900 - top_k_categorical_accuracy: 1.0000 - precision: 0.9904 - recall: 0.9897 - fmeasure: 0.9901 - val_loss: 7.4659 - val_acc: 0.3702 - val_top_k_categorical_accuracy: 0.7001 - val_precision: 0.3748 - val_recall: 0.3672 - val_fmeasure: 0.3709\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 117us/step - loss: 0.0317 - acc: 0.9912 - top_k_categorical_accuracy: 1.0000 - precision: 0.9914 - recall: 0.9908 - fmeasure: 0.9911 - val_loss: 7.5147 - val_acc: 0.3673 - val_top_k_categorical_accuracy: 0.6973 - val_precision: 0.3723 - val_recall: 0.3641 - val_fmeasure: 0.3681\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.0325 - acc: 0.9906 - top_k_categorical_accuracy: 0.9999 - precision: 0.9910 - recall: 0.9904 - fmeasure: 0.9907 - val_loss: 7.5209 - val_acc: 0.3714 - val_top_k_categorical_accuracy: 0.6998 - val_precision: 0.3766 - val_recall: 0.3682 - val_fmeasure: 0.3723\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.0296 - acc: 0.9922 - top_k_categorical_accuracy: 1.0000 - precision: 0.9924 - recall: 0.9919 - fmeasure: 0.9922 - val_loss: 7.5679 - val_acc: 0.3695 - val_top_k_categorical_accuracy: 0.6952 - val_precision: 0.3746 - val_recall: 0.3663 - val_fmeasure: 0.3704\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.0324 - acc: 0.9911 - top_k_categorical_accuracy: 1.0000 - precision: 0.9913 - recall: 0.9908 - fmeasure: 0.9911 - val_loss: 7.6099 - val_acc: 0.3710 - val_top_k_categorical_accuracy: 0.7020 - val_precision: 0.3753 - val_recall: 0.3681 - val_fmeasure: 0.3716\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.0315 - acc: 0.9912 - top_k_categorical_accuracy: 1.0000 - precision: 0.9914 - recall: 0.9909 - fmeasure: 0.9911 - val_loss: 7.6258 - val_acc: 0.3697 - val_top_k_categorical_accuracy: 0.7005 - val_precision: 0.3738 - val_recall: 0.3669 - val_fmeasure: 0.3703\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 117us/step - loss: 0.0286 - acc: 0.9918 - top_k_categorical_accuracy: 1.0000 - precision: 0.9919 - recall: 0.9916 - fmeasure: 0.9918 - val_loss: 7.6611 - val_acc: 0.3702 - val_top_k_categorical_accuracy: 0.7011 - val_precision: 0.3745 - val_recall: 0.3678 - val_fmeasure: 0.3711\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 117us/step - loss: 0.0302 - acc: 0.9919 - top_k_categorical_accuracy: 1.0000 - precision: 0.9920 - recall: 0.9916 - fmeasure: 0.9918 - val_loss: 7.7267 - val_acc: 0.3671 - val_top_k_categorical_accuracy: 0.6942 - val_precision: 0.3718 - val_recall: 0.3648 - val_fmeasure: 0.3683\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 117us/step - loss: 0.0302 - acc: 0.9917 - top_k_categorical_accuracy: 0.9999 - precision: 0.9919 - recall: 0.9914 - fmeasure: 0.9917 - val_loss: 7.6606 - val_acc: 0.3699 - val_top_k_categorical_accuracy: 0.6963 - val_precision: 0.3745 - val_recall: 0.3669 - val_fmeasure: 0.3706\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.0336 - acc: 0.9913 - top_k_categorical_accuracy: 1.0000 - precision: 0.9916 - recall: 0.9912 - fmeasure: 0.9914 - val_loss: 7.6584 - val_acc: 0.3681 - val_top_k_categorical_accuracy: 0.6984 - val_precision: 0.3721 - val_recall: 0.3653 - val_fmeasure: 0.3686\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 117us/step - loss: 0.0276 - acc: 0.9926 - top_k_categorical_accuracy: 0.9999 - precision: 0.9928 - recall: 0.9925 - fmeasure: 0.9927 - val_loss: 7.7365 - val_acc: 0.3722 - val_top_k_categorical_accuracy: 0.7030 - val_precision: 0.3766 - val_recall: 0.3700 - val_fmeasure: 0.3733\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.0283 - acc: 0.9925 - top_k_categorical_accuracy: 0.9999 - precision: 0.9927 - recall: 0.9923 - fmeasure: 0.9925 - val_loss: 7.7820 - val_acc: 0.3688 - val_top_k_categorical_accuracy: 0.6978 - val_precision: 0.3727 - val_recall: 0.3664 - val_fmeasure: 0.3695\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 124us/step - loss: 0.0258 - acc: 0.9931 - top_k_categorical_accuracy: 1.0000 - precision: 0.9933 - recall: 0.9930 - fmeasure: 0.9932 - val_loss: 7.8159 - val_acc: 0.3735 - val_top_k_categorical_accuracy: 0.7023 - val_precision: 0.3779 - val_recall: 0.3714 - val_fmeasure: 0.3746\n",
            "shuffling......\n",
            "00% data has been dealed\n",
            "10% data has been dealed\n",
            "20% data has been dealed\n",
            "30% data has been dealed\n",
            "40% data has been dealed\n",
            "50% data has been dealed\n",
            "60% data has been dealed\n",
            "70% data has been dealed\n",
            "80% data has been dealed\n",
            "90% data has been dealed\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 123us/step - loss: 3.1095 - acc: 0.1782 - top_k_categorical_accuracy: 0.4914 - precision: 0.4042 - recall: 0.0222 - fmeasure: 0.0412 - val_loss: 2.6731 - val_acc: 0.2652 - val_top_k_categorical_accuracy: 0.6314 - val_precision: 0.5782 - val_recall: 0.0643 - val_fmeasure: 0.1144\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 114us/step - loss: 2.2609 - acc: 0.3566 - top_k_categorical_accuracy: 0.7365 - precision: 0.6582 - recall: 0.1350 - fmeasure: 0.2216 - val_loss: 2.4777 - val_acc: 0.3145 - val_top_k_categorical_accuracy: 0.6930 - val_precision: 0.5705 - val_recall: 0.1030 - val_fmeasure: 0.1728\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 115us/step - loss: 1.7680 - acc: 0.4800 - top_k_categorical_accuracy: 0.8335 - precision: 0.7235 - recall: 0.2865 - fmeasure: 0.4081 - val_loss: 2.5068 - val_acc: 0.3417 - val_top_k_categorical_accuracy: 0.7120 - val_precision: 0.5247 - val_recall: 0.1887 - val_fmeasure: 0.2762\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 114us/step - loss: 1.3844 - acc: 0.5804 - top_k_categorical_accuracy: 0.8932 - precision: 0.7692 - recall: 0.4302 - fmeasure: 0.5501 - val_loss: 2.7330 - val_acc: 0.3291 - val_top_k_categorical_accuracy: 0.6954 - val_precision: 0.4626 - val_recall: 0.2163 - val_fmeasure: 0.2937\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 114us/step - loss: 1.0767 - acc: 0.6718 - top_k_categorical_accuracy: 0.9303 - precision: 0.8125 - recall: 0.5571 - fmeasure: 0.6597 - val_loss: 2.9234 - val_acc: 0.3294 - val_top_k_categorical_accuracy: 0.6943 - val_precision: 0.4336 - val_recall: 0.2344 - val_fmeasure: 0.3035\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 115us/step - loss: 0.8627 - acc: 0.7315 - top_k_categorical_accuracy: 0.9528 - precision: 0.8388 - recall: 0.6446 - fmeasure: 0.7281 - val_loss: 3.2780 - val_acc: 0.3225 - val_top_k_categorical_accuracy: 0.6827 - val_precision: 0.4061 - val_recall: 0.2491 - val_fmeasure: 0.3080\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 114us/step - loss: 0.6863 - acc: 0.7851 - top_k_categorical_accuracy: 0.9676 - precision: 0.8690 - recall: 0.7220 - fmeasure: 0.7880 - val_loss: 3.6397 - val_acc: 0.3251 - val_top_k_categorical_accuracy: 0.6765 - val_precision: 0.3865 - val_recall: 0.2658 - val_fmeasure: 0.3144\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 114us/step - loss: 0.5561 - acc: 0.8244 - top_k_categorical_accuracy: 0.9777 - precision: 0.8884 - recall: 0.7753 - fmeasure: 0.8276 - val_loss: 3.9139 - val_acc: 0.3189 - val_top_k_categorical_accuracy: 0.6702 - val_precision: 0.3735 - val_recall: 0.2745 - val_fmeasure: 0.3160\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.4485 - acc: 0.8574 - top_k_categorical_accuracy: 0.9847 - precision: 0.9048 - recall: 0.8200 - fmeasure: 0.8599 - val_loss: 4.3130 - val_acc: 0.3153 - val_top_k_categorical_accuracy: 0.6639 - val_precision: 0.3581 - val_recall: 0.2805 - val_fmeasure: 0.3142\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 114us/step - loss: 0.3649 - acc: 0.8842 - top_k_categorical_accuracy: 0.9890 - precision: 0.9200 - recall: 0.8549 - fmeasure: 0.8859 - val_loss: 4.6789 - val_acc: 0.3193 - val_top_k_categorical_accuracy: 0.6604 - val_precision: 0.3539 - val_recall: 0.2915 - val_fmeasure: 0.3194\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 113us/step - loss: 0.3025 - acc: 0.9031 - top_k_categorical_accuracy: 0.9933 - precision: 0.9306 - recall: 0.8807 - fmeasure: 0.9047 - val_loss: 4.9796 - val_acc: 0.3134 - val_top_k_categorical_accuracy: 0.6537 - val_precision: 0.3429 - val_recall: 0.2908 - val_fmeasure: 0.3145\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 114us/step - loss: 0.2619 - acc: 0.9166 - top_k_categorical_accuracy: 0.9950 - precision: 0.9377 - recall: 0.8986 - fmeasure: 0.9176 - val_loss: 5.3310 - val_acc: 0.3058 - val_top_k_categorical_accuracy: 0.6540 - val_precision: 0.3300 - val_recall: 0.2868 - val_fmeasure: 0.3067\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 114us/step - loss: 0.2331 - acc: 0.9244 - top_k_categorical_accuracy: 0.9962 - precision: 0.9424 - recall: 0.9094 - fmeasure: 0.9255 - val_loss: 5.3805 - val_acc: 0.3128 - val_top_k_categorical_accuracy: 0.6510 - val_precision: 0.3387 - val_recall: 0.2946 - val_fmeasure: 0.3149\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 114us/step - loss: 0.1926 - acc: 0.9390 - top_k_categorical_accuracy: 0.9978 - precision: 0.9520 - recall: 0.9272 - fmeasure: 0.9393 - val_loss: 5.6802 - val_acc: 0.3071 - val_top_k_categorical_accuracy: 0.6483 - val_precision: 0.3276 - val_recall: 0.2918 - val_fmeasure: 0.3085\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 114us/step - loss: 0.1673 - acc: 0.9479 - top_k_categorical_accuracy: 0.9984 - precision: 0.9582 - recall: 0.9387 - fmeasure: 0.9483 - val_loss: 6.0003 - val_acc: 0.3135 - val_top_k_categorical_accuracy: 0.6595 - val_precision: 0.3313 - val_recall: 0.3023 - val_fmeasure: 0.3160\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 113us/step - loss: 0.1464 - acc: 0.9530 - top_k_categorical_accuracy: 0.9990 - precision: 0.9610 - recall: 0.9452 - fmeasure: 0.9530 - val_loss: 6.1956 - val_acc: 0.3071 - val_top_k_categorical_accuracy: 0.6477 - val_precision: 0.3222 - val_recall: 0.2956 - val_fmeasure: 0.3082\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 114us/step - loss: 0.1429 - acc: 0.9550 - top_k_categorical_accuracy: 0.9991 - precision: 0.9616 - recall: 0.9491 - fmeasure: 0.9552 - val_loss: 6.0224 - val_acc: 0.3037 - val_top_k_categorical_accuracy: 0.6368 - val_precision: 0.3231 - val_recall: 0.2913 - val_fmeasure: 0.3063\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 115us/step - loss: 0.1269 - acc: 0.9609 - top_k_categorical_accuracy: 0.9994 - precision: 0.9662 - recall: 0.9554 - fmeasure: 0.9607 - val_loss: 6.2297 - val_acc: 0.3083 - val_top_k_categorical_accuracy: 0.6455 - val_precision: 0.3240 - val_recall: 0.2964 - val_fmeasure: 0.3095\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 119us/step - loss: 0.1216 - acc: 0.9624 - top_k_categorical_accuracy: 0.9995 - precision: 0.9672 - recall: 0.9580 - fmeasure: 0.9625 - val_loss: 6.3774 - val_acc: 0.3090 - val_top_k_categorical_accuracy: 0.6417 - val_precision: 0.3236 - val_recall: 0.2979 - val_fmeasure: 0.3101\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 114us/step - loss: 0.1112 - acc: 0.9658 - top_k_categorical_accuracy: 0.9995 - precision: 0.9701 - recall: 0.9617 - fmeasure: 0.9659 - val_loss: 6.3859 - val_acc: 0.3103 - val_top_k_categorical_accuracy: 0.6461 - val_precision: 0.3253 - val_recall: 0.3002 - val_fmeasure: 0.3121\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 115us/step - loss: 0.0926 - acc: 0.9718 - top_k_categorical_accuracy: 0.9996 - precision: 0.9744 - recall: 0.9685 - fmeasure: 0.9714 - val_loss: 6.7273 - val_acc: 0.3113 - val_top_k_categorical_accuracy: 0.6449 - val_precision: 0.3240 - val_recall: 0.3034 - val_fmeasure: 0.3133\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 18s 101us/step - loss: 2.6390 - acc: 0.2729 - top_k_categorical_accuracy: 0.6438 - precision: 0.5639 - recall: 0.0782 - fmeasure: 0.1349 - val_loss: 2.6178 - val_acc: 0.3076 - val_top_k_categorical_accuracy: 0.6556 - val_precision: 0.5547 - val_recall: 0.1192 - val_fmeasure: 0.1946\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 113us/step - loss: 0.5517 - acc: 0.8238 - top_k_categorical_accuracy: 0.9829 - precision: 0.8778 - recall: 0.7741 - fmeasure: 0.8196 - val_loss: 4.3324 - val_acc: 0.3409 - val_top_k_categorical_accuracy: 0.6883 - val_precision: 0.3748 - val_recall: 0.3140 - val_fmeasure: 0.3415\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 18s 100us/step - loss: 2.1650 - acc: 0.3709 - top_k_categorical_accuracy: 0.7523 - precision: 0.6483 - recall: 0.1653 - fmeasure: 0.2610 - val_loss: 2.6776 - val_acc: 0.3114 - val_top_k_categorical_accuracy: 0.6618 - val_precision: 0.5102 - val_recall: 0.1545 - val_fmeasure: 0.2355\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 113us/step - loss: 0.5707 - acc: 0.8152 - top_k_categorical_accuracy: 0.9836 - precision: 0.8700 - recall: 0.7658 - fmeasure: 0.8128 - val_loss: 4.0423 - val_acc: 0.3420 - val_top_k_categorical_accuracy: 0.6965 - val_precision: 0.3844 - val_recall: 0.3115 - val_fmeasure: 0.3438\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 18s 101us/step - loss: 1.8264 - acc: 0.4556 - top_k_categorical_accuracy: 0.8201 - precision: 0.7014 - recall: 0.2655 - fmeasure: 0.3828 - val_loss: 2.8816 - val_acc: 0.3009 - val_top_k_categorical_accuracy: 0.6480 - val_precision: 0.4680 - val_recall: 0.1720 - val_fmeasure: 0.2502\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 114us/step - loss: 0.5239 - acc: 0.8307 - top_k_categorical_accuracy: 0.9863 - precision: 0.8779 - recall: 0.7878 - fmeasure: 0.8292 - val_loss: 4.2141 - val_acc: 0.3455 - val_top_k_categorical_accuracy: 0.6979 - val_precision: 0.3826 - val_recall: 0.3181 - val_fmeasure: 0.3471\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 18s 101us/step - loss: 1.5636 - acc: 0.5259 - top_k_categorical_accuracy: 0.8650 - precision: 0.7380 - recall: 0.3592 - fmeasure: 0.4811 - val_loss: 3.1274 - val_acc: 0.2938 - val_top_k_categorical_accuracy: 0.6456 - val_precision: 0.4123 - val_recall: 0.1908 - val_fmeasure: 0.2597\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 114us/step - loss: 0.4774 - acc: 0.8437 - top_k_categorical_accuracy: 0.9889 - precision: 0.8847 - recall: 0.8082 - fmeasure: 0.8440 - val_loss: 4.3806 - val_acc: 0.3458 - val_top_k_categorical_accuracy: 0.6942 - val_precision: 0.3786 - val_recall: 0.3200 - val_fmeasure: 0.3467\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 18s 101us/step - loss: 1.3402 - acc: 0.5870 - top_k_categorical_accuracy: 0.8999 - precision: 0.7660 - recall: 0.4437 - fmeasure: 0.5601 - val_loss: 3.3862 - val_acc: 0.2929 - val_top_k_categorical_accuracy: 0.6309 - val_precision: 0.3951 - val_recall: 0.2110 - val_fmeasure: 0.2743\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 113us/step - loss: 0.4314 - acc: 0.8594 - top_k_categorical_accuracy: 0.9916 - precision: 0.8932 - recall: 0.8287 - fmeasure: 0.8591 - val_loss: 4.5969 - val_acc: 0.3430 - val_top_k_categorical_accuracy: 0.6908 - val_precision: 0.3751 - val_recall: 0.3208 - val_fmeasure: 0.3456\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 18s 101us/step - loss: 1.1578 - acc: 0.6387 - top_k_categorical_accuracy: 0.9238 - precision: 0.7900 - recall: 0.5156 - fmeasure: 0.6225 - val_loss: 3.6903 - val_acc: 0.2981 - val_top_k_categorical_accuracy: 0.6323 - val_precision: 0.3768 - val_recall: 0.2296 - val_fmeasure: 0.2847\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 113us/step - loss: 0.3868 - acc: 0.8735 - top_k_categorical_accuracy: 0.9932 - precision: 0.9016 - recall: 0.8485 - fmeasure: 0.8738 - val_loss: 4.8428 - val_acc: 0.3386 - val_top_k_categorical_accuracy: 0.6872 - val_precision: 0.3663 - val_recall: 0.3184 - val_fmeasure: 0.3404\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 18s 101us/step - loss: 1.0016 - acc: 0.6824 - top_k_categorical_accuracy: 0.9433 - precision: 0.8085 - recall: 0.5791 - fmeasure: 0.6737 - val_loss: 4.0945 - val_acc: 0.2852 - val_top_k_categorical_accuracy: 0.6217 - val_precision: 0.3486 - val_recall: 0.2331 - val_fmeasure: 0.2788\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 113us/step - loss: 0.3548 - acc: 0.8836 - top_k_categorical_accuracy: 0.9948 - precision: 0.9079 - recall: 0.8629 - fmeasure: 0.8845 - val_loss: 5.2297 - val_acc: 0.3336 - val_top_k_categorical_accuracy: 0.6745 - val_precision: 0.3573 - val_recall: 0.3165 - val_fmeasure: 0.3355\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 18s 100us/step - loss: 0.8696 - acc: 0.7218 - top_k_categorical_accuracy: 0.9567 - precision: 0.8268 - recall: 0.6345 - fmeasure: 0.7171 - val_loss: 4.5060 - val_acc: 0.2783 - val_top_k_categorical_accuracy: 0.6096 - val_precision: 0.3260 - val_recall: 0.2338 - val_fmeasure: 0.2719\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 113us/step - loss: 0.3270 - acc: 0.8915 - top_k_categorical_accuracy: 0.9957 - precision: 0.9114 - recall: 0.8742 - fmeasure: 0.8921 - val_loss: 5.4266 - val_acc: 0.3323 - val_top_k_categorical_accuracy: 0.6809 - val_precision: 0.3514 - val_recall: 0.3161 - val_fmeasure: 0.3327\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 18s 100us/step - loss: 0.7590 - acc: 0.7554 - top_k_categorical_accuracy: 0.9673 - precision: 0.8423 - recall: 0.6808 - fmeasure: 0.7523 - val_loss: 4.6642 - val_acc: 0.2824 - val_top_k_categorical_accuracy: 0.6176 - val_precision: 0.3277 - val_recall: 0.2451 - val_fmeasure: 0.2802\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.2995 - acc: 0.9019 - top_k_categorical_accuracy: 0.9963 - precision: 0.9194 - recall: 0.8876 - fmeasure: 0.9030 - val_loss: 5.6617 - val_acc: 0.3286 - val_top_k_categorical_accuracy: 0.6723 - val_precision: 0.3478 - val_recall: 0.3152 - val_fmeasure: 0.3306\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 18s 100us/step - loss: 0.6572 - acc: 0.7869 - top_k_categorical_accuracy: 0.9761 - precision: 0.8579 - recall: 0.7253 - fmeasure: 0.7854 - val_loss: 4.9228 - val_acc: 0.2842 - val_top_k_categorical_accuracy: 0.6133 - val_precision: 0.3211 - val_recall: 0.2493 - val_fmeasure: 0.2804\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 113us/step - loss: 0.2703 - acc: 0.9119 - top_k_categorical_accuracy: 0.9974 - precision: 0.9247 - recall: 0.9007 - fmeasure: 0.9124 - val_loss: 5.8216 - val_acc: 0.3307 - val_top_k_categorical_accuracy: 0.6703 - val_precision: 0.3475 - val_recall: 0.3179 - val_fmeasure: 0.3319\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 18s 100us/step - loss: 0.5750 - acc: 0.8119 - top_k_categorical_accuracy: 0.9821 - precision: 0.8705 - recall: 0.7622 - fmeasure: 0.8122 - val_loss: 5.2280 - val_acc: 0.2838 - val_top_k_categorical_accuracy: 0.6118 - val_precision: 0.3180 - val_recall: 0.2566 - val_fmeasure: 0.2838\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 113us/step - loss: 0.2551 - acc: 0.9172 - top_k_categorical_accuracy: 0.9976 - precision: 0.9292 - recall: 0.9075 - fmeasure: 0.9181 - val_loss: 6.0219 - val_acc: 0.3234 - val_top_k_categorical_accuracy: 0.6676 - val_precision: 0.3399 - val_recall: 0.3117 - val_fmeasure: 0.3251\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 18s 101us/step - loss: 0.5114 - acc: 0.8323 - top_k_categorical_accuracy: 0.9863 - precision: 0.8808 - recall: 0.7899 - fmeasure: 0.8325 - val_loss: 5.5825 - val_acc: 0.2812 - val_top_k_categorical_accuracy: 0.6078 - val_precision: 0.3080 - val_recall: 0.2577 - val_fmeasure: 0.2804\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 118us/step - loss: 0.2461 - acc: 0.9209 - top_k_categorical_accuracy: 0.9980 - precision: 0.9314 - recall: 0.9123 - fmeasure: 0.9217 - val_loss: 6.0666 - val_acc: 0.3283 - val_top_k_categorical_accuracy: 0.6698 - val_precision: 0.3436 - val_recall: 0.3163 - val_fmeasure: 0.3293\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 18s 100us/step - loss: 0.4537 - acc: 0.8507 - top_k_categorical_accuracy: 0.9898 - precision: 0.8905 - recall: 0.8156 - fmeasure: 0.8510 - val_loss: 5.7377 - val_acc: 0.2796 - val_top_k_categorical_accuracy: 0.6049 - val_precision: 0.3053 - val_recall: 0.2582 - val_fmeasure: 0.2796\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 113us/step - loss: 0.2175 - acc: 0.9304 - top_k_categorical_accuracy: 0.9982 - precision: 0.9388 - recall: 0.9238 - fmeasure: 0.9312 - val_loss: 6.3606 - val_acc: 0.3264 - val_top_k_categorical_accuracy: 0.6681 - val_precision: 0.3390 - val_recall: 0.3166 - val_fmeasure: 0.3273\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "179550/179550 [==============================] - 18s 101us/step - loss: 0.3983 - acc: 0.8680 - top_k_categorical_accuracy: 0.9927 - precision: 0.9011 - recall: 0.8388 - fmeasure: 0.8686 - val_loss: 6.0269 - val_acc: 0.2701 - val_top_k_categorical_accuracy: 0.5975 - val_precision: 0.2935 - val_recall: 0.2511 - val_fmeasure: 0.2705\n",
            "Train on 59850 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            "59850/59850 [==============================] - 7s 113us/step - loss: 0.2126 - acc: 0.9338 - top_k_categorical_accuracy: 0.9985 - precision: 0.9415 - recall: 0.9272 - fmeasure: 0.9342 - val_loss: 6.5235 - val_acc: 0.3239 - val_top_k_categorical_accuracy: 0.6653 - val_precision: 0.3364 - val_recall: 0.3158 - val_fmeasure: 0.3257\n",
            "Train on 179550 samples, validate on 25650 samples\n",
            "Epoch 1/1\n",
            " 73984/179550 [===========>..................] - ETA: 9s - loss: 0.3316 - acc: 0.8922 - top_k_categorical_accuracy: 0.9949 - precision: 0.9177 - recall: 0.8683 - fmeasure: 0.8921Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6Py6svpp1YU",
        "colab_type": "code",
        "outputId": "0b96464f-c5a9-49c1-e6cc-781e1780a915",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "#DNN               ############# test for noise adding  , function :: creat_data############\n",
        "\n",
        "history_DNN = []\n",
        "for i in range(NB_TECHS):\n",
        "    his = my_history()\n",
        "    X_temp,y_temp = random_launch(X_train_saved,y_train_saved,i)\n",
        "    X_temp = sequence.pad_sequences(X_temp, maxlen=maxlen,padding='post',truncating='post')\n",
        "    y_temp = np_utils.to_categorical(y_temp, NB_CLASSES)\n",
        "    X_train = sequence.pad_sequences(X_train_saved, maxlen=maxlen,padding='post',truncating='post')\n",
        "    y_train = np_utils.to_categorical(y_train_saved, NB_CLASSES)\n",
        "    X_test = sequence.pad_sequences(X_test_saved, maxlen=maxlen,padding='post',truncating='post')\n",
        "    y_test = np_utils.to_categorical(y_test_saved, NB_CLASSES)\n",
        "    model_DNN = DNN(maxlen,NB_CLASSES)\n",
        "    #model_DNN.summary()\n",
        "    model_DNN.compile(loss='categorical_crossentropy',optimizer=OPTIMIZER,metrics=['accuracy','top_k_categorical_accuracy', precision, recall, fmeasure])\n",
        "    run_DNN(X_train,y_train,X_test,y_test,X_temp,y_temp,BATCH_SIZE,history = his)\n",
        "    history_DNN.append(his)\n",
        "\n",
        "\n",
        "print('the shape of x_train',X_train.shape)\n",
        "print('the shape of y_train',y_train.shape)\n",
        "print('the shape of x_test',X_test.shape)\n",
        "print('the shape of y_test',y_test.shape)\n",
        "\n",
        "print('base_line_now#######################################################################################################################################################################################################')\n",
        "model_DNN_org = DNN(maxlen,NB_CLASSES)\n",
        "model_DNN_org.compile(loss='categorical_crossentropy',optimizer=OPTIMIZER,metrics=['accuracy','top_k_categorical_accuracy', precision, recall, fmeasure])\n",
        "history_DNN_org = model_DNN_org.fit(X_train,y_train,batch_size=BATCH_SIZE,epochs=NB_EPOCHS,validation_data=(X_test,y_test),verbose=1)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adding noise......\n",
            "00% data has been dealed\n",
            "10% data has been dealed\n",
            "20% data has been dealed\n",
            "30% data has been dealed\n",
            "40% data has been dealed\n",
            "50% data has been dealed\n",
            "60% data has been dealed\n",
            "70% data has been dealed\n",
            "80% data has been dealed\n",
            "90% data has been dealed\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-e5e64e574e1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_saved\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtruncating\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_saved\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNB_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmodel_DNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNB_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;31m#model_DNN.summary()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmodel_DNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOPTIMIZER\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'top_k_categorical_accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmeasure\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-8b2d55cbd161>\u001b[0m in \u001b[0;36mDNN\u001b[0;34m(input_shape, classes)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGlobalAveragePooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#parimatier is not defined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_source_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 raise TypeError('All layers in a Sequential model '\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    303\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m                                      str(K.ndim(x)))\n\u001b[0m\u001b[1;32m    306\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer global_average_pooling1d_1: expected ndim=3, found ndim=2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PCV4GTZ3Vjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CNN               ############# test for noise adding  , function :: creat_data############\n",
        "\n",
        "\n",
        "history_CNN = []\n",
        "for i in range(NB_TECHS):\n",
        "    his = my_history()\n",
        "    X_temp,y_temp = random_launch(X_train_saved,y_train_saved,i)\n",
        "    X_temp = sequence.pad_sequences(X_temp, maxlen=maxlen,padding='post',truncating='post')\n",
        "    X_temp = X_temp[:,:,np.newaxis]\n",
        "    y_temp = np_utils.to_categorical(y_temp, NB_CLASSES)\n",
        "    X_train = sequence.pad_sequences(X_train_saved, maxlen=maxlen,padding='post',truncating='post')\n",
        "    X_train = X_train[:,:,np.newaxis]\n",
        "    y_train = np_utils.to_categorical(y_train_saved, NB_CLASSES)\n",
        "    X_test = sequence.pad_sequences(X_test_saved, maxlen=maxlen,padding='post',truncating='post')\n",
        "    X_test = X_test[:,:,np.newaxis]\n",
        "    y_test = np_utils.to_categorical(y_test_saved, NB_CLASSES)\n",
        "    model_CNN = CNN(maxlen,NB_CLASSES)\n",
        "    model_CNN.compile(loss='categorical_crossentropy',optimizer=OPTIMIZER,metrics=['accuracy','top_k_categorical_accuracy', precision, recall, fmeasure])\n",
        "    run_CNN(X_train,y_train,X_test,y_test,X_temp,y_temp,BATCH_SIZE,history = his)\n",
        "    history_CNN.append(his)\n",
        "\n",
        "print('the shape of x_train',X_train.shape)\n",
        "print('the shape of y_train',y_train.shape)\n",
        "print('the shape of x_test',X_test.shape)\n",
        "print('the shape of y_test',y_test.shape)\n",
        "\n",
        "print('base_line_now#######################################################################################################################################################################################################')\n",
        "model_CNN_org = CNN(maxlen,NB_CLASSES)\n",
        "model_CNN_org.compile(loss='categorical_crossentropy',optimizer=OPTIMIZER,metrics=['accuracy','top_k_categorical_accuracy', precision, recall, fmeasure])\n",
        "history_CNN_org = model_CNN_org.fit(X_train,y_train,batch_size=BATCH_SIZE,epochs=NB_EPOCHS,validation_data=(X_test,y_test),verbose=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syHL5o8x3Vaq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pylab as plt  \n",
        "\n",
        "LIST_label = ['noise','delete','shuffle','transplant','mixup']\n",
        "LIST_index = ['val_loss','val_acc','val_top_k_categorical_accuracy','val_precision','val_recall','val_fmeasure']\n",
        "NB_index = len(LIST_index)\n",
        "\n",
        "def my_plot(history_org,history_after): #\n",
        "    fig,axs = plt.subplots(NB_index,1,figsize=(30,100))\n",
        "    for j,index in zip(range(NB_index),LIST_index):  #for each index like: loss....acc ....top5\n",
        "        alpha = 1\n",
        "        axs[j].plot(range(NB_EPOCHS),history_org.history[index],label='before')\n",
        "        #axs[j].annotate('before'+str(round(sum(history_org.history[index][-6:-1])/5,2)),(NB_EPOCHS,sum(history_org.history[index][-6:-1])/5),xytext=(NB_EPOCHS*alpha,sum(history_org.history[index][-6:-1])*alpha/5),arrowprops=dict(arrowstyle='->'))\n",
        "        before = round(sum(history_org.history[index][-11:-1])/10,3)\n",
        "        print(\"############################################## split line for index####################################################################\")\n",
        "        for i in range(NB_TECHS): #for each techs i.e. each line with different color\n",
        "            alpha = alpha*0.6\n",
        "            axs[j].plot(range(NB_EPOCHS),history_after[i].history[index],label=LIST_label[i])\n",
        "            #axs[j].annotate(LIST_label[i]+str(round(sum(history_after[i].history[index][-6:-1])/5,2)),(NB_EPOCHS,sum(history_after[i].history[index][-6:-1])/5),xytext=(NB_EPOCHS*alpha,sum(history_after[i].history[index][-6:-1])*alpha/5),arrowprops=dict(arrowstyle='->'))\n",
        "            after = round(sum(history_after[i].history[index][-11:-1])/10,3)\n",
        "            t = LIST_label[i] + str(round(100*(after - before)/before,3)) + '% improved'\n",
        "            #h_max = max(history_after[i].history[index])\n",
        "            #axs[j].text(NB_EPOCHS*alpha, h_max, t, ha='left', wrap=True,  bbox=dict(boxstyle='round,pad=0.5', fc='yellow', ec='k',lw=1))\n",
        "            print(\"############################################## split line for techs ####################################################################\")\n",
        "            print(LIST_label[i])\n",
        "            print(str() + str(index) + \" before \"+ str(before))\n",
        "            print(str(index) + \" after \" + str(after))\n",
        "            print(str(index) + \" changed \"+ str(t) + \"% \")\n",
        "        #axs[j].text(NB_EPOCHS, history_DNN_org.history['val_precision'][-1]*1.1, t, ha='left', wrap=True,  bbox=dict(boxstyle='round,pad=0.5', fc='yellow', ec='k',lw=1))\n",
        "        axs[j].set_title(LIST_index[j])\n",
        "        axs[j].legend()\n",
        "\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ah_JCMAUj_Xh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_plot(history_MLP_org,history_MLP)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uih441fsP-BJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_plot(history_DNN_org,history_DNN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4LB3Lhv9AyC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_plot(history_CNN_org,history_CNN)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}