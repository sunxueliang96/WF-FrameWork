{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "locally_change.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOI47Vo11lwDVeFvbmebCd3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunxueliang96/WF-FrameWork/blob/master/locally_change_single_dataset(DNN%2BCNN%2BLSTM%2BCNN%2BLSTM).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3WCsIaUTuNV",
        "colab_type": "code",
        "outputId": "86eaaca2-c138-4bef-a286-02965047fed8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Mount Google Drive as folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF9tyC-TTxmk",
        "colab_type": "code",
        "outputId": "b50319b4-d2cd-4b3d-e520-a90b6c618603",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/drive/'My Drive'/datasets/no_paded/close_world/walkiebatch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/datasets/no_paded/close_world/walkiebatch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thC1b8glT6p9",
        "colab_type": "code",
        "outputId": "563f0788-3381-483e-9bb1-d779d1263bcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_walkiebatch.pkl  y_walkiebatch.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOpTb4DkT9db",
        "colab_type": "code",
        "outputId": "bf4166e0-b9e9-40c7-98b1-12c11c0c9184",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "print('loading data...')\n",
        "with open('X_walkiebatch.pkl','rb') as handle:\n",
        "  X = np.array(pickle.load(handle))\n",
        "with open('y_walkiebatch.pkl','rb') as handle:\n",
        "  y = np.array(pickle.load(handle))\n",
        "print('the shape of X',X.shape)\n",
        "print('the shape of y',y.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading data...\n",
            "the shape of X (11868,)\n",
            "the shape of y (11868,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBm0gBfZZ_UG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math \n",
        "import random\n",
        "def locally_changed(X,y,PERCENT_CHANGED,NB_TIMES):        # 1~100% of sequence is been shuffled\n",
        "    X = list(X)\n",
        "    y = list(y)\n",
        "    PERCENT_NOISE = PERCENT_CHANGED                #PERCENT of shuffled\n",
        "    NB_TIMES = NB_TIMES                            #TIMES of Magnification\n",
        "    X_sum = X.copy()\n",
        "    y_sum = y.copy()\n",
        "    print(\"{} samples has beed add to X_train, {}% of sequence has benn shuffled at each sample\".format(math.ceil(len(X)*NB_TIMES),PERCENT_CHANGED))\n",
        "    for i in range(math.ceil(len(X)*NB_TIMES)):\n",
        "        p = random.choice(range(len(X)))\n",
        "        X_new = X[p].copy()\n",
        "        y_new = y[p]\n",
        "\n",
        "        length = math.ceil(PERCENT_CHANGED*len(X_new)/100)\n",
        "        start = random.choice(range(len(X_new)-length))\n",
        "        temp = X_new[start:start+length]\n",
        "        random.shuffle(temp)\n",
        "        X_new[start:start+length] = temp\n",
        "\n",
        "        X_sum.append(X_new)\n",
        "        y_sum.append(y_new)\n",
        "    return np.array(X_sum),np.array(y_sum)                        #return two array of X, y \n",
        "'''\n",
        "def locally_delete():                  \n",
        "    pass\n",
        "def rnn_padding():                                  #a seq to seq model\n",
        "    pass\n",
        "def bayes_padding():\n",
        "    pass\n",
        "def mult_scale_windowing():\n",
        "    pass\n",
        "def GAN_padding():\n",
        "    pass\n",
        "def data_mixing():\n",
        "    pass\n",
        "    '''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZpMmHj3Uefl",
        "colab_type": "code",
        "outputId": "ab49ed62-422e-4442-9d01-253a4785c7b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import sequence\n",
        "from collections import Counter\n",
        "#CLASSES_KNN = 101########################################################################################################################################################################################################\n",
        "#CLASSES_WAKIE = 100########################################################################################################################################################################################################\n",
        "MAXLEN_KNN = 2000\n",
        "MAXLEN_WAKIE =5000\n",
        "\n",
        "maxlen = 5000\n",
        "NB_CLASSES = 0\n",
        "\n",
        "def choose():\n",
        "    global maxlen,NB_CLASSES\n",
        "    print('if padding, the max of length of seq is {}'.format(maxlen))\n",
        "    NB_CLASSES = len(Counter(y).keys())\n",
        "    print('number of classes is {}'.format(NB_CLASSES))\n",
        "\n",
        "choose()################################################################################################################################################################################################################\n",
        "\n",
        "#print('spliting data...')\n",
        "#X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
        "#X_train,y_train = creat_data(X_train,y_train,PERCENT_NOISE=0.5,NB_TIMES=1)\n",
        "\n",
        "#print(len(X_train), 'train sequences')\n",
        "#print(len(y_train), 'test sequences')\n",
        "print('Average sequence length: {}'.format(np.mean(list(map(len, X)), dtype=int)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "if padding, the max of length of seq is 5000\n",
            "number of classes is 108\n",
            "Average sequence length: 4471\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_gm7tqd4yEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#FCN\n",
        "from keras import Sequential\n",
        "from keras.layers.core import Activation, Flatten, Dense, Dropout\n",
        "from keras.optimizers import Adamax\n",
        "\n",
        "def DNN(input_shape, classes):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_shape=(input_shape,)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(256))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(512))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(1024))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(512))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(classes))\n",
        "    model.add(Activation('softmax'))\n",
        "    return model\n",
        "def run_DNN(X_train,y_train,X_test,y_test):\n",
        "    print('Pad sequences to ',maxlen)\n",
        "    x_train = X_train[:]\n",
        "    x_test = X_test[:]\n",
        "    x_train = sequence.pad_sequences(x_train, maxlen=maxlen,padding='post',truncating='post')\n",
        "    x_test = sequence.pad_sequences(x_test, maxlen=maxlen,padding='post',truncating='post')\n",
        "    y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
        "    y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
        "    print('the shape of x_train',x_train.shape)\n",
        "    print('the shape of y_train',y_train.shape)\n",
        "    print('the shape of x_test',x_test.shape)\n",
        "    print('the shape of y_test',y_test.shape)\n",
        "    model_DNN = DNN(maxlen,NB_CLASSES)\n",
        "    #model_DNN.summary()\n",
        "    model_DNN.compile(loss='categorical_crossentropy',optimizer=OPTIMIZER,metrics=['accuracy', precision, recall, fmeasure])\n",
        "    history = model_DNN.fit(x_train,y_train,batch_size=BATCH_SIZE,epochs=NB_EPOCH,validation_data=(x_test,y_test),verbose=0)\n",
        "    score = model_DNN.evaluate(x_test,y_test,verbose=VERBOSE)\n",
        "    #print(history.history)\n",
        "    #print(score)\n",
        "    return(history.history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9hQ5LZK6sFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CNN\n",
        "from keras import Input,Model,Sequential\n",
        "from keras.layers import Embedding,GlobalAveragePooling1D,Dense,Dropout\n",
        "from keras.layers import Conv1D, MaxPooling1D, BatchNormalization\n",
        "from keras.layers.core import Activation, Flatten, Dense, Dropout\n",
        "from keras.layers.advanced_activations import ELU\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras.optimizers import Adamax\n",
        "def CNN(input_shape, classes):\n",
        "    model = Sequential()\n",
        "    #Block1\n",
        "    filter_num = ['None',32,64,128,256]\n",
        "    kernel_size = ['None',8,8,8,8]\n",
        "    conv_stride_size = ['None',1,1,1,1]\n",
        "    pool_stride_size = ['None',4,4,4,4]\n",
        "    pool_size = ['None',8,8,8,8]\n",
        "\n",
        "    model.add(Conv1D(filters=filter_num[1], kernel_size=kernel_size[1], input_shape=(input_shape,1),\n",
        "                      strides=conv_stride_size[1], padding='same',\n",
        "                      name='block1_conv1'))\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "    model.add(ELU(alpha=1.0, name='block1_adv_act1'))\n",
        "    model.add(Conv1D(filters=filter_num[1], kernel_size=kernel_size[1],\n",
        "                      strides=conv_stride_size[1], padding='same',\n",
        "                      name='block1_conv2'))\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "    model.add(ELU(alpha=1.0, name='block1_adv_act2'))\n",
        "    model.add(MaxPooling1D(pool_size=pool_size[1], strides=pool_stride_size[1],\n",
        "                            padding='same', name='block1_pool'))\n",
        "    model.add(Dropout(0.1, name='block1_dropout'))\n",
        "\n",
        "    model.add(Conv1D(filters=filter_num[2], kernel_size=kernel_size[2],\n",
        "                      strides=conv_stride_size[2], padding='same',\n",
        "                      name='block2_conv1'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu', name='block2_act1'))\n",
        "\n",
        "    model.add(Conv1D(filters=filter_num[2], kernel_size=kernel_size[2],\n",
        "                      strides=conv_stride_size[2], padding='same',\n",
        "                      name='block2_conv2'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu', name='block2_act2'))\n",
        "    model.add(MaxPooling1D(pool_size=pool_size[2], strides=pool_stride_size[3],\n",
        "                            padding='same', name='block2_pool'))\n",
        "    model.add(Dropout(0.1, name='block2_dropout'))\n",
        "\n",
        "    model.add(Conv1D(filters=filter_num[3], kernel_size=kernel_size[3],\n",
        "                      strides=conv_stride_size[3], padding='same',\n",
        "                      name='block3_conv1'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu', name='block3_act1'))\n",
        "    model.add(Conv1D(filters=filter_num[3], kernel_size=kernel_size[3],\n",
        "                      strides=conv_stride_size[3], padding='same',\n",
        "                      name='block3_conv2'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu', name='block3_act2'))\n",
        "    model.add(MaxPooling1D(pool_size=pool_size[3], strides=pool_stride_size[3],\n",
        "                            padding='same', name='block3_pool'))\n",
        "    model.add(Dropout(0.1, name='block3_dropout'))\n",
        "\n",
        "    model.add(Conv1D(filters=filter_num[4], kernel_size=kernel_size[4],\n",
        "                      strides=conv_stride_size[4], padding='same',\n",
        "                      name='block4_conv1'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu', name='block4_act1'))\n",
        "    model.add(Conv1D(filters=filter_num[4], kernel_size=kernel_size[4],\n",
        "                      strides=conv_stride_size[4], padding='same',\n",
        "                      name='block4_conv2'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu', name='block4_act2'))\n",
        "    model.add(MaxPooling1D(pool_size=pool_size[4], strides=pool_stride_size[4],\n",
        "                            padding='same', name='block4_pool'))\n",
        "    model.add(Dropout(0.1, name='block4_dropout'))\n",
        "\n",
        "    model.add(Flatten(name='flatten'))\n",
        "    model.add(Dense(512, kernel_initializer=glorot_uniform(seed=0), name='fc1'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu', name='fc1_act'))\n",
        "\n",
        "    model.add(Dropout(0.7, name='fc1_dropout'))\n",
        "\n",
        "    model.add(Dense(512, kernel_initializer=glorot_uniform(seed=0), name='fc2'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu', name='fc2_act'))\n",
        "\n",
        "    model.add(Dropout(0.5, name='fc2_dropout'))\n",
        "\n",
        "    model.add(Dense(classes, kernel_initializer=glorot_uniform(seed=0), name='fc3'))\n",
        "    model.add(Activation('softmax', name=\"softmax\"))\n",
        "    return model\n",
        "def run_CNN(X_train,y_train,X_test,y_test):\n",
        "    print('Pad sequences to ',maxlen)\n",
        "    x_train = X_train[:]\n",
        "    x_test = X_test[:]\n",
        "    x_train = sequence.pad_sequences(x_train, maxlen=maxlen,padding='post',truncating='post')\n",
        "    x_test = sequence.pad_sequences(x_test, maxlen=maxlen,padding='post',truncating='post')\n",
        "    x_train = x_train[:,:,np.newaxis]\n",
        "    x_test = x_test[:,:,np.newaxis]\n",
        "    y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
        "    y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
        "    print('the shape of x_train',x_train.shape)\n",
        "    print('the shape of y_train',y_train.shape)\n",
        "    print('the shape of x_test',x_test.shape)\n",
        "    print('the shape of y_test',y_test.shape)\n",
        "    model_CNN = CNN(maxlen,NB_CLASSES)\n",
        "    #model_CNN.summary()\n",
        "    model_CNN.compile(loss='categorical_crossentropy',optimizer=OPTIMIZER,metrics=['accuracy', precision, recall, fmeasure])\n",
        "    history = model_CNN.fit(x_train,y_train,batch_size=BATCH_SIZE,epochs=NB_EPOCH,validation_data=(x_test,y_test),verbose=0)\n",
        "    score = model_CNN.evaluate(x_test,y_test,verbose=VERBOSE)\n",
        "    #print(score)\n",
        "    return(history.history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNxUC1-miWSA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#LSTM\n",
        "from keras import Input,Model,Sequential\n",
        "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, TimeDistributed\n",
        "from keras.layers.convolutional import Conv1D,MaxPooling1D\n",
        "from keras.layers.core import Activation, Flatten, Dense\n",
        "from keras.layers.advanced_activations import ELU\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras.optimizers import Adamax\n",
        "def my_LSTM(input_shape, classes):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(256,activation='relu',return_sequences=True,input_shape=(input_shape,1)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(LSTM(256,return_sequences=True))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(LSTM(512,return_sequences=False))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dense(classes, activation='softmax'))\n",
        "    return model\n",
        "\n",
        "def run_LSTM(X_train,y_train,X_test,y_test):\n",
        "    print('Pad sequences to ',maxlen)\n",
        "    x_train = X_train[:]\n",
        "    x_test = X_test[:]\n",
        "    x_train = sequence.pad_sequences(x_train, maxlen=maxlen,padding='post',truncating='post')\n",
        "    x_test = sequence.pad_sequences(x_test, maxlen=maxlen,padding='post',truncating='post')\n",
        "    x_train = x_train[:,:,np.newaxis]\n",
        "    x_test = x_test[:,:,np.newaxis]\n",
        "    y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
        "    y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
        "    print('the shape of x_train',x_train.shape)\n",
        "    print('the shape of y_train',y_train.shape)\n",
        "    print('the shape of x_test',x_test.shape)\n",
        "    print('the shape of y_test',y_test.shape)\n",
        "    model_LSTM = my_LSTM(maxlen,NB_CLASSES)\n",
        "    #model_LSTM.summary()\n",
        "    model_LSTM.compile(loss='categorical_crossentropy',optimizer=OPTIMIZER,metrics=['accuracy', precision, recall, fmeasure])\n",
        "    history = model_LSTM.fit(x_train,y_train,batch_size=BATCH_SIZE,epochs=NB_EPOCH,validation_data=(x_test,y_test),verbose=0)\n",
        "    score = model_LSTM.evaluate(x_test,y_test,verbose=VERBOSE)\n",
        "    return(history.history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-J-rv6Ri0r2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CNN+LSTM\n",
        "from keras import Input,Model,Sequential\n",
        "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, TimeDistributed,BatchNormalization\n",
        "from keras.layers.convolutional import Conv1D,MaxPooling1D\n",
        "from keras.layers.core import Activation, Flatten, Dense\n",
        "from keras.layers.advanced_activations import ELU\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras.optimizers import Adamax\n",
        "\n",
        "def CNN_LSTM(n_features,NB_SPLIT,classes):\n",
        "    model = Sequential()\n",
        "    '''\n",
        "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'),input_shape=(None,n_features//NB_SPLIT,1)))\n",
        "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))\n",
        "    model.add(TimeDistributed(Dropout(0.5)))\n",
        "    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
        "    model.add(TimeDistributed(Flatten()))\n",
        "    model.add(LSTM(256))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(classes, activation='softmax'))\n",
        "    '''\n",
        "    #Block1\n",
        "    filter_num = ['None',32,64,128,256]\n",
        "    kernel_size = ['None',8,8,8,8]\n",
        "    conv_stride_size = ['None',1,1,1,1]\n",
        "    pool_stride_size = ['None',4,4,4,4]\n",
        "    pool_size = ['None',8,8,8,8]\n",
        "    model.add(TimeDistributed(Conv1D(filters=filter_num[1], kernel_size=kernel_size[1],\n",
        "                      strides=conv_stride_size[1], padding='same',\n",
        "                      name='block1_conv1'),input_shape=(None,n_features//NB_SPLIT,1)))\n",
        "    model.add(TimeDistributed(BatchNormalization(axis=-1)))\n",
        "    model.add(TimeDistributed(ELU(alpha=1.0, name='block1_adv_act1')))\n",
        "    model.add(TimeDistributed(Conv1D(filters=filter_num[1], kernel_size=kernel_size[1],\n",
        "                      strides=conv_stride_size[1], padding='same',\n",
        "                      name='block1_conv2')))\n",
        "    model.add(TimeDistributed(BatchNormalization(axis=-1)))\n",
        "    model.add(TimeDistributed(ELU(alpha=1.0, name='block1_adv_act2')))\n",
        "    model.add(TimeDistributed(MaxPooling1D(pool_size=pool_size[1], strides=pool_stride_size[1],\n",
        "                            padding='same', name='block1_pool')))\n",
        "    model.add(TimeDistributed(Dropout(0.1, name='block1_dropout')))\n",
        "\n",
        "    model.add(TimeDistributed(Conv1D(filters=filter_num[2], kernel_size=kernel_size[2],\n",
        "                      strides=conv_stride_size[2], padding='same',\n",
        "                      name='block2_conv1')))\n",
        "    model.add(TimeDistributed(BatchNormalization()))\n",
        "    model.add(TimeDistributed(Activation('relu', name='block2_act1')))\n",
        "\n",
        "    model.add(TimeDistributed(Conv1D(filters=filter_num[2], kernel_size=kernel_size[2],\n",
        "                      strides=conv_stride_size[2], padding='same',\n",
        "                      name='block2_conv2')))\n",
        "    model.add(TimeDistributed(BatchNormalization()))\n",
        "    model.add(TimeDistributed(Activation('relu', name='block2_act2')))\n",
        "    model.add(TimeDistributed(MaxPooling1D(pool_size=pool_size[2], strides=pool_stride_size[3],\n",
        "                            padding='same', name='block2_pool')))\n",
        "    model.add(TimeDistributed(Dropout(0.1, name='block2_dropout')))\n",
        "\n",
        "    model.add(TimeDistributed(Conv1D(filters=filter_num[3], kernel_size=kernel_size[3],\n",
        "                      strides=conv_stride_size[3], padding='same',\n",
        "                      name='block3_conv1')))\n",
        "    model.add(TimeDistributed(BatchNormalization()))\n",
        "    model.add(TimeDistributed(Activation('relu', name='block3_act1')))\n",
        "    model.add(TimeDistributed(Conv1D(filters=filter_num[3], kernel_size=kernel_size[3],\n",
        "                      strides=conv_stride_size[3], padding='same',\n",
        "                      name='block3_conv2')))\n",
        "    model.add(TimeDistributed(BatchNormalization()))\n",
        "    model.add(TimeDistributed(Activation('relu', name='block3_act2')))\n",
        "    model.add(TimeDistributed(MaxPooling1D(pool_size=pool_size[3], strides=pool_stride_size[3],\n",
        "                            padding='same', name='block3_pool')))\n",
        "    model.add(TimeDistributed(Dropout(0.1, name='block3_dropout')))\n",
        "\n",
        "    model.add(TimeDistributed(Conv1D(filters=filter_num[4], kernel_size=kernel_size[4],\n",
        "                      strides=conv_stride_size[4], padding='same',\n",
        "                      name='block4_conv1')))\n",
        "    model.add(TimeDistributed(BatchNormalization()))\n",
        "    model.add(TimeDistributed(Activation('relu', name='block4_act1')))\n",
        "    model.add(TimeDistributed(Conv1D(filters=filter_num[4], kernel_size=kernel_size[4],\n",
        "                      strides=conv_stride_size[4], padding='same',\n",
        "                      name='block4_conv2')))\n",
        "    model.add(TimeDistributed(BatchNormalization()))\n",
        "    model.add(TimeDistributed(Activation('relu', name='block4_act2')))\n",
        "    model.add(TimeDistributed(MaxPooling1D(pool_size=pool_size[4], strides=pool_stride_size[4],\n",
        "                            padding='same', name='block4_pool')))\n",
        "    model.add(TimeDistributed(Dropout(0.1, name='block4_dropout')))\n",
        "    model.add(TimeDistributed(Flatten(name='flatten')))\n",
        "    model.add(LSTM(256,return_sequences=True))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(LSTM(512))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(classes, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "def run_CNN_LSTM(X_train,y_train,X_test,y_test):\n",
        "    print('Pad sequences to ',maxlen)\n",
        "    NB_SPLIT = 10\n",
        "    x_train = X_train[:]\n",
        "    x_test = X_test[:]\n",
        "    x_train = sequence.pad_sequences(x_train, maxlen=maxlen,padding='post',truncating='post')\n",
        "    x_test = sequence.pad_sequences(x_test, maxlen=maxlen,padding='post',truncating='post')\n",
        "\n",
        "    x_train = x_train.reshape(x_train.shape[0],NB_SPLIT,maxlen//NB_SPLIT,1)# 把他拆分成NB_SPLIT个子序列，每个子序列分别交给CNN去处理，\n",
        "    x_test = x_test.reshape(x_test.shape[0],NB_SPLIT,maxlen//NB_SPLIT,1)\n",
        "    y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
        "    y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
        "    print('the shape of x_train',x_train.shape)\n",
        "    print('the shape of y_train',y_train.shape)\n",
        "    print('the shape of x_test',x_test.shape)\n",
        "    print('the shape of y_test',y_test.shape)\n",
        "\n",
        "    model_CNN_LSTM = CNN_LSTM(maxlen,NB_SPLIT,NB_CLASSES)\n",
        "    #model_CNN_LSTM.build()\n",
        "    #model_CNN_LSTM.summary()\n",
        "    model_CNN_LSTM.compile(loss='categorical_crossentropy',optimizer=OPTIMIZER,metrics=['accuracy', precision, recall, fmeasure])\n",
        "    history = model_CNN_LSTM.fit(x_train,y_train,batch_size=BATCH_SIZE,epochs=NB_EPOCH,validation_data=(x_test,y_test),verbose=0)\n",
        "    score = model_CNN_LSTM.evaluate(x_test,y_test,verbose=VERBOSE)\n",
        "    return(history.history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuVICfeHwymL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras import backend as K\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    # Calculates the precision\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    # Calculates the recall\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def fbeta_score(y_true, y_pred, beta=1):\n",
        "    # Calculates the F score, the weighted harmonic mean of precision and recall.\n",
        "    if beta < 0:\n",
        "        raise ValueError('The lowest choosable beta is zero (only precision).')\n",
        " \n",
        "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
        "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
        "        return 0\n",
        "\n",
        "    p = precision(y_true, y_pred)\n",
        "    r = recall(y_true, y_pred)\n",
        "    bb = beta ** 2\n",
        "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
        "    return fbeta_score\n",
        "\n",
        "def fmeasure(y_true, y_pred):\n",
        "    # Calculates the f-measure, the harmonic mean of precision and recall.\n",
        "    return fbeta_score(y_true, y_pred, beta=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEMNvc6PntbC",
        "colab_type": "code",
        "outputId": "369c5225-0307-4f09-fcf7-07e8cfafc0fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#DNN               ############# test for PERCENT_CHANGED  \n",
        "BATCH_SIZE = 64\n",
        "NB_EPOCH = 50\n",
        "VALIDATION_SPLIT = 0.3\n",
        "VERBOSE = 1\n",
        "OPTIMIZER = Adamax()\n",
        "\n",
        "historys_DNN_PERCENT_CHANGED = []\n",
        "historys_DNN_NB_TIMES = []\n",
        "PERCENT_CHANGED = [0.1,0.5,1,1.5,2,2.5,3,3.5,4,4.5,5,5.5,6,10,20] #hundred percent  %\n",
        "NB_TIMES = [0.1,0.5,1,1.5,2,3,4,5]      #new part of sample\n",
        "\n",
        "print('base_line for PERCENT_CHANGED#######################################################################################################################################')\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
        "historys_DNN_PERCENT_CHANGED.append(run_DNN(X_train,y_train,X_test,y_test))\n",
        "for i in PERCENT_CHANGED:\n",
        "    print('spliting data...')\n",
        "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
        "    X_train,y_train = locally_changed(X_train,y_train,PERCENT_CHANGED=i,NB_TIMES=0.2) \n",
        "    historys_DNN_PERCENT_CHANGED.append(run_DNN(X_train,y_train,X_test,y_test))\n",
        "print('base_line for NB_CLASSES###########################################################################################################################################')\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
        "historys_DNN_NB_TIMES.append(run_DNN(X_train,y_train,X_test,y_test))\n",
        "for i in NB_TIMES:\n",
        "    print('spliting data...')\n",
        "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
        "    X_train,y_train = creat_data(X_train,y_train,PERCENT_CHANGED=0.2,NB_TIMES=i) \n",
        "    historys_DNN_NB_TIMES.append(run_DNN(X_train,y_train,X_test,y_test))\n",
        "with open('historys_DNN_PERCENT_CHANGED.pkl','wb') as handle:\n",
        "  pickle.dump(historys_DNN_PERCENT_CHANGED,handle)\n",
        "with open('historys_DNN_NB_TIMES.pkl','wb') as handle:\n",
        "  pickle.dump(historys_DNN_NB_TIMES,handle) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "base_line for PERCENT_NOISE#######################################################################################################################################\n",
            "Pad sequences to  5000\n",
            "the shape of x_train (8307, 5000)\n",
            "the shape of y_train (8307, 108)\n",
            "the shape of x_test (3561, 5000)\n",
            "the shape of y_test (3561, 108)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "3561/3561 [==============================] - 0s 73us/step\n",
            "spliting data...\n",
            "1662 samples has beed add to X_train, 0.1% of noise for each sample\n",
            "Pad sequences to  5000\n",
            "the shape of x_train (9969, 5000)\n",
            "the shape of y_train (9969, 108)\n",
            "the shape of x_test (3561, 5000)\n",
            "the shape of y_test (3561, 108)\n",
            "3561/3561 [==============================] - 0s 77us/step\n",
            "spliting data...\n",
            "1662 samples has beed add to X_train, 0.5% of noise for each sample\n",
            "Pad sequences to  5000\n",
            "the shape of x_train (9969, 5000)\n",
            "the shape of y_train (9969, 108)\n",
            "the shape of x_test (3561, 5000)\n",
            "the shape of y_test (3561, 108)\n",
            "3561/3561 [==============================] - 0s 80us/step\n",
            "spliting data...\n",
            "1662 samples has beed add to X_train, 1% of noise for each sample\n",
            "Pad sequences to  5000\n",
            "the shape of x_train (9969, 5000)\n",
            "the shape of y_train (9969, 108)\n",
            "the shape of x_test (3561, 5000)\n",
            "the shape of y_test (3561, 108)\n",
            "3561/3561 [==============================] - 0s 76us/step\n",
            "spliting data...\n",
            "1662 samples has beed add to X_train, 1.5% of noise for each sample\n",
            "Pad sequences to  5000\n",
            "the shape of x_train (9969, 5000)\n",
            "the shape of y_train (9969, 108)\n",
            "the shape of x_test (3561, 5000)\n",
            "the shape of y_test (3561, 108)\n",
            "3561/3561 [==============================] - 0s 79us/step\n",
            "spliting data...\n",
            "1662 samples has beed add to X_train, 2% of noise for each sample\n",
            "Pad sequences to  5000\n",
            "the shape of x_train (9969, 5000)\n",
            "the shape of y_train (9969, 108)\n",
            "the shape of x_test (3561, 5000)\n",
            "the shape of y_test (3561, 108)\n",
            "3561/3561 [==============================] - 0s 81us/step\n",
            "spliting data...\n",
            "1662 samples has beed add to X_train, 2.5% of noise for each sample\n",
            "Pad sequences to  5000\n",
            "the shape of x_train (9969, 5000)\n",
            "the shape of y_train (9969, 108)\n",
            "the shape of x_test (3561, 5000)\n",
            "the shape of y_test (3561, 108)\n",
            "3561/3561 [==============================] - 0s 76us/step\n",
            "spliting data...\n",
            "1662 samples has beed add to X_train, 3% of noise for each sample\n",
            "Pad sequences to  5000\n",
            "the shape of x_train (9969, 5000)\n",
            "the shape of y_train (9969, 108)\n",
            "the shape of x_test (3561, 5000)\n",
            "the shape of y_test (3561, 108)\n",
            "3561/3561 [==============================] - 0s 77us/step\n",
            "spliting data...\n",
            "1662 samples has beed add to X_train, 3.5% of noise for each sample\n",
            "Pad sequences to  5000\n",
            "the shape of x_train (9969, 5000)\n",
            "the shape of y_train (9969, 108)\n",
            "the shape of x_test (3561, 5000)\n",
            "the shape of y_test (3561, 108)\n",
            "3561/3561 [==============================] - 0s 75us/step\n",
            "spliting data...\n",
            "1662 samples has beed add to X_train, 4% of noise for each sample\n",
            "Pad sequences to  5000\n",
            "the shape of x_train (9969, 5000)\n",
            "the shape of y_train (9969, 108)\n",
            "the shape of x_test (3561, 5000)\n",
            "the shape of y_test (3561, 108)\n",
            "3561/3561 [==============================] - 0s 77us/step\n",
            "spliting data...\n",
            "1662 samples has beed add to X_train, 4.5% of noise for each sample\n",
            "Pad sequences to  5000\n",
            "the shape of x_train (9969, 5000)\n",
            "the shape of y_train (9969, 108)\n",
            "the shape of x_test (3561, 5000)\n",
            "the shape of y_test (3561, 108)\n",
            "3561/3561 [==============================] - 0s 81us/step\n",
            "spliting data...\n",
            "1662 samples has beed add to X_train, 5% of noise for each sample\n",
            "Pad sequences to  5000\n",
            "the shape of x_train (9969, 5000)\n",
            "the shape of y_train (9969, 108)\n",
            "the shape of x_test (3561, 5000)\n",
            "the shape of y_test (3561, 108)\n",
            "3561/3561 [==============================] - 0s 82us/step\n",
            "spliting data...\n",
            "1662 samples has beed add to X_train, 5.5% of noise for each sample\n",
            "Pad sequences to  5000\n",
            "the shape of x_train (9969, 5000)\n",
            "the shape of y_train (9969, 108)\n",
            "the shape of x_test (3561, 5000)\n",
            "the shape of y_test (3561, 108)\n",
            "3561/3561 [==============================] - 0s 77us/step\n",
            "spliting data...\n",
            "1662 samples has beed add to X_train, 6% of noise for each sample\n",
            "Pad sequences to  5000\n",
            "the shape of x_train (9969, 5000)\n",
            "the shape of y_train (9969, 108)\n",
            "the shape of x_test (3561, 5000)\n",
            "the shape of y_test (3561, 108)\n",
            "3561/3561 [==============================] - 0s 81us/step\n",
            "spliting data...\n",
            "1662 samples has beed add to X_train, 10% of noise for each sample\n",
            "Pad sequences to  5000\n",
            "the shape of x_train (9969, 5000)\n",
            "the shape of y_train (9969, 108)\n",
            "the shape of x_test (3561, 5000)\n",
            "the shape of y_test (3561, 108)\n",
            "3561/3561 [==============================] - 0s 79us/step\n",
            "spliting data...\n",
            "1662 samples has beed add to X_train, 20% of noise for each sample\n",
            "Pad sequences to  5000\n",
            "the shape of x_train (9969, 5000)\n",
            "the shape of y_train (9969, 108)\n",
            "the shape of x_test (3561, 5000)\n",
            "the shape of y_test (3561, 108)\n",
            "3561/3561 [==============================] - 0s 83us/step\n",
            "base_line for NB_CLASSES###########################################################################################################################################\n",
            "Pad sequences to  5000\n",
            "the shape of x_train (8307, 5000)\n",
            "the shape of y_train (8307, 108)\n",
            "the shape of x_test (3561, 5000)\n",
            "the shape of y_test (3561, 108)\n",
            "3561/3561 [==============================] - 0s 88us/step\n",
            "spliting data...\n",
            "831 samples has beed add to X_train, 0.2% of noise for each sample\n",
            "Pad sequences to  5000\n",
            "the shape of x_train (9138, 5000)\n",
            "the shape of y_train (9138, 108)\n",
            "the shape of x_test (3561, 5000)\n",
            "the shape of y_test (3561, 108)\n",
            "3561/3561 [==============================] - 0s 85us/step\n",
            "spliting data...\n",
            "4154 samples has beed add to X_train, 0.2% of noise for each sample\n",
            "Pad sequences to  5000\n",
            "the shape of x_train (12461, 5000)\n",
            "the shape of y_train (12461, 108)\n",
            "the shape of x_test (3561, 5000)\n",
            "the shape of y_test (3561, 108)\n",
            "3561/3561 [==============================] - 0s 85us/step\n",
            "spliting data...\n",
            "8307 samples has beed add to X_train, 0.2% of noise for each sample\n",
            "Pad sequences to  5000\n",
            "the shape of x_train (16614, 5000)\n",
            "the shape of y_train (16614, 108)\n",
            "the shape of x_test (3561, 5000)\n",
            "the shape of y_test (3561, 108)\n",
            "3561/3561 [==============================] - 0s 86us/step\n",
            "spliting data...\n",
            "12461 samples has beed add to X_train, 0.2% of noise for each sample\n",
            "Pad sequences to  5000\n",
            "the shape of x_train (20768, 5000)\n",
            "the shape of y_train (20768, 108)\n",
            "the shape of x_test (3561, 5000)\n",
            "the shape of y_test (3561, 108)\n",
            "3561/3561 [==============================] - 0s 92us/step\n",
            "spliting data...\n",
            "16614 samples has beed add to X_train, 0.2% of noise for each sample\n",
            "Pad sequences to  5000\n",
            "the shape of x_train (24921, 5000)\n",
            "the shape of y_train (24921, 108)\n",
            "the shape of x_test (3561, 5000)\n",
            "the shape of y_test (3561, 108)\n",
            "3561/3561 [==============================] - 0s 84us/step\n",
            "spliting data...\n",
            "24921 samples has beed add to X_train, 0.2% of noise for each sample\n",
            "Pad sequences to  5000\n",
            "the shape of x_train (33228, 5000)\n",
            "the shape of y_train (33228, 108)\n",
            "the shape of x_test (3561, 5000)\n",
            "the shape of y_test (3561, 108)\n",
            "3561/3561 [==============================] - 0s 82us/step\n",
            "spliting data...\n",
            "33228 samples has beed add to X_train, 0.2% of noise for each sample\n",
            "Pad sequences to  5000\n",
            "the shape of x_train (41535, 5000)\n",
            "the shape of y_train (41535, 108)\n",
            "the shape of x_test (3561, 5000)\n",
            "the shape of y_test (3561, 108)\n",
            "3561/3561 [==============================] - 0s 83us/step\n",
            "spliting data...\n",
            "41535 samples has beed add to X_train, 0.2% of noise for each sample\n",
            "Pad sequences to  5000\n",
            "the shape of x_train (49842, 5000)\n",
            "the shape of y_train (49842, 108)\n",
            "the shape of x_test (3561, 5000)\n",
            "the shape of y_test (3561, 108)\n",
            "3561/3561 [==============================] - 0s 84us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6Py6svpp1YU",
        "colab_type": "code",
        "outputId": "2c87570d-a422-4bfa-fe20-ef9d717a7f3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#CNN               ############# test for PERCENT_CHANGED  \n",
        "BATCH_SIZE = 64\n",
        "NB_EPOCH = 50\n",
        "VALIDATION_SPLIT = 0.3\n",
        "VERBOSE = 1\n",
        "OPTIMIZER = Adamax()\n",
        "\n",
        "historys_CNN_PERCENT_CHANGED = []\n",
        "historys_CNN_NB_TIMES = []\n",
        "PERCENT_CHANGED = [0.1,0.5,1,1.5,2,2.5,3,3.5,4,4.5,5,5.5,6,10,20] #hundred percent  %\n",
        "NB_TIMES = [0.1,0.5,1,1.5,2,3,4,5]      #new part of sample\n",
        "\n",
        "print('base_line for PERCENT_CHANGED#######################################################################################################################################')\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
        "historys_CNN_PERCENT_CHANGED.append(run_CNN(X_train,y_train,X_test,y_test))\n",
        "for i in PERCENT_CHANGED:\n",
        "    print('spliting data...')\n",
        "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
        "    X_train,y_train = locally_changed(X_train,y_train,PERCENT_CHANGED=i,NB_TIMES=0.2) \n",
        "    historys_CNN_PERCENT_CHANGED.append(run_CNN(X_train,y_train,X_test,y_test))\n",
        "print('base_line for NB_CLASSES###########################################################################################################################################')\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
        "historys_CNN_NB_TIMES.append(run_CNN(X_train,y_train,X_test,y_test))\n",
        "for i in NB_TIMES:\n",
        "    print('spliting data...')\n",
        "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
        "    X_train,y_train = creat_data(X_train,y_train,PERCENT_CHANGED=0.2,NB_TIMES=i) \n",
        "    historys_CNN_NB_TIMES.append(run_CNN(X_train,y_train,X_test,y_test))\n",
        "with open('historys_CNN_PERCENT_CHANGED.pkl','wb') as handle:\n",
        "  pickle.dump(historys_CNN_PERCENT_CHANGED,handle)\n",
        "with open('historys_CNN_NB_TIMES.pkl','wb') as handle:\n",
        "  pickle.dump(historys_CNN_NB_TIMES,handle) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "base_line for PERCENT_NOISE#######################################################################################################################################\n",
            "Pad sequences to  5000\n",
            "the shape of x_train (8307, 5000, 1)\n",
            "the shape of y_train (8307, 108)\n",
            "the shape of x_test (3561, 5000, 1)\n",
            "the shape of y_test (3561, 108)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "3561/3561 [==============================] - 1s 394us/step\n",
            "spliting data...\n",
            "1662 samples has beed add to X_train, 0.1% of noise for each sample\n",
            "Pad sequences to  5000\n",
            "the shape of x_train (9969, 5000, 1)\n",
            "the shape of y_train (9969, 108)\n",
            "the shape of x_test (3561, 5000, 1)\n",
            "the shape of y_test (3561, 108)\n",
            "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "3561/3561 [==============================] - 1s 366us/step\n",
            "spliting data...\n",
            "1662 samples has beed add to X_train, 0.2% of noise for each sample\n",
            "Pad sequences to  5000\n",
            "the shape of x_train (9969, 5000, 1)\n",
            "the shape of y_train (9969, 108)\n",
            "the shape of x_test (3561, 5000, 1)\n",
            "the shape of y_test (3561, 108)\n",
            "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "3561/3561 [==============================] - 1s 375us/step\n",
            "spliting data...\n",
            "1662 samples has beed add to X_train, 0.3% of noise for each sample\n",
            "Pad sequences to  5000\n",
            "the shape of x_train (9969, 5000, 1)\n",
            "the shape of y_train (9969, 108)\n",
            "the shape of x_test (3561, 5000, 1)\n",
            "the shape of y_test (3561, 108)\n",
            "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "3561/3561 [==============================] - 1s 378us/step\n",
            "spliting data...\n",
            "1662 samples has beed add to X_train, 0.4% of noise for each sample\n",
            "Pad sequences to  5000\n",
            "the shape of x_train (9969, 5000, 1)\n",
            "the shape of y_train (9969, 108)\n",
            "the shape of x_test (3561, 5000, 1)\n",
            "the shape of y_test (3561, 108)\n",
            "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "3561/3561 [==============================] - 1s 382us/step\n",
            "spliting data...\n",
            "1662 samples has beed add to X_train, 0.5% of noise for each sample\n",
            "Pad sequences to  5000\n",
            "the shape of x_train (9969, 5000, 1)\n",
            "the shape of y_train (9969, 108)\n",
            "the shape of x_test (3561, 5000, 1)\n",
            "the shape of y_test (3561, 108)\n",
            "3561/3561 [==============================] - 1s 378us/step\n",
            "spliting data...\n",
            "1662 samples has beed add to X_train, 0.6% of noise for each sample\n",
            "Pad sequences to  5000\n",
            "the shape of x_train (9969, 5000, 1)\n",
            "the shape of y_train (9969, 108)\n",
            "the shape of x_test (3561, 5000, 1)\n",
            "the shape of y_test (3561, 108)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ah_JCMAUj_Xh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#LSTM               ############# test for PERCENT_CHANGED  \n",
        "BATCH_SIZE = 64\n",
        "NB_EPOCH = 50\n",
        "VALIDATION_SPLIT = 0.3\n",
        "VERBOSE = 1\n",
        "OPTIMIZER = Adamax()\n",
        "\n",
        "historys_LSTM_PERCENT_CHANGED = []\n",
        "historys_LSTM_NB_TIMES = []\n",
        "PERCENT_CHANGED = [0.1,0.5,1,1.5,2,2.5,3,3.5,4,4.5,5,5.5,6,10,20] #hundred percent  %\n",
        "NB_TIMES = [0.1,0.5,1,1.5,2,3,4,5]      #new part of sample\n",
        "\n",
        "print('base_line for PERCENT_CHANGED#######################################################################################################################################')\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
        "historys_LSTM_PERCENT_CHANGED.append(run_LSTM(X_train,y_train,X_test,y_test))\n",
        "for i in PERCENT_CHANGED:\n",
        "    print('spliting data...')\n",
        "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
        "    X_train,y_train = locally_changed(X_train,y_train,PERCENT_CHANGED=i,NB_TIMES=0.2) \n",
        "    historys_LSTM_PERCENT_CHANGED.append(run_LSTM(X_train,y_train,X_test,y_test))\n",
        "print('base_line for NB_CLASSES###########################################################################################################################################')\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
        "historys_LSTM_NB_TIMES.append(run_LSTM(X_train,y_train,X_test,y_test))\n",
        "for i in NB_TIMES:\n",
        "    print('spliting data...')\n",
        "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
        "    X_train,y_train = creat_data(X_train,y_train,PERCENT_CHANGED=0.2,NB_TIMES=i) \n",
        "    historys_LSTM_NB_TIMES.append(run_LSTM(X_train,y_train,X_test,y_test))\n",
        "with open('historys_LSTM_PERCENT_CHANGED.pkl','wb') as handle:\n",
        "  pickle.dump(historys_LSTM_PERCENT_CHANGED,handle)\n",
        "with open('historys_LSTM_NB_TIMES.pkl','wb') as handle:\n",
        "  pickle.dump(historys_LSTM_NB_TIMES,handle) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFReqQ7kkdcK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CNN_LSTM               ############# test for PERCENT_CHANGED  \n",
        "BATCH_SIZE = 64\n",
        "NB_EPOCH = 50\n",
        "VALIDATION_SPLIT = 0.3\n",
        "VERBOSE = 1\n",
        "OPTIMIZER = Adamax()\n",
        "\n",
        "historys_CNN_LSTM_PERCENT_CHANGED = []\n",
        "historys_CNN_LSTM_NB_TIMES = []\n",
        "PERCENT_CHANGED = [0.1,0.5,1,1.5,2,2.5,3,3.5,4,4.5,5,5.5,6,10,20] #hundred percent  %\n",
        "NB_TIMES = [0.1,0.5,1,1.5,2,3,4,5]      #new part of sample\n",
        "\n",
        "print('base_line for PERCENT_CHANGED#######################################################################################################################################')\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
        "historys_CNN_LSTM_PERCENT_CHANGED.append(run_CNN_LSTM(X_train,y_train,X_test,y_test))\n",
        "for i in PERCENT_CHANGED:\n",
        "    print('spliting data...')\n",
        "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
        "    X_train,y_train = locally_changed(X_train,y_train,PERCENT_CHANGED=i,NB_TIMES=0.2) \n",
        "    historys_CNN_LSTM_PERCENT_CHANGED.append(run_CNN_LSTM(X_train,y_train,X_test,y_test))\n",
        "print('base_line for NB_CLASSES###########################################################################################################################################')\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
        "historys_CNN_LSTM_NB_TIMES.append(run_CNN_LSTM(X_train,y_train,X_test,y_test))\n",
        "for i in NB_TIMES:\n",
        "    print('spliting data...')\n",
        "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
        "    X_train,y_train = creat_data(X_train,y_train,PERCENT_CHANGED=0.2,NB_TIMES=i) \n",
        "    historys_CNN_LSTM_NB_TIMES.append(run_CNN_LSTM(X_train,y_train,X_test,y_test))\n",
        "with open('historys_CNN_LSTM_PERCENT_CHANGED.pkl','wb') as handle:\n",
        "  pickle.dump(historys_CNN_LSTM_PERCENT_CHANGED,handle)\n",
        "with open('historys_CNN_LSTM_NB_TIMES.pkl','wb') as handle:\n",
        "  pickle.dump(historys_CNN_LSTM_NB_TIMES,handle) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmJjFI9gkmPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aklsdjfldsalkfjasdlkf\n",
        "故意报错停止"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZkPsw1S2SZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(30,10))\n",
        "labels_NB_TIMES = NB_TIMES.copy()\n",
        "labels_NB_TIMES.insert(0,0)\n",
        "for history,label in zip(historys_DNN_NB_TIMES,labels_NB_TIMES):\n",
        "    print(np.mean(history[-6:-1]),label)\n",
        "    plt.title('Result Analysis')\n",
        "    plt.plot(range(len(history)),history,label='P'+str(label))\n",
        "    [plt.annotate(label,(i,history[i])) for i in range(len(history))]\n",
        "    plt.legend() # 显示图例\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('acc')\n",
        "plt.show()\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X09FNfzf-K0k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "plt.figure(figsize=(30,10))\n",
        "labels_PERCENT_NOISE = PERCENT_NOISE.copy()\n",
        "labels_PERCENT_NOISE.insert(0,0)\n",
        "for history,label in zip(historys_DNN_PERCENT_NOISE,labels_PERCENT_NOISE):\n",
        "    print(np.mean(history[-6:-1]),label)\n",
        "    plt.title('Result Analysis')\n",
        "    plt.plot(range(len(history)),history,label='P'+str(label))\n",
        "    [plt.annotate(label,(i,history[i])) for i in range(len(history))]\n",
        "    plt.legend() # 显示图例\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('acc')\n",
        "plt.show()\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhoWRsiS95Fh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(30,10))\n",
        "labels_NB_TIMES = NB_TIMES.copy()\n",
        "labels_NB_TIMES.insert(0,0)\n",
        "for history,label in zip(historys_CNN_NB_TIMES,labels_NB_TIMES):\n",
        "    print(np.mean(history[-6:-1]),label)\n",
        "    plt.title('Result Analysis')\n",
        "    plt.plot(range(len(history)),history,label='P'+str(label))\n",
        "    [plt.annotate(label,(i,history[i])) for i in range(len(history))]\n",
        "    plt.legend() # 显示图例\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('acc')\n",
        "plt.show()\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ct7ddkVv2fsO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "plt.figure(figsize=(30,10))\n",
        "labels_PERCENT_NOISE = PERCENT_NOISE.copy()\n",
        "labels_PERCENT_NOISE.insert(0,0)\n",
        "for history,label in zip(historys_CNN_PERCENT_NOISE,labels_PERCENT_NOISE):\n",
        "    print(np.mean(history[-6:-1]),label)\n",
        "    plt.title('Result Analysis')\n",
        "    plt.plot(range(len(history)),history,label='P'+str(label))\n",
        "    [plt.annotate(label,(i,history[i])) for i in range(len(history))]\n",
        "    plt.legend() # 显示图例\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('acc')\n",
        "plt.show()\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASOofuK6U_Ps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5fGfE7yYekt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Rv1aH55UQFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbDvHvuPSLTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE9KpCI5Jdhl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY9Pt18bJtjD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}